[
  {
    "objectID": "skills.html#pandas-keras-numpy-scikitlearn-pyspark-and-tensorflow",
    "href": "skills.html#pandas-keras-numpy-scikitlearn-pyspark-and-tensorflow",
    "title": "Naveen Kannan’s Data Analytics Skills",
    "section": "PANDAS, Keras, Numpy, scikitlearn, PySpark and TensorFlow",
    "text": "PANDAS, Keras, Numpy, scikitlearn, PySpark and TensorFlow\n\nProficient in the Python programming language and related packages such as pandas, NumPy, and scipy for data manipulation, exploration, and analysis.\nExperienced in statistical analysis and modeling using Python, including regression analysis, time series analysis, clustering, and classification using packages such as statsmodels and scikit-learn.\nSkilled in machine learning techniques such as decision trees, random forests, gradient boosting, and neural networks using packages such as TensorFlow, Keras, and PyTorch.\nExperienced in using visualization tools such as Matplotlib, Seaborn, and Plotly for creating static and interactive visualizations.\nProficient in data preparation and cleaning using pandas and related packages, and able to handle missing values, outliers, and data transformations.\nSkilled in feature engineering using techniques such as one-hot encoding, scaling, and feature selection.\nExperienced in using scikit-learn for model selection, hyperparameter tuning, and evaluation using techniques such as cross-validation and ensemble methods.\nComfortable with using Jupyter notebooks for interactive data analysis and reporting."
  },
  {
    "objectID": "skills.html#advanced-statistical-analysis-regression-analysis",
    "href": "skills.html#advanced-statistical-analysis-regression-analysis",
    "title": "Naveen Kannan’s Data Analytics Skills",
    "section": "Advanced Statistical Analysis, Regression Analysis",
    "text": "Advanced Statistical Analysis, Regression Analysis\n\nProficient in the R programming language and related packages such as tidyverse, ggplot2, dplyr, and data.table for data manipulation, exploration, and visualization.\nExperienced in statistical analysis and modeling using R, including regression analysis, time series analysis, clustering, and classification.\nExperienced in using visualization tools such as ggplot2 and Shiny for creating interactive dashboards and reports to communicate insights to stakeholders."
  },
  {
    "objectID": "skills.html#ggplot-gt-and-matplotlib",
    "href": "skills.html#ggplot-gt-and-matplotlib",
    "title": "Naveen Kannan’s Data Analytics Skills",
    "section": "ggplot, gt, and matplotlib",
    "text": "ggplot, gt, and matplotlib\n\nFamiliar with ggplot2, a popular data visualization package in R, and able to create a wide range of visualizations such as scatter plots, box plots, and density plots using the Grammar of Graphics framework.\nExperienced in creating publication-quality plots and charts using Matplotlib and related libraries such as Seaborn and Plotly, as well as ggplot in R.\nExperienced in creating publication-quality tables for scientific journals."
  },
  {
    "objectID": "skills.html#hadoop-pyspark-and-sql",
    "href": "skills.html#hadoop-pyspark-and-sql",
    "title": "Naveen Kannan’s Data Analytics Skills",
    "section": "Hadoop, PySpark and SQL",
    "text": "Hadoop, PySpark and SQL\n\nProficient in PySpark, a Python library for Apache Spark, for large-scale data processing and machine learning tasks, including data transformation, feature engineering, and model training.\nExperienced in working with Hadoop distributed computing system and related technologies such as HDFS.\nSkilled in writing and optimizing SQL queries for data extraction and analysis, using various database management systems."
  },
  {
    "objectID": "skills.html#cli-docker-bash-scripting-and-apache-web-server",
    "href": "skills.html#cli-docker-bash-scripting-and-apache-web-server",
    "title": "Naveen Kannan’s Data Analytics Skills",
    "section": "CLI, Docker, Bash scripting and Apache Web Server",
    "text": "CLI, Docker, Bash scripting and Apache Web Server\n\nProficient in Linux operating system and command-line interface, including shell scripting, file system management, package installation, and system administration.\nExperienced in managing Linux-based servers for hosting data science applications, using technologies such as Docker, and Apache web server.\nComfortable with programming languages commonly used in Linux environment such as Python, R, and Bash scripting."
  },
  {
    "objectID": "posts/spark_postgres/spark_postgres.html",
    "href": "posts/spark_postgres/spark_postgres.html",
    "title": "Integrating HDFS and PostgreSQL through Apache Spark.",
    "section": "",
    "text": "The HDFS (Hadoop Distributed File System) and PostgreSQL databases are both powerful tools for data storage, queries and analyses. Each have their own unique strengths that make them well suited for specific tasks.\nThe HDFS, being distributed across several computing nodes, is robust and amenable to storing massive datasets, provided your computing infrastructure has the prerequisite width (the number of nodes in your cluster) and the depth(the available memory on each individual node). The HDFS is optimized for batch processing of massive datasets, making it suitable for big data applications like data warehousing, log processing, and large-scale data analytics. In fact, Spark, the HDFS’ natural companion, has it’s own machine learning library MLlib, making large scale data analytics very much possible.\nPostgreSQL databases, on the other hand, are robust RDBMS optimized for ACID (Atomicity, Consistency, Isolation, Durability) compliance. It provides robust data security and access control mechanisms, making it suitable for applications that require strict data protection. PostgreSQL supports SQL, a powerful query language that enables efficient querying and retrieval of data based on specific criteria. PostgreSQL shines at handling structured, transactional workloads that require data integrity, consistency, and complex querying capabilities.\n\n\n\nSpark and PostgreSQL make for a uniquely powerful combination."
  },
  {
    "objectID": "posts/spark_postgres/spark_postgres.html#the-prerequisites",
    "href": "posts/spark_postgres/spark_postgres.html#the-prerequisites",
    "title": "Integrating HDFS and PostgreSQL through Apache Spark.",
    "section": "The prerequisites",
    "text": "The prerequisites\nThis tutorial assumes you have the following prerequisites set up:\n\nHadoop Distributed File System (HDFS): You have HDFS installed and running on your cluster. HDFS will serve as the distributed storage system for your data files.\nApache Spark: You have Apache Spark installed and configured to run on your cluster. Spark will act as the data processing engine to read data from HDFS and load it into PostgreSQL.\nPostgreSQL Database: You have a PostgreSQL database instance up and running, either on the same cluster or a separate server. This is the target database where you want to load your data from HDFS."
  },
  {
    "objectID": "posts/spark_postgres/spark_postgres.html#downloading-the-postgresql-jdbc-driver",
    "href": "posts/spark_postgres/spark_postgres.html#downloading-the-postgresql-jdbc-driver",
    "title": "Integrating HDFS and PostgreSQL through Apache Spark.",
    "section": "Downloading the PostgreSQL JDBC driver",
    "text": "Downloading the PostgreSQL JDBC driver\nFind the PostgreSQL JDBC driver that is appropriate for your Java version and download it. The JDBC driver provides a standard API for Java applications (like Spark) to interact with relational databases. For example, if you’re using PostgreSQL 14.x and Java 8, you might download the postgresql-42.x.x.jar file.\nwget https://jdbc.postgresql.org/download/postgresql-xx.x.x.jar\nOnce you have the jar file downloaded, place it in the jars folder of your Spark installation.\nEdit your spark-env.sh file (which can be found in the conf folder) to have the following line:\nexport SPARK_CLASSPATH=/path/to/spark-x.x.x-bin-hadoop3/jars/postgresql-xx.x.x.jar \nNow restart your HDFS cluster, YARN, and MapReduce services to ensure the changes take effect. You can typically do this by running the stop-all.sh and start-all.sh scripts provided with your Hadoop installation."
  },
  {
    "objectID": "posts/spark_postgres/spark_postgres.html#connecting-pyspark-to-postgresql",
    "href": "posts/spark_postgres/spark_postgres.html#connecting-pyspark-to-postgresql",
    "title": "Integrating HDFS and PostgreSQL through Apache Spark.",
    "section": "Connecting PySpark to PostgreSQL",
    "text": "Connecting PySpark to PostgreSQL\nThe following code chunk makes the following assumptions. Change them to suit your use case:\nThe host name of the server that runs the PostgreSQL database is postgres.server.\nThe port on which PostgreSQL is listening is 5432 (the default port).\nThe name of the database you want to load the tables into is postgres.\nThe name of the user who has connection and write privileges on that database is user.\nThe password for that user is password.\nThe raw data (that has been cleaned) is named data_file, and is a CSV file. This data type can be anything that’s loaded on the HDFS, just remember to change the type of reader being used.\n# Assumptions and configurations\ndata_file = \"/path/to/data_file.csv\"  # Replace with the actual path to your data file on HDFS\ntable_name = \"your_table_name\"  # Replace with the desired table name in PostgreSQL\n\n# Read the data from HDFS\ndata = spark.read.csv(data_file,\n                      header=True,\n                      sep=',',\n                      inferSchema=True)  # inferSchema=True will automatically infer the schema\n\n# PostgreSQL database connection details\njdbc_url = \"jdbc:postgresql://postgres.server:5432/postgres\"\nproperties = {\n    \"user\": \"user\",\n    \"password\": \"password\",\n    \"driver\": \"org.postgresql.Driver\"\n}\n\n# Write the data to the PostgreSQL table\ndata.write.jdbc(url=jdbc_url, table=table_name, mode=\"overwrite\", properties=properties)\nThis code chunk will:\n\nRead the data in, and automatically create a schema. The spark.read.csv() method is used to read the data from the specified CSV file located on HDFS. It assumes that the file has a header row, and the columns are separated by commas. The inferSchema=True option tells Spark to automatically infer the schema (column names and data types) from the data.\nSet up a connection to your PostgreSQL database. The jdbc_url variable specifies the JDBC connection string for the PostgreSQL database, including the hostname, port, and database name. The properties dictionary contains the user credentials (username and password) and the PostgreSQL JDBC driver class name.\nWrite the data to the database with the name table_name. The data.write.jdbc() method is used to write the Spark DataFrame (data) to the specified PostgreSQL table. The url parameter specifies the JDBC connection URL, the table parameter specifies the target table name, and the mode=\"overwrite\" option tells Spark to overwrite the table if it already exists. The properties parameter passes the connection properties, including the user credentials and driver class name.\n\nWith this integration between HDFS, Apache Spark, and PostgreSQL, you can efficiently ingest and process large volumes of data stored in the HDFS and load it into your PostgreSQL database. Leveraging Spark’s powerful data processing capabilities and its ability to automatically infer schemas streamlines the data ingestion process, making it more efficient and flexible. By combining the strengths of these two data storage systems, you can unlock new possibilities for storing and analyzing large datasets."
  },
  {
    "objectID": "posts/mamba_pipeline/mamba_pipeline.html",
    "href": "posts/mamba_pipeline/mamba_pipeline.html",
    "title": "Mamba implementation in Scientific Pipelines.",
    "section": "",
    "text": "Mamba is intended to be a drop-in replacement for /reimplementation of conda (written in C++).\n\n\n\nThe Mamba logo.\n\n\nMamba has been something that I have implemented into all of my pipelines, since it trivializes the package management process. I do almost all of my work within the context of containers/virtual environments, and mamba makes my work life so much easier.\nPreviously, I used to use Conda as my package manager of choice, relying on it to cut down the amount of time I would need to build an environment using pip as an installer. Once I discovered Mamba, however, I never went back to Conda."
  },
  {
    "objectID": "posts/mamba_pipeline/mamba_pipeline.html#non-containerized-mamba",
    "href": "posts/mamba_pipeline/mamba_pipeline.html#non-containerized-mamba",
    "title": "Mamba implementation in Scientific Pipelines.",
    "section": "Non-containerized mamba",
    "text": "Non-containerized mamba\nIf you’re intending on installing mamba, the official documentation says to ensure that mamba and conda are the only packages to be installed in the base environment.\nNote that as of August 2023, miniforge and mambaforge are essentially the same thing.\n\n\n\n\n\n\nNote\n\n\n\nFrom the official miniforge documentation,\nAfter the release of Miniforge 23.3.1 in August 2023, Miniforge and Mambaforge are essentially identical. The only difference is the name of the installer and subsequently the default installation path.\nBefore that release, Miniforge only shipped conda, while Mambaforge added mamba on top. Since Miniconda started shipping conda-libmamba-solver in July 2023, Miniforge followed suit and started shipping it too in August. At that point, since conda-libmamba-solver depends on libmambapy, the only difference between Miniforge and Mambaforge was the presence of the mamba Python package. To minimize surprises, we decided to add mamba to Miniforge too.\n\n\nGiven that the official documentation encourages the use of Miniforge over Mambaforge, the rest of this blog post will be looking at installing and using Miniforge.\n\nMiniforge\nThe official miniforge documentation has a list of installers for different operating systems and architectures.\n\n\n\nA screencap of the docs.\n\n\nThe easiest way to do this is via a non-interactive method, using the following commands (for Unix-like platforms such as Mac OS & Linux)\n\n\n\n\n\n\nImportant\n\n\n\nThis section assumes you already have conda installed and set up in the base environment.\n\n\n## Download the appropriate installer for your architecture and OS\nwget \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\n\n## Run the script in non-interactive mode.\n## -p is prefix option. A directory will be created at \"${HOME}/conda\".\nbash Miniforge3-$(uname)-$(uname -m).sh -b -p \"${HOME}/conda\"\n\n##  Create the path to conda\nsource \"${HOME}/conda/etc/profile.d/conda.sh\"\n\n## For mamba support also run the following command\nsource \"${HOME}/conda/etc/profile.d/mamba.sh\"\n\n## Activate the base environment\nconda activate\nThis above script installs mamba into the base conda environment and then activates the conda environment, enabling the usage of mamba. Now, using mamba is as simple as replacing all instances of conda with mamba!\n\n\n\n\n\n\nNote\n\n\n\nRemember that the only packages you want installed in the base conda environment are mamba and conda, nothing else.\n\n\n\n\nMicromamba\nThe documentation for installation of micromamba can be found here.\nmicromamba is a tiny executable version of the mamba package manager. It is a statically linked C++ executable with a separate command line interface. It comes with an empty base environment, and does not come with a default version of Python or conda.\n\n\n\n\n\n\nNote\n\n\n\nAs a prerequisite, basic utilities (such as curl,tar and bzip2) need to be installed. Also you need a glibc based system like Ubuntu, Fedora or Centos (Alpine Linux does not work natively).\n\n\nThe following magic URL always returns the latest available version of micromamba, and the bin/micromamba part is automatically extracted using tar.\n# Linux Intel (x86_64):\ncurl -Ls https://micro.mamba.pm/api/micromamba/linux-64/latest | tar -xvj bin/micromamba\n# Linux ARM64:\ncurl -Ls https://micro.mamba.pm/api/micromamba/linux-aarch64/latest | tar -xvj bin/micromamba\n# Linux Power:\ncurl -Ls https://micro.mamba.pm/api/micromamba/linux-ppc64le/latest | tar -xvj bin/micromamba\n# macOS Intel (x86_64):\ncurl -Ls https://micro.mamba.pm/api/micromamba/osx-64/latest | tar -xvj bin/micromamba\n# macOS Silicon/M1 (ARM64):\ncurl -Ls https://micro.mamba.pm/api/micromamba/osx-arm64/latest | tar -xvj bin/micromamba\nAfter extraction is complete, the micromamba binary can be used.\n# Linux/bash:\n./bin/micromamba shell init -s bash -p ~/micromamba  # this writes to your .bashrc file\n# sourcing the bashrc file incorporates the changes into the running session.\n# better yet, restart your terminal!\nsource ~/.bashrc\nThe above code chunk incorporates the path to the micromamba binary into the restarted terminal.\nYou can now activate the base environment, and build other environments.\nmicromamba activate  # this activates the base environment\nmicromamba install python=3.6 jupyter -c conda-forge\n# or\nmicromamba create -n env_name xtensor -c conda-forge\nmicromamba activate env_name"
  },
  {
    "objectID": "posts/mamba_pipeline/mamba_pipeline.html#containerized-mamba-specifically-docker",
    "href": "posts/mamba_pipeline/mamba_pipeline.html#containerized-mamba-specifically-docker",
    "title": "Mamba implementation in Scientific Pipelines.",
    "section": "Containerized mamba (specifically Docker)",
    "text": "Containerized mamba (specifically Docker)\nThis is my recommended method of using mamba or micromamba. If you’re using mamba, you’re already encouraged to use conda environments. If you’re using a conda environment, you should 100% consider incorporating the usage of containerization into your scientific workflow.\nContainers make scientific pipelines easy to reproduce and easy to share, two concepts which form the bedrock of responsible science.\nI have a blog post about containers and why they rule. You can find it here.\n\nDocker-based Miniforge\nThis is very simple. Simply pull from the condaforge/miniforge3:latest tag.\nStart your dockerfile with the line:\nFROM condaforge/miniforge3:latest\nThis uses miniforge as a base image, and now you have a base environment with mamba and conda already installed and configured.\n\n\nDocker-based Micromamba\nThis is similar to the above section. Start your dockerfile with the line:\nFROM mambaorg/micromamba:latest"
  },
  {
    "objectID": "posts/hadoop_spark/hadoop_spark.html",
    "href": "posts/hadoop_spark/hadoop_spark.html",
    "title": "Installing and configuring Hadoop and Spark on a 4 node cluster.",
    "section": "",
    "text": "Apache Hadoop is an open source software library that allows for the distributed processing of large datasets across a computing cluster.\n\n\n\n\n\nHadoop shines at processing enormously massive amounts of data that is too big to fit on a single computing system."
  },
  {
    "objectID": "posts/hadoop_spark/hadoop_spark.html#prerequisites",
    "href": "posts/hadoop_spark/hadoop_spark.html#prerequisites",
    "title": "Installing and configuring Hadoop and Spark on a 4 node cluster.",
    "section": "Prerequisites:",
    "text": "Prerequisites:\nTo begin with, the 4 clusters require a Unix-like operating system.\nFurthermore, while not essential, using Ansible enormously reduces the time needed to configure and install HDFS with the aforementioned components.\nIn my previous blog post, I described how to use Ansible (deployed via a Docker container) to remotely configure a computing cluster. This obviates the need for Ansible itself to be installed on the head node. For details on how to use Ansible to configure a cluster, please refer my previous blog post.\nI used Ansible to perform all of the tasks detailed in this blog post. While this post describes the general steps needed for installing Hadoop, the next blog post will have the Ansible playbooks posted so as to describe each step of the process in more detail.\n\nJDK 8\nEnsure that each cluster node has OpenJDK installed.\nNote that if this is being installed using the yum package manager, the EPEL (Extra Packages for Enterprise Linux) repository needs to be imported and activated.\n\n\nSSH (Secure Shell) and PDSH\nHadoop requires SSH and PDSH to be installed on each node of the cluster. Communication between nodes is done via Hadoop.\nAlso, ensure that each node can SSH into every other node without a password prompt. This will require performing an SSH Key exchange between every node of the cluster and every other node. (Refer to my blog post about using Ansible to remotely configure a cluster for more details on SSH key exchanges.)\nPro-tip! On my RHEL 8 head node, I ran into a lot of erros from using pdsh since it didn’t come with SSH incorporated. I used dnf to download pdsh-rcmd-ssh after installing pdsh using yum.\nUpdate your .bashrc or .bash_profile file with the following:\nexport PDSH_RCMD_TYPE=ssh\n\n\nPython 3.9\nWhile not a hard requirement for just Hadoop, we intend to install Spark, and use Python to interact with the Scala API, using Jupyter Notebook as the driver.\n\n\nJupyter Notebook\nOnce Python is installed, run python3 -m pip install --upgrade pip in order to upgrade pip, and then install Jupyter using pip install jupyter."
  },
  {
    "objectID": "posts/hadoop_spark/hadoop_spark.html#installing-hadoop-and-spark",
    "href": "posts/hadoop_spark/hadoop_spark.html#installing-hadoop-and-spark",
    "title": "Installing and configuring Hadoop and Spark on a 4 node cluster.",
    "section": "Installing Hadoop and Spark",
    "text": "Installing Hadoop and Spark\n\nDownloading and Unzipping Hadoop\nDownload a recent stable release from one of the Apache Download Mirrors for Hadoop. (I downloaded Hadoop 3.3.6.)\nUnzip the downloaded tarball.\n\n\nDownloading and Unzipping Spark\nDownload a recent stable release from one of the Apache Download Mirrors for Spark. (I downloaded Spark 3.3.2 to be compatiable with Hail and Glow.)\nUnzip the downloaded tarball.\n\n\nConfiguring Hadoop and Spark\nIn your .bashrc or bash_profile, update the locations of Java, Hadoop and Spark to your PATH.\nUpdate the locations of HDFS, MapReduce and Common.\nReload the terminal.\nexport HADOOP_HOME=/opt/hadoop-3.3.6\nexport PATH=$PATH:$HADOOP_HOME/bin\nexport PATH=$PATH:$HADOOP_HOME/sbin\nexport HADOOP_MAPRED_HOME=${HADOOP_HOME}\nexport HADOOP_COMMON_HOME=${HADOOP_HOME}\nexport HADOOP_HDFS_HOME=${HADOOP_HOME}\nexport HADOOP_YARN_HOME=${HADOOP_HOME}\nexport JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.382.b05-2.el8.x86_64\nexport LANG=en_US.UTF-8\nexport TZ=UTC\nexport CLASSPATH_PREPEND_DISTCACHE=\nexport SPARK_HOME=/opt/spark-3.3.2-bin-hadoop3\nexport PATH=$PATH:$SPARK_HOME/bin\nexport HADOOP_CONF_DIR=/opt/hadoop-3.3.6/etc/hadoop\nexport LD_LIBRARY_PATH=/opt/hadoop-3.3.6/lib/native:$LD_LIBRARY_PATH\nexport PYSPARK_DRIVER_PYTHON=jupyter\nexport PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --NotebookApp.open_browser=False --NotebookApp.ip='localhost' \"\nexport PATH=$PATH:/usr/local/bin\n\n\nUpdating the Hadoop configuration files\nThe following configuration files need to be edited across the cluster.\n\nhadoop-env.sh\n\nThis file can be found in your installation directory. Assuming the name of the directory hadoop was unzipped to is /hadoop, the file can be found at /hadoop/etc/hadoop/hadoop-env.sh.\nAdd the following line to the file in order to add the location of the Java installation to Hadoop:\nexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/\nNote that the name/location of the Java installation can change depending on your system.\n\nworkers\n\nEdit the workers file to add the names of your cluster. Note that this requires the names of the nodes with their IP addresses to be present in the /etc/hosts file.\n\ncore-site.xml\n\nThis file can be found in your installation directory. Assuming the name of the directory hadoop was unzipped to is /hadoop, the file can be found at /hadoop/etc/hadoop/core-site.xml.\nInsert this code block between &lt;configuration&gt; and &lt;/configuration&gt;.\n&lt;property&gt;\n    &lt;name&gt;fs.defaultFS&lt;/name&gt;\n    &lt;value&gt;hdfs://namenode:9000&lt;/value&gt;\n&lt;/property&gt;\n\nhdfs-site.xml\n\nThis file can be found in your installation directory. Assuming the name of the directory hadoop was unzipped to is /hadoop, the file can be found at /hadoop/etc/hadoop/hdfs-site.xml.\nInsert this code block between &lt;configuration&gt; and &lt;/configuration&gt;.\n&lt;property&gt;\n    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n    &lt;value&gt;/home/temp/data/nameNode&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n    &lt;value&gt;/home/temp/data/dataNode&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;dfs.replication&lt;/name&gt;\n    &lt;value&gt;4&lt;/value&gt;\n&lt;/property&gt;\n\nyarn-site.xml\n\nThis file can be found in your installation directory. Assuming the name of the directory hadoop was unzipped to is /hadoop, the file can be found at /hadoop/etc/hadoop/yarn-site.xml.\nInsert this code block between &lt;configuration&gt; and &lt;/configuration&gt;.\n&lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt;\n    &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n    &lt;value&gt;(namenode IP Address)&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;yarn.acl.enable&lt;/name&gt;\n    &lt;value&gt;0&lt;/value&gt;\n&lt;/property&gt;\n\nmapred-site.xml\n\nThis file can be found in your installation directory. Assuming the name of the directory hadoop was unzipped to is /hadoop, the file can be found at /hadoop/etc/hadoop/mapred-site.xml.\nInsert this code block between &lt;configuration&gt; and &lt;/configuration&gt;.\n&lt;property&gt;\n    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n    &lt;value&gt;yarn&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;\n    &lt;value&gt;HADOOP_MAPRED_HOME=$HADOOP_HOME&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;mapreduce.map.env&lt;/name&gt;\n    &lt;value&gt;HADOOP_MAPRED_HOME=$HADOOP_HOME&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;\n    &lt;value&gt;HADOOP_MAPRED_HOME=$HADOOP_HOME&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;mapreduce.application.classpath&lt;/name&gt;\n    &lt;value&gt;$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*,$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*,$HADOOP_MAPRED_HOME/share/hadoop/common/*,$HADOOP_MAPRED_HOME/share/hadoop/common/lib/*,$HADOOP_MAPRED_HOME/share/hadoop/yarn/*,$HADOOP_MAPRED_HOME/share/hadoop/yarn/lib/*,$HADOOP_MAPRED_HOME/share/hadoop/hdfs/*,$HADOOP_MAPRED_HOME/share/hadoop/hdfs/lib/*&lt;/value&gt;\n&lt;/property&gt;\n\n\nUpdating the Spark configuration files\nRename spark-defaults.conf.template to spark-defaults.conf. The file can be found at spark-3.3.2-bin-hadoop3/conf/spark-defaults.conf.template.\nReplace\nspark.master   spark://master:7077\nwith\nspark.master   yarn\n\n\nStarting the HDFS and YARN services\nOnce the configuration files are edited and the PATH is updated, the HDFS namenode can be formatted by running hdfs namenode -format.\nThe HDFS node services can be initiated by running start-dfs.sh.\nYARN can be initiated by running start-yarn.sh.\nEnsure that these services are running by running the jps command.\nThe head node will have the services NameNode and ResourceManager running. Remember that these services run only on the head node.\nIf used as a data node, it will also have the services DataNode and NodeManager running on it.\nThe slave nodes will have DataNode and NodeManager running on it."
  },
  {
    "objectID": "posts/dnsmsq_pxe/dnsmasq_pxe.html",
    "href": "posts/dnsmsq_pxe/dnsmasq_pxe.html",
    "title": "Introduction to PXE boot servers.",
    "section": "",
    "text": "The Dnsmasq icon.\n\n\nWhat is a PXE boot server?\nThe term PXE stands for Preboot Execution Environment. This environment consists of a server which serves multiple clients. The server hosts software images, and the clients it serves are able to boot these images by retrieving them from the server via the network.\nEssentially, this allows for the clients to boot over the network, instead of from physical media such as a CD-ROM or hard disk, provided they are PXE boot capable. This typically includes BIOS and UEFI PCs.\nGroundwork\nThe PXE boot process relies on a lot of standard Internet protocols, and these include the following:\n\nDHCP (Dynamic Host Configuration Protocol): A client-server based network management protocol used on IPv4 and IPv6 networks to automatically assign IP addresses to devices within the network, ensuring that no two devices share the same IP address.\nTFTP (Trivial File Transfer Protocol): A simple client-server based file transfer protocol.\nNFS (Network File System): NFS is a file sharing mechanism that allows users within a computer network to access files over the network in a manner similar to how local storage is accessed.\nHTTP (Hypertext Transfer Protocol): In the PXE boot process, HTTP can be used by the PXE server to deliver OS images and other necessary files to the client machine over the network. Serving these files over HTTP can offer advantages such as higher transfer speeds and better reliability compared to TFTP.\n\nThe PXE server will serve the OS to it’s client node through a combination of these network protocols.\n\n\n\n\n\nA basic example of the network architecture of a PXE environment. The PXE server is a part of the local network to which it serves OS images.\n\n\nThe following are the tasks accomplished by a PXE server:\n\n\nThe PXE server is typically located on the same local network as the clients it serves, usually through direct Ethernet connections to a network switch. This setup ensures a high-speed, low-latency connection between the server and the clients.\n\n\n\nWhen a client initiates a PXE boot, it sends out a DHCP request to obtain an IP address and other network configuration details. The PXE server, which often also functions as a DHCP server or works in conjunction with one, responds to this request, assigning a local IP address to the client from a predefined range. This local IP address is part of the same subnet as the PXE server.\n\n\n\nThis IP address can be hard-coded to the MAC address of a NIC (Network Interface Card), a process known as DHCP reservation. By configuring DHCP reservations, the PXE server assigns a specific IP address to each device based on its unique MAC address. This ensures that the same IP address is consistently assigned to a particular device each time it boots, leading to a static and predictable network configuration.\n\n\n\nAdditionally, the PXE server can serve different images based on the MAC address of the NIC making the DHCP request. This allows for tailored deployment scenarios where different devices receive customized boot images or configurations. For example, workstations, servers, and other networked devices can each receive an appropriate OS image or boot configuration suited to their roles within the network.\nOnce the IP address is assigned, the PXE server can serve the necessary boot files to the client. This usually includes a network boot program (NBP) and an operating system image. The server may use TFTP for the initial stages of the boot process due to its simplicity and ease of implementation. However, for larger files, more efficient protocols like HTTP or NFS can be used to improve transfer speeds and reliability.\nThis local network setup allows the PXE server to handle multiple client requests simultaneously, while also managing IP address allocation. It also simplifies network management, as all devices are within the same local network, reducing the complexity associated with routing and firewall configurations that might be required if the PXE server and clients were on different networks."
  },
  {
    "objectID": "posts/dnsmsq_pxe/dnsmasq_pxe.html#network-architecture",
    "href": "posts/dnsmsq_pxe/dnsmasq_pxe.html#network-architecture",
    "title": "Introduction to PXE boot servers.",
    "section": "",
    "text": "A basic example of the network architecture of a PXE environment. The PXE server is a part of the local network to which it serves OS images.\n\n\nThe following are the tasks accomplished by a PXE server:\n\n\nThe PXE server is typically located on the same local network as the clients it serves, usually through direct Ethernet connections to a network switch. This setup ensures a high-speed, low-latency connection between the server and the clients.\n\n\n\nWhen a client initiates a PXE boot, it sends out a DHCP request to obtain an IP address and other network configuration details. The PXE server, which often also functions as a DHCP server or works in conjunction with one, responds to this request, assigning a local IP address to the client from a predefined range. This local IP address is part of the same subnet as the PXE server.\n\n\n\nThis IP address can be hard-coded to the MAC address of a NIC (Network Interface Card), a process known as DHCP reservation. By configuring DHCP reservations, the PXE server assigns a specific IP address to each device based on its unique MAC address. This ensures that the same IP address is consistently assigned to a particular device each time it boots, leading to a static and predictable network configuration.\n\n\n\nAdditionally, the PXE server can serve different images based on the MAC address of the NIC making the DHCP request. This allows for tailored deployment scenarios where different devices receive customized boot images or configurations. For example, workstations, servers, and other networked devices can each receive an appropriate OS image or boot configuration suited to their roles within the network.\nOnce the IP address is assigned, the PXE server can serve the necessary boot files to the client. This usually includes a network boot program (NBP) and an operating system image. The server may use TFTP for the initial stages of the boot process due to its simplicity and ease of implementation. However, for larger files, more efficient protocols like HTTP or NFS can be used to improve transfer speeds and reliability.\nThis local network setup allows the PXE server to handle multiple client requests simultaneously, while also managing IP address allocation. It also simplifies network management, as all devices are within the same local network, reducing the complexity associated with routing and firewall configurations that might be required if the PXE server and clients were on different networks."
  },
  {
    "objectID": "posts/ansible_playbooks/ansible_playbooks.html",
    "href": "posts/ansible_playbooks/ansible_playbooks.html",
    "title": "Using Ansible to install Hive on a Spark cluster.",
    "section": "",
    "text": "Apache Hive is a distributed, fault-tolerant data warehouse system, built on top of Hadoop, designed to simplify and streamline the processing of large datasets.\nThrough Hive, a user can manage and analyze massive volumes of data by organizing it into tables, resembling a traditional relational database.\nHive uses the HiveQL (HQL) language, which is very similar to SQL. These SQL-like queries get translated into MapReduce tasks, leveraging the power of Hadoop’s MapReduce functionalities while bypassing the need to know how to program MapReduce jobs.\nHive abstracts the complexity of Java-based utilization of Hadoop’s file storage system. It is best used for traditional data warehousing tasks.\nIn this blog post, we will look at installing Hive on a Hadoop cluster utilizing Ansible!"
  },
  {
    "objectID": "posts/ansible_playbooks/ansible_playbooks.html#part-1-defining-a-play",
    "href": "posts/ansible_playbooks/ansible_playbooks.html#part-1-defining-a-play",
    "title": "Using Ansible to install Hive on a Spark cluster.",
    "section": "Part 1: Defining a play",
    "text": "Part 1: Defining a play\n\n- name: Installing Apache Hive and its prerequisties on the RHEL 8 head node.\n  hosts: head_node\n  become: true\n  become_user: temp\n  become_method: sudo\n  \nThis section defines the play with the title “Installing Apache Hive and its prerequisties on the RHEL 8 head node”.\nhosts defines the managed node to run this playbook on (in this case, the Rhel 8 head node, named head_node in the inventory file).\nbecome tells Ansible that this play needs to be executed with elevated privileges.\nbecome_user defines the user to run the commands defined by Ansible as. (in this case, temp is the user.)\nbecome_method tells Ansible that the method to elevate the privilieges by is sudo."
  },
  {
    "objectID": "posts/ansible_playbooks/ansible_playbooks.html#part-2-downloading-hive-with-its-prerequisites",
    "href": "posts/ansible_playbooks/ansible_playbooks.html#part-2-downloading-hive-with-its-prerequisites",
    "title": "Using Ansible to install Hive on a Spark cluster.",
    "section": "Part 2: Downloading Hive (with it’s prerequisites)",
    "text": "Part 2: Downloading Hive (with it’s prerequisites)\n  tasks:\n  - name: Installing Maven on the RHEL 8 head node\n    ansible.builtin.get_url:\n      url: https://dlcdn.apache.org/maven/maven-3/3.9.4/binaries/apache-maven-3.9.4-bin.tar.gz\n      dest: /tmp/apache-maven-3.9.4-bin.tar.gz\n      checksum: \"sha512:https://downloads.apache.org/maven/maven-3/3.9.4/binaries/apache-maven-3.9.4-bin.tar.gz.sha512\"\n\n  - name: Decompressing the Maven tarball on the Rhel 8 head node\n    ansible.builtin.unarchive:\n      src: /tmp/apache-maven-3.9.4-bin.tar.gz\n      dest: /opt/\n      remote_src: yes\n\n  - name: Adding the Maven `bin` directory to the PATH\n    ansible.builtin.lineinfile:\n      path: /home/temp/.bashrc\n      insertafter: EOF\n      line: export PATH=$PATH:/opt/apache-maven-3.9.4/bin\n\n  - name: Downloading Apache Hive for the RHEL 8 head node\n    ansible.builtin.get_url:\n      url: https://dlcdn.apache.org/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz\n      dest: /tmp/apache-hive-3.1.3-bin.tar.gz\n      checksum: \"sha256:https://dlcdn.apache.org/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz.sha256\"\n      timeout: 60\n\n  - name: Decompressing the Apache Hive tarball for the RHEL 8 head node\n    ansible.builtin.unarchive:\n      src: /tmp/apache-hive-3.1.3-bin.tar.gz\n      dest: /opt/\n      remote_src: yes\n\n  - name: Adding the installation directory to the PATH\n    ansible.builtin.lineinfile:\n      path: /home/temp/.bashrc\n      insertafter: EOF\n      line: export HIVE_HOME=/opt/apache-hive-3.1.3-bin\n\n  - name: Adding the installation directory to the PATH\n    ansible.builtin.lineinfile:\n      path: /home/temp/.bashrc\n      insertafter: EOF\n      line: export PATH=$PATH:$HIVE_HOME/bin\n\n  - name: Editing the log4j.properties file\n    ansible.builtin.blockinfile:\n      path: /opt/hadoop-3.3.6/etc/hadoop/log4j.properties\n      insertafter: EOF\n      block: |\n        # Define an appender for the MRAppMaster logger\n        log4j.logger.org.apache.hadoop.mapreduce.v2.app.MRAppMaster=INFO, mrappmaster\n        # Appender for MRAppMaster logger\n        log4j.appender.mrappmaster=org.apache.log4j.ConsoleAppender\n        log4j.appender.mrappmaster.layout=org.apache.log4j.PatternLayout\n        log4j.appender.mrappmaster.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n\n  \n  - name: Reloading the shell\n    ansible.builtin.shell: source ~/.bashrc\nThis code of chunk downloads Maven (a software project management and comprehension tool), and installs it, and then proceeds to download and install Hive, and then updates the .bashrc file to add the installed binaries.\nCommands like ansible.builtin.blockinfile and ansible.builtin.get_url are modules that are built-in by Ansible to perform particular functions, and are part of each named task, which are themselves a part of each named play."
  },
  {
    "objectID": "posts/ansible_playbooks/ansible_playbooks.html#part-3-configuring-hive",
    "href": "posts/ansible_playbooks/ansible_playbooks.html#part-3-configuring-hive",
    "title": "Using Ansible to install Hive on a Spark cluster.",
    "section": "Part 3: Configuring Hive",
    "text": "Part 3: Configuring Hive\n- name: Configuring Hive on the RHEL 8 head node.\n  hosts: head_node\n  remote_user: temp\n  become_user: temp\n  become: true\n  become_method: sudo\n  tasks:\n\n  - name: Configuring the Hive Warehouse using hadoop\n    ansible.builtin.shell: |\n      source ~/.bashrc\n      export HADOOP_HOME=/opt/hadoop-3.3.6\n      export PATH=$PATH:$HADOOP_HOME/bin\n      export PATH=$PATH:$HADOOP_HOME/sbin\n      export HADOOP_MAPRED_HOME=${HADOOP_HOME}\n      export HADOOP_COMMON_HOME=${HADOOP_HOME}\n      export HADOOP_HDFS_HOME=${HADOOP_HOME}\n      export HADOOP_YARN_HOME=${HADOOP_HOME}\n      export HIVE_HOME=/opt/apache-hive-3.1.3-bin\n      export PATH=$PATH:$HIVE_HOME/bin\n      hadoop fs -mkdir /tmp\n      hadoop fs -mkdir /user\n      hadoop fs -mkdir /user/hive\n      hadoop fs -mkdir /user/hive/warehouse\n      hadoop fs -chmod g+w /tmp\n      hadoop fs -chmod g+w /user/hive/warehouse\n      schematool -dbType derby -initSchema\n    args:\n      executable: /bin/bash\n      \n  - name: Creating a hive-site.xml file\n    copy:\n      dest: /opt/apache-hive-3.1.3-bin/conf/hive-site.xml\n      content: |\n        &lt;property&gt;\n            &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;\n            &lt;value&gt;jdbc:mysql://localhost/metastore&lt;/value&gt;\n        &lt;/property&gt;\n        &lt;property&gt;\n            &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;\n            &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;\n        &lt;/property&gt;\n        &lt;property&gt;\n            &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;\n            &lt;value&gt;hive&lt;/value&gt;\n        &lt;/property&gt;\n        &lt;property&gt;\n            &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;\n            &lt;value&gt;hivepassword&lt;/value&gt;\n        &lt;/property&gt;\n\n  - name: Configuring Spark to use the Hive Metastore\n    ansible.builtin.lineinfile:\n      path: /opt/spark-3.3.2-bin-hadoop3/conf/spark-defaults.conf\n      insertafter: EOF\n      line: spark.sql.warehouse.dir=hdfs://localhost:9000/user/hive/warehouse\nLet’s reiterate. The first few lines define the play, which is to configure Hive. What follows that are tasks, whice each have a name and run a module. This above chunk initiates the Hive warehouse and creates a hive-site.xml file to use so that Spark can detect and use the Hive Metastore. Modules are the commands like ansible.builtin.shell and ansible.builtin.lineinfile, and they execute specific functions."
  },
  {
    "objectID": "posts/ansible_playbooks/ansible_playbooks.html#part-4-downloading-and-configuring-hive-for-the-cluster-nodes",
    "href": "posts/ansible_playbooks/ansible_playbooks.html#part-4-downloading-and-configuring-hive-for-the-cluster-nodes",
    "title": "Using Ansible to install Hive on a Spark cluster.",
    "section": "Part 4: Downloading and configuring Hive for the cluster nodes",
    "text": "Part 4: Downloading and configuring Hive for the cluster nodes\n- name: Installing Apache Hive and its prerequisties on the Ubuntu cluster nodes.\n  hosts: cluster_nodes\n  remote_user: temp\n  become: true\n  become_method: sudo\n  tasks:\n  - name: Installing Maven on the RHEL 8 head node\n    ansible.builtin.get_url:\n      url: https://dlcdn.apache.org/maven/maven-3/3.9.4/binaries/apache-maven-3.9.4-bin.tar.gz\n      dest: /tmp/apache-maven-3.9.4-bin.tar.gz\n      checksum: \"sha512:https://downloads.apache.org/maven/maven-3/3.9.4/binaries/apache-maven-3.9.4-bin.tar.gz.sha512\"\n\n  - name: Decompressing the Maven tarball on the Rhel 8 head node\n    ansible.builtin.unarchive:\n      src: /tmp/apache-maven-3.9.4-bin.tar.gz\n      dest: /opt/\n      remote_src: yes\n\n  - name: Adding the Maven `bin` directory to the PATH\n    ansible.builtin.lineinfile:\n      path: /home/temp/.bashrc\n      insertafter: EOF\n      line: export PATH=$PATH:/opt/apache-maven-3.9.4/bin\n\n  - name: Downloading Apache Hive for the RHEL 8 head node\n    ansible.builtin.get_url:\n      url: https://dlcdn.apache.org/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz\n      dest: /tmp/apache-hive-3.1.3-bin.tar.gz\n      checksum: \"sha256:https://dlcdn.apache.org/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz.sha256\"\n      timeout: 60\n\n  - name: Decompressing the Apache Hive tarball for the RHEL 8 head node\n    ansible.builtin.unarchive:\n      src: /tmp/apache-hive-3.1.3-bin.tar.gz\n      dest: /opt/\n      remote_src: yes\n\n  - name: Adding the installation directory to the PATH\n    ansible.builtin.lineinfile:\n      path: /home/temp/.bashrc\n      insertafter: EOF\n      line: export HIVE_HOME=/opt/apache-hive-3.1.3-bin\n\n  - name: Adding the installation directory to the PATH\n    ansible.builtin.lineinfile:\n      path: /home/temp/.bashrc\n      insertafter: EOF\n      line: export PATH=$PATH:$HIVE_HOME/bin\n\n  - name: Editing the log4j.properties file\n    ansible.builtin.blockinfile:\n      path: /opt/hadoop-3.3.6/etc/hadoop/log4j.properties\n      insertafter: EOF\n      block: |\n        # Define an appender for the MRAppMaster logger\n        log4j.logger.org.apache.hadoop.mapreduce.v2.app.MRAppMaster=INFO, mrappmaster\n        # Appender for MRAppMaster logger\n        log4j.appender.mrappmaster=org.apache.log4j.ConsoleAppender\n        log4j.appender.mrappmaster.layout=org.apache.log4j.PatternLayout\n        log4j.appender.mrappmaster.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n\n\n  - name: Reloading the environment variables\n    ansible.builtin.shell: |\n      . /home/temp/.bashrc\n    args:\n      executable: /bin/bash\n\n  - name: Configuring the Hive Warehouse using hadoop\n    ansible.builtin.shell: |\n      . /home/temp/.bashrc\n      export HADOOP_HOME=/opt/hadoop-3.3.6\n      export PATH=$PATH:$HADOOP_HOME/bin\n      export PATH=$PATH:$HADOOP_HOME/sbin\n      export HADOOP_MAPRED_HOME=${HADOOP_HOME}\n      export HADOOP_COMMON_HOME=${HADOOP_HOME}\n      export HADOOP_HDFS_HOME=${HADOOP_HOME}\n      export HADOOP_YARN_HOME=${HADOOP_HOME}\n      export HIVE_HOME=/opt/apache-hive-3.1.3-bin\n      export PATH=$PATH:$HIVE_HOME/bin\n      hadoop fs -mkdir /tmp\n      hadoop fs -mkdir /user\n      hadoop fs -mkdir /user/hive\n      hadoop fs -mkdir /user/hive/warehouse\n      hadoop fs -chmod g+w /tmp\n      hadoop fs -chmod g+w /user/hive/warehouse\n      schematool -dbType derby -initSchema\n    args:\n      executable: /bin/bash\n\n  - name: Creating a hive-site.xml file\n    copy:\n      dest: /opt/apache-hive-3.1.3-bin/conf/hive-site.xml\n      content: |\n        &lt;property&gt;\n            &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;\n            &lt;value&gt;jdbc:mysql://localhost/metastore&lt;/value&gt;\n        &lt;/property&gt;\n        &lt;property&gt;\n            &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;\n            &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;\n        &lt;/property&gt;\n        &lt;property&gt;\n            &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;\n            &lt;value&gt;hive&lt;/value&gt;\n        &lt;/property&gt;\n        &lt;property&gt;\n            &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;\n            &lt;value&gt;hivepassword&lt;/value&gt;\n        &lt;/property&gt;\n\n  - name: Configuring Spark to use the Hive Metastore\n    ansible.builtin.lineinfile:\n      path: /opt/spark-3.3.2-bin-hadoop3/conf/spark-defaults.conf\n      insertafter: EOF\n      line: spark.sql.warehouse.dir=hdfs://localhost:9000/user/hive/warehouse\nThis chunk of code performs the same tasks that the first three did, but on the cluster nodes! The hosts defined here are the cluster_nodes, which include the three cluster nodes, as seen from the inventory file."
  },
  {
    "objectID": "posts/2023-06-14/slurm_blog.html",
    "href": "posts/2023-06-14/slurm_blog.html",
    "title": "SLURM and HPC.",
    "section": "",
    "text": "SLURM Workload Manager\nSLURM (formerly known as Simple Linux Utility for Resource Management) is an open-source job scheduling system for Linux clusters.\nIt does not require kernel modification and is relatively self contained. It has three key functions:\n\nAllocation of access to resources (compute nodes) to users for a defined period of time.\nProviding a framework that allows for starting and executing jobs, including parallel computing processes.\nQueue management to arbitrate resource contention.\n\n\n\nSLURM and HPC clusters\nSLURM is uniquely suited for use in HPC clusters, due to its ability to facilitate efficient utilization of the resources available for an HPC cluster.\nSLURM places jobs/tasks in a queue, and access to resources is allowed based on the processes that are already running at the time, which is very well suited for an HPC cluster, where resources are under heavy usage.\n\n\n\nSLURM architecture. Slurm has a centralized manager, slurmctld, to monitor resources and work. Each compute server (node) has a slurmd daemon, which can be compared to a remote shell: it waits for work, executes that work, returns status, and waits for more work.\n\n\nSLURM makes it super easy to simplify and streamline the job submission process, without having to rely on complicated Python or Bash scripting. It also allows for proper resource allocation depending on the job’s requirements, which is essential in an HPC environment, especially with multiple people using it.\n\n\nSLURM Directives\nSLURM allows for specifying commands in the SLURM scripts that allow it to arbitrate job allocation and resource management. These commands are present at the top of a SLURM script, preceeded by #SBATCH.\nThese SBATCH commands and how specific they can get are the lynchpin of SLURM’s ability to perform resource and task arbitration. There are a large number of #SBATCH commands.\nOf these commands, the most relevant are:\n\n\n\n\n\n\n\nCommand\nUtility\n\n\n\n\n-N or --nodes=minnodes[-maxnodes]\nRequest that a minimum of N nodes be allocated\n\n\n-n or --ntasks=number\nRequest sufficient resources to launch a maximum of number tasks\n\n\n--mem-per-node=#MB or --mem-per-cpu=#MB\nSpecify minimum memory requirement per node or per CPU\n\n\n--ntasks-per-node=ntasks\nRequest the maximum ntasks be invoked on each node\n\n\n–mail-user=email_adress\nSend emails to the specified address for job-related events\n\n\n\nBased on these request parameters, SLURM can queue and allocate jobs to run on the available compute nodes. Each job that SLURM runs has it’s own job ID.\nSubmitted jobs by the user can be accessed by the command squeue -u &lt;USERNAME&gt;. These resulting output shows the job ID associated with each job.\n\n\nSLURM job arrays\nIf the same job needs to be run several times for different files, or if the same script needs to be run with several times with different parameters, SLURM job arrays allow for each individual job to be run in parallel without the need to submit an external script, or to run an iterative loop.\nHere, the #SBATCH command --array is used along with other SLURM directives. For example, #SBATCH --array=1-20 instructs the SLURM central manager to run the script 20 times. 20 independent tasks are created, that will run in parallel. These tasks are all assigned their own unique task ID, in addition to a common job ID (since these tasks are all submitted as part of a single job).\nSLURM uses an environmental variable that is unique for each task in the array. This variable is called $SLURM_ARRAY_TASK_ID.\n\n\nSLURM Scripts\nThe following is an example of a simple SLURM script. A folder named text_files was created, and that contained text files in the format chr{x}.txt, where x could be from 1 to 22, as well as X, Y and M. The purpose of this script is to read the absolute location of each text file, and print that in an output file called output.txt.\n#!/bin/bash\n#SBATCH --mail-user=XXX@gmail.com\n#SBATCH --mail-type=ALL\n#SBATCH --job-name=array_test\n#SBATCH --nodes 1\n#SBATCH --ntasks 25\n#SBATCH --mem 24g\n#SBATCH --array=1-25\n\n#Specifying path to array testing folder\nconfig=\"/home/nxk562/slurm_array_testing/text_files\"\n\n#Setting the array testing folder as the working directory\ncd \"$config\"\n\n#Using the index to iterate through the text files.\n#There are 25 text files.\n#This command uses \"ls -1\" to obtain a list of files.\n#sed -n \"${SLURM_ARRAY_TASK_ID}p\" uses the array task id as sort of a numerical index. It selects the line of the\n#ls output associated with the array ID.\ntext_file=$(ls | sed -n \"${SLURM_ARRAY_TASK_ID}p\")\n\n#Obtaining the absolute location of the text files\ntext_file_location=\"${config}/${text_file}\"\necho \"$text_file_location\"\necho \".\"\necho \".\"\necho \".\"\necho \".\"\n#Printing output\necho \"$text_file_location\" &gt;&gt; /home/nxk562/slurm_array_testing/output.txt\nLet’s look at this code. The #SBATCH options here are:\n\n--mail-user=XXX@gmail.com, which directs SLURM to send requested emails to the specified email address.\n--mail-type=ALL, which directs SLURM to notify user by email when certain event types occur. These can include NONE, BEGIN, END, FAIL, ARRAY_TASKS.\n--job-name=array_test, which provides a name for the job to go along with the job ID.\n--nodes=1, which directs SLURM to request a single compute node.\n--ntasks=25, which directs SLURM to request a node which can handle running a minimum of 25 tasks simultaneously.\n--mem 24g, which specifies the real memory required per node.\n--array=1-25, which specifies that there are 25 jobs to be executed with identical parameters.\n\nThe rest of the code uses $SLURM_ARRAY_TASK_ID as a numerical variable to sort through the files in the text_files folder using the ls command piped to a sed command. sed -n \"${SLURM_ARRAY_TASK_ID}p\" allows for selection of the line of output from ls associated with the $SLURM_ARRAY_TASK_ID variable.\nFor example, when $SLURM_ARRAY_TASK_ID is equal to 1, the SLURM script executes ls | sed -n \"1p\" , which takes the first line of output from the ls command, and prints it to output.txt.\n\n\nSLURM Output Files\nBy default, SLURM produces output files in the format slurm-&lt;job_ID&gt;.out.\nFor task arrays within a job, each task in the array will be associated with its own output file in the format slurm-&lt;job_ID&gt;_&lt;task_ID&gt;.out.\n\n\nSubmitting, viewing and cancelling SLURM jobs\nSLURM jobs are submitted using the sbatch command followed by the script.\nSLURM jobs that have been submitted can be viewed by the command squeue -u &lt;username&gt;.\nSLURM jobs can also be cancelled using the scancel &lt;job_ID&gt; command.\n\n\nReferences\nSLURM documentation can be found with great detail here. This post is intended to be a very basic example of SLURM, and is meant to motivate further interest in SLURM and how SLURM job arrays are specifically suited to a HPC environment with multiple users."
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(comment = NA) \nlibrary(knitr)\nlibrary(broom)\nlibrary(janitor) \nlibrary(naniar)\nlibrary(glue)\nlibrary(mice)\nlibrary(MASS)\nlibrary(pscl)\nlibrary(GGally)\nlibrary(VGAM)\nlibrary(car)\nlibrary(nnet)\nlibrary(rsample)\nlibrary(yardstick)\nlibrary(patchwork)\nlibrary(rms)\nlibrary(tidymodels)\nlibrary(boot)\nlibrary(conflicted)\nlibrary(kableExtra)\nlibrary(countreg)\nlibrary(topmodels)\nlibrary(equatiomatic)\nlibrary(tidyverse)\n\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#r-packages-and-setup",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#r-packages-and-setup",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(comment = NA) \nlibrary(knitr)\nlibrary(broom)\nlibrary(janitor) \nlibrary(naniar)\nlibrary(glue)\nlibrary(mice)\nlibrary(MASS)\nlibrary(pscl)\nlibrary(GGally)\nlibrary(VGAM)\nlibrary(car)\nlibrary(nnet)\nlibrary(rsample)\nlibrary(yardstick)\nlibrary(patchwork)\nlibrary(rms)\nlibrary(tidymodels)\nlibrary(boot)\nlibrary(conflicted)\nlibrary(kableExtra)\nlibrary(countreg)\nlibrary(topmodels)\nlibrary(equatiomatic)\nlibrary(tidyverse)\n\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#loading-the-raw-data",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#loading-the-raw-data",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "3.1 Loading the Raw Data",
    "text": "3.1 Loading the Raw Data\nThe data comes from Kaggle, and was part of Tidy Tuesday’s dataset for 2021-07-13, and can be found here.\n\n\nCode\nscooby_raw &lt;- read_csv('scooby_doo.csv', show_col_types = F)\nscooby_raw\n\n\n# A tibble: 603 × 75\n   index series_name   network season title imdb  engagement date_aired run_time\n   &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;date&gt;        &lt;dbl&gt;\n 1     1 Scooby Doo, … CBS     1      What… 8.1   556        1969-09-13       21\n 2     2 Scooby Doo, … CBS     1      A Cl… 8.1   479        1969-09-20       22\n 3     3 Scooby Doo, … CBS     1      Hass… 8     455        1969-09-27       21\n 4     4 Scooby Doo, … CBS     1      Mine… 7.8   426        1969-10-04       21\n 5     5 Scooby Doo, … CBS     1      Deco… 7.5   391        1969-10-11       21\n 6     6 Scooby Doo, … CBS     1      What… 8.4   384        1969-10-18       21\n 7     7 Scooby Doo, … CBS     1      Neve… 7.6   358        1969-10-25       21\n 8     8 Scooby Doo, … CBS     1      Foul… 8.2   358        1969-11-01       21\n 9     9 Scooby Doo, … CBS     1      The … 8.1   371        1969-11-08       21\n10    10 Scooby Doo, … CBS     1      Bedl… 8     346        1969-11-15       21\n# ℹ 593 more rows\n# ℹ 66 more variables: format &lt;chr&gt;, monster_name &lt;chr&gt;, monster_gender &lt;chr&gt;,\n#   monster_type &lt;chr&gt;, monster_subtype &lt;chr&gt;, monster_species &lt;chr&gt;,\n#   monster_real &lt;chr&gt;, monster_amount &lt;dbl&gt;, caught_fred &lt;chr&gt;,\n#   caught_daphnie &lt;chr&gt;, caught_velma &lt;chr&gt;, caught_shaggy &lt;chr&gt;,\n#   caught_scooby &lt;chr&gt;, captured_fred &lt;chr&gt;, captured_daphnie &lt;chr&gt;,\n#   captured_velma &lt;chr&gt;, captured_shaggy &lt;chr&gt;, captured_scooby &lt;chr&gt;, …"
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#cleaning-the-data",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#cleaning-the-data",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "3.2 Cleaning the Data",
    "text": "3.2 Cleaning the Data\n\n3.2.1 Selecting Variables We’ll Use\nThe variables that are intended to be used for the two models were selected and placed in a tibble titled scooby_filtered.\n\n\nCode\nscooby_filtered &lt;- scooby_raw |&gt;\n  select(index, imdb, engagement, network, season, rooby_rooby_roo,\n         motive, monster_amount,monster_gender, monster_real) \nscooby_filtered  &lt;- scooby_filtered %&gt;% replace(.==\"NULL\", NA) # replace with NA\nscooby_filtered\n\n\n# A tibble: 603 × 10\n   index imdb  engagement network season rooby_rooby_roo motive   monster_amount\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;           &lt;chr&gt;             &lt;dbl&gt;\n 1     1 8.1   556        CBS     1      1               Theft                 1\n 2     2 8.1   479        CBS     1      0               Theft                 1\n 3     3 8     455        CBS     1      0               Treasure              1\n 4     4 7.8   426        CBS     1      0               Natural…              1\n 5     5 7.5   391        CBS     1      0               Competi…              1\n 6     6 8.4   384        CBS     1      1               Extorti…              1\n 7     7 7.6   358        CBS     1      1               Competi…              1\n 8     8 8.2   358        CBS     1      1               Safety                1\n 9     9 8.1   371        CBS     1      1               Counter…              1\n10    10 8     346        CBS     1      0               Competi…              1\n# ℹ 593 more rows\n# ℹ 2 more variables: monster_gender &lt;chr&gt;, monster_real &lt;chr&gt;\n\n\n\n\n3.2.2 Converting Variable Types\nEach variable was then converted into the appropriate variable type. These variables were then placed in a tibble titled scooby_converted.\n\n\nCode\nscooby_converted &lt;- scooby_filtered |&gt;\n  mutate(index = as.character(index),\n         imdb = as.numeric(imdb), \n         engagement = as.numeric(engagement),\n         network = as_factor(network),\n         season = as_factor(season),\n         rooby_rooby_roo = as.double(rooby_rooby_roo),\n         motive = as_factor(motive),\n         monster_gender = as_factor(monster_gender),\n         monster_real = as_factor(monster_real))\nscooby_converted\n\n\n# A tibble: 603 × 10\n   index  imdb engagement network season rooby_rooby_roo motive   monster_amount\n   &lt;chr&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt;            &lt;dbl&gt; &lt;fct&gt;             &lt;dbl&gt;\n 1 1       8.1        556 CBS     1                    1 Theft                 1\n 2 2       8.1        479 CBS     1                    0 Theft                 1\n 3 3       8          455 CBS     1                    0 Treasure              1\n 4 4       7.8        426 CBS     1                    0 Natural…              1\n 5 5       7.5        391 CBS     1                    0 Competi…              1\n 6 6       8.4        384 CBS     1                    1 Extorti…              1\n 7 7       7.6        358 CBS     1                    1 Competi…              1\n 8 8       8.2        358 CBS     1                    1 Safety                1\n 9 9       8.1        371 CBS     1                    1 Counter…              1\n10 10      8          346 CBS     1                    0 Competi…              1\n# ℹ 593 more rows\n# ℹ 2 more variables: monster_gender &lt;fct&gt;, monster_real &lt;fct&gt;\n\n\n\n\n3.2.3 Sampling the Data\nscooby_converted was then filtered down to a tibble called scooby_sample that had complete cases on both the outcome variables. (rooby-rooby_roo and motive.)\n\n\nCode\nscooby_sample &lt;- scooby_converted |&gt;\n  filter(complete.cases(rooby_rooby_roo, motive))\nscooby_sample\n\n\n# A tibble: 535 × 10\n   index  imdb engagement network season rooby_rooby_roo motive   monster_amount\n   &lt;chr&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt;            &lt;dbl&gt; &lt;fct&gt;             &lt;dbl&gt;\n 1 1       8.1        556 CBS     1                    1 Theft                 1\n 2 2       8.1        479 CBS     1                    0 Theft                 1\n 3 3       8          455 CBS     1                    0 Treasure              1\n 4 4       7.8        426 CBS     1                    0 Natural…              1\n 5 5       7.5        391 CBS     1                    0 Competi…              1\n 6 6       8.4        384 CBS     1                    1 Extorti…              1\n 7 7       7.6        358 CBS     1                    1 Competi…              1\n 8 8       8.2        358 CBS     1                    1 Safety                1\n 9 9       8.1        371 CBS     1                    1 Counter…              1\n10 10      8          346 CBS     1                    0 Competi…              1\n# ℹ 525 more rows\n# ℹ 2 more variables: monster_gender &lt;fct&gt;, monster_real &lt;fct&gt;\n\n\n\n\n3.2.4 Working with Categorical Predictors\nThe categorical variables were then individually examined to assess the levels and the quantity of each level.\n\n\nCode\nscooby_sample |&gt; \n  tabyl(motive)|&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nmotive\nn\npercent\n\n\n\n\nTheft\n124\n0.2317757\n\n\nTreasure\n54\n0.1009346\n\n\nNatural Resource\n26\n0.0485981\n\n\nCompetition\n168\n0.3140187\n\n\nExtortion\n4\n0.0074766\n\n\nSafety\n2\n0.0037383\n\n\nCounterfeit\n6\n0.0112150\n\n\nInheritance\n8\n0.0149533\n\n\nSmuggling\n22\n0.0411215\n\n\nPreservation\n11\n0.0205607\n\n\nExperimentation\n4\n0.0074766\n\n\nFood\n11\n0.0205607\n\n\nTrespassing\n15\n0.0280374\n\n\nAssistance\n5\n0.0093458\n\n\nAbduction\n12\n0.0224299\n\n\nHaunt\n1\n0.0018692\n\n\nAnger\n3\n0.0056075\n\n\nImagination\n6\n0.0112150\n\n\nBully\n1\n0.0018692\n\n\nLoneliness\n1\n0.0018692\n\n\nTraining\n1\n0.0018692\n\n\nConquer\n42\n0.0785047\n\n\nMistake\n1\n0.0018692\n\n\nAutomated\n1\n0.0018692\n\n\nProduction\n1\n0.0018692\n\n\nEntertainment\n3\n0.0056075\n\n\nSimulation\n2\n0.0037383\n\n\n\n\n\n\n\nThe levels of motive were significantly collapsed into appropriate levels, and were then releveled according to size.\n\n\nCode\nscooby_sample &lt;- scooby_sample |&gt;\n  mutate(motive = fct_collapse(motive, \n                               theft = \"Theft\",\n                               treasure = \"Treasure\",\n                               natural_resource = \"Natural Resource\",\n                               competition = \"Competition\",\n                               conquer = \"Conquer\",\n                               indirect_monetary_gain = c(\"Extortion\",\"Counterfeit\",\"Inheritance\",\"Smuggling\"),\n                               property_related = c(\"Preservation\",\"Trespassing\"),\n                               harm_others = c(\"Abduction\",\"Bully\"),\n                               indirect_self_interest = c(\"Safety\",\"Experimentation\",\"Food\",\n                                                          \"Assistance\",\"Loneliness\",\"Entertainment\",\"Training\"),\n                               misc_motive =c(\"Haunt\",\"Anger\",\"Imagination\",\"Training\",\n                                              \"Mistake\",\"Automated\",\"Production\",\"Simulation\")))\nscooby_sample &lt;- scooby_sample |&gt;\n  mutate(motive = fct_collapse(motive,\n                               misc_motive = c(\"misc_motive\",\"harm_others\")))\n\nscooby_sample &lt;- scooby_sample |&gt;\n  mutate(motive = fct_collapse(motive,\n                               indirect_self_interest = c(\"indirect_self_interest\", \"natural_resource\",\"indirect_monetary_gain\",\"property_related\"),\n                               misc_motive = c(\"misc_motive\"))) |&gt;\n  mutate(motive = fct_relevel(motive, c(\"competition\",\"theft\",\"indirect_self_interest\",\"treasure\",\"conquer\",\"misc_motive\")))\n\n\nscooby_sample |&gt; \n  tabyl(motive)|&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nmotive\nn\npercent\n\n\n\n\ncompetition\n168\n0.3140187\n\n\ntheft\n124\n0.2317757\n\n\nindirect_self_interest\n118\n0.2205607\n\n\ntreasure\n54\n0.1009346\n\n\nconquer\n42\n0.0785047\n\n\nmisc_motive\n29\n0.0542056\n\n\n\n\n\n\n\nThe variable monster_gender was then examined. The variable originally describes the gender (as referred to by the Mystery Inc. gang) of the monster/monsters present in each episode.\n\n\nCode\nscooby_sample|&gt;\n  tabyl(monster_gender)|&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nmonster_gender\nn\npercent\nvalid_percent\n\n\n\n\nMale\n312\n0.5831776\n0.6178218\n\n\nFemale\n28\n0.0523364\n0.0554455\n\n\nMale,Male,Male\n23\n0.0429907\n0.0455446\n\n\nMale,Female\n10\n0.0186916\n0.0198020\n\n\nMale,Male\n47\n0.0878505\n0.0930693\n\n\nFemale,Male\n5\n0.0093458\n0.0099010\n\n\nMale,Male,Male,Male,None\n1\n0.0018692\n0.0019802\n\n\nMale,Male,Male,Male,Male,Male,Male,Male,Male\n4\n0.0074766\n0.0079208\n\n\nMale,Female,Male,Male,Male,Male,Male\n1\n0.0018692\n0.0019802\n\n\nMale,Male,Male,Male\n14\n0.0261682\n0.0277228\n\n\nMale,Male,Male,Male,Male,Male,Male,Male\n4\n0.0074766\n0.0079208\n\n\nMale,Female,Male\n4\n0.0074766\n0.0079208\n\n\nFemale,Male,\n1\n0.0018692\n0.0019802\n\n\nMale,Male,Male,Female\n1\n0.0018692\n0.0019802\n\n\nMale,Female,Male,Male,Male\n2\n0.0037383\n0.0039604\n\n\nMale,Male,Male,Female,Male,Male\n1\n0.0018692\n0.0019802\n\n\nFemale,Male,Male,Female\n1\n0.0018692\n0.0019802\n\n\nMale,Male,Female,Male\n3\n0.0056075\n0.0059406\n\n\nMale,Male,Male,Male,Female\n1\n0.0018692\n0.0019802\n\n\nMale,Male,Male,Male,Male,Male\n4\n0.0074766\n0.0079208\n\n\nMale,Male,Female\n4\n0.0074766\n0.0079208\n\n\nMale,Male,Male,Male,Male\n2\n0.0037383\n0.0039604\n\n\nFemale,Female,Female,Female\n1\n0.0018692\n0.0019802\n\n\nMale,Male,Male,Male,Male,Male,Male\n5\n0.0093458\n0.0099010\n\n\nFemale,Male,Male,Male\n1\n0.0018692\n0.0019802\n\n\nMale,Male,Male,Female,Female,Female,Male,Male,Male,Female,Male,Male,Male,Male,Male,Male,Male,Male,Male\n1\n0.0018692\n0.0019802\n\n\nMale,Male,Male,Male,Female,Female,Male\n1\n0.0018692\n0.0019802\n\n\nMale,Male,Male,Male,Male,Male,Female,Female,Female,Male\n1\n0.0018692\n0.0019802\n\n\nMale,Male,Male,Female,Male,Male,Female,Female\n1\n0.0018692\n0.0019802\n\n\nMale,Male,Male,Male,Male,Male,Male,Male,Male,Male,Male\n4\n0.0074766\n0.0079208\n\n\nFemale,Female\n1\n0.0018692\n0.0019802\n\n\nMale,Female,Female,Male,Male\n2\n0.0037383\n0.0039604\n\n\nFemale,Male,Male\n1\n0.0018692\n0.0019802\n\n\nMale,Male,Male,Male,Male,Male,Male,Male,Male,Male,Male,Male,Male,Male,Male\n1\n0.0018692\n0.0019802\n\n\nMale,Male,Male,Male,Male,Male,Male,Male,Male,Male,Male,Male,Male,Male,Male,Female\n1\n0.0018692\n0.0019802\n\n\nMale,Male,Male,Female,Male\n1\n0.0018692\n0.0019802\n\n\nMale,Male,Male,Male,Male,Male,Male,Male,Male,Male\n2\n0.0037383\n0.0039604\n\n\nMale,Male,Male,Male,Male,Male,Male,Male,Male,Male,Male,Male,Male\n2\n0.0037383\n0.0039604\n\n\nMale,Male,Male,Male,Male,Male,Male,Male,Male,Male,Male,Male\n2\n0.0037383\n0.0039604\n\n\nFemale,Male,Female,Male\n1\n0.0018692\n0.0019802\n\n\nMale,Male,Male,Male,Female,Male,Male,Male\n1\n0.0018692\n0.0019802\n\n\nMale,Female,Male,Male\n1\n0.0018692\n0.0019802\n\n\nMale,Male,Male,Male,Male,Male,Male,Female,Male,Male,Male,Male\n1\n0.0018692\n0.0019802\n\n\nNA\n30\n0.0560748\nNA\n\n\n\n\n\n\n\nIf a particular value of the variable contained the term “Female”, it was then converted into the term “female” alone, and “male” alone otherwise. Missing values were left untouched. The variable was then releveled to have “male” as the baseline level.\n\n\nCode\n# Collapsing values into a binary variable\nscooby_sample$monster_gender &lt;- \n  ifelse(grepl(\"Female\", scooby_sample$monster_gender), \"female\", ifelse(grepl(\"Male\", scooby_sample$monster_gender), \"male\", scooby_sample$monster_gender))\nscooby_sample &lt;- scooby_sample |&gt;\n  mutate(monster_gender = fct_relevel(monster_gender, c(\"male\",\"female\")))\nscooby_sample|&gt;\n  tabyl(monster_gender)|&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nmonster_gender\nn\npercent\nvalid_percent\n\n\n\n\nmale\n427\n0.7981308\n0.8455446\n\n\nfemale\n78\n0.1457944\n0.1544554\n\n\nNA\n30\n0.0560748\nNA\n\n\n\n\n\n\n\nThe network variable describes the television networks that ran the relevant episode of Scooby-Doo.\n\n\nCode\nscooby_sample|&gt;\n  tabyl(network)|&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nnetwork\nn\npercent\n\n\n\n\nCBS\n49\n0.0915888\n\n\nABC\n218\n0.4074766\n\n\nSyndication\n3\n0.0056075\n\n\nTBC\n0\n0.0000000\n\n\nCartoon Network\n82\n0.1532710\n\n\nWarner Home Video\n38\n0.0710280\n\n\nWarner Bros. Picture\n3\n0.0056075\n\n\nAdult Swim\n1\n0.0018692\n\n\nThe WB\n41\n0.0766355\n\n\nThe CW\n27\n0.0504673\n\n\nBoomerang\n73\n0.1364486\n\n\n\n\n\n\n\nThe levels of the variables were then collapsed to appropriate levels. The variable was then releveled according to level size.\n\n\nCode\nscooby_sample &lt;- scooby_sample |&gt;\n  mutate(network = fct_collapse(network, \n                               other_networks = c(\"Syndication\",\"TBC\",\"Adult Swim\"),\n                               warner_brothers = c(\"Warner Home Video\", \"Warner Bros. Picture\", \"The WB\"),\n                               cw = \"The CW\",\n                               cbs = \"CBS\",\n                               abc = \"ABC\",\n                               cartoon_network = \"Cartoon Network\",\n                               boomerang = \"Boomerang\")) |&gt;\n  mutate(network = fct_relevel(network, c(\"abc\",\"cartoon_network\",\"warner_brothers\",\"boomerang\",\"cbs\",\"cw\",\"other_networks\")))\nscooby_sample &lt;- scooby_sample |&gt;\n  mutate(network = fct_collapse(network, \n                               other_networks = c(\"cbs\",\"cw\",\"other_networks\")))\n                               \nscooby_sample|&gt;\n  tabyl(network)|&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nnetwork\nn\npercent\n\n\n\n\nabc\n218\n0.4074766\n\n\ncartoon_network\n82\n0.1532710\n\n\nwarner_brothers\n82\n0.1532710\n\n\nboomerang\n73\n0.1364486\n\n\nother_networks\n80\n0.1495327\n\n\n\n\n\n\n\nThe season variable describes the season of the episode of Scooby-Doo.\n\n\nCode\nscooby_sample|&gt;\n  tabyl(season)|&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nseason\nn\npercent\n\n\n\n\n1\n275\n0.5140187\n\n\n2\n143\n0.2672897\n\n\nCrossover\n7\n0.0130841\n\n\n3\n54\n0.1009346\n\n\nMovie\n40\n0.0747664\n\n\nSpecial\n12\n0.0224299\n\n\n4\n4\n0.0074766\n\n\n\n\n\n\n\nThe levels were renamed, and then the variable itself was releveled according to level size.\n\n\nCode\nscooby_sample &lt;- scooby_sample |&gt;\n  mutate(season = fct_collapse(season, \n                               season_1 = \"1\",\n                               season_2 = \"2\",\n                               season_3 = \"3\",\n                               season_4 = \"4\",\n                               crossover = \"Crossover\",\n                               movie = \"Movie\",\n                               special = \"Special\")) |&gt;\n  mutate(season = fct_relevel(season, c(\"season_1\",\"season_2\",\"season_3\",\"movie\",\"special\",\"crossover\",\"season_4\")))\nscooby_sample &lt;- scooby_sample |&gt;\n  mutate(season = fct_collapse(season, \n                               season_3_4 = c(\"season_3\",\"season_4\"),\n                               special_crossover = c(\"special\",\"crossover\")))\nscooby_sample|&gt;\n  tabyl(season)|&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nseason\nn\npercent\n\n\n\n\nseason_1\n275\n0.5140187\n\n\nseason_2\n143\n0.2672897\n\n\nseason_3_4\n58\n0.1084112\n\n\nmovie\n40\n0.0747664\n\n\nspecial_crossover\n19\n0.0355140\n\n\n\n\n\n\n\nThe monster_real variable describes if the monster was actually real or if was just a man in a mask, mechanically controlled or trained/hypnotized by a human.\nThe levels were renamed.\n\n\nCode\nscooby_sample|&gt;\n  tabyl(monster_real)|&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nmonster_real\nn\npercent\nvalid_percent\n\n\n\n\nFALSE\n402\n0.7514019\n0.7960396\n\n\nTRUE\n103\n0.1925234\n0.2039604\n\n\nNA\n30\n0.0560748\nNA\n\n\n\n\n\n\n\n\n\nCode\nscooby_sample &lt;- scooby_sample |&gt;\n  mutate(monster_real = fct_collapse(monster_real, \n                               not_real = \"FALSE\",\n                               real = \"TRUE\"))\nscooby_sample|&gt;\n  tabyl(monster_real)|&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nmonster_real\nn\npercent\nvalid_percent\n\n\n\n\nnot_real\n402\n0.7514019\n0.7960396\n\n\nreal\n103\n0.1925234\n0.2039604\n\n\nNA\n30\n0.0560748\nNA\n\n\n\n\n\n\n\n\n\n3.2.5 Working with Quantitative Variables\nEach individual quantitative variable was then described.\n\n\nCode\nscooby_sample |&gt;\n  select(imdb) |&gt;\n  describe()\n\n\nselect(scooby_sample, imdb) \n\n 1  Variables      535  Observations\n--------------------------------------------------------------------------------\nimdb \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     520       15       48    0.996    7.347   0.7684      5.9      6.5 \n     .25      .50      .75      .90      .95 \n     7.1      7.4      7.7      8.2      8.4 \n\nlowest : 4.6 4.8 4.9 5   5.1, highest: 9   9.1 9.2 9.3 9.6\n--------------------------------------------------------------------------------\n\n\n\n\nCode\nscooby_sample |&gt;\n  select(engagement) |&gt;\n  describe()\n\n\nselect(scooby_sample, engagement) \n\n 1  Variables      535  Observations\n--------------------------------------------------------------------------------\nengagement \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     520       15      229        1    645.2     1153     17.0     22.9 \n     .25      .50      .75      .90      .95 \n    31.0     75.0    166.2    384.6   1891.4 \n\nlowest :      7      9     11     12     13, highest:   6929  12620  20226  50847 100951\n--------------------------------------------------------------------------------\n\n\n\n\nCode\nscooby_sample |&gt;\n  select(monster_amount) |&gt;\n  describe()\n\n\nselect(scooby_sample, monster_amount) \n\n 1  Variables      535  Observations\n--------------------------------------------------------------------------------\nmonster_amount \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     535        0       16    0.739    1.944    1.781        0        1 \n     .25      .50      .75      .90      .95 \n       1        1        2        4        7 \n                                                                            \nValue          0     1     2     3     4     5     6     7     8     9    10\nFrequency     30   341    65    32    23     7     5     7     6     4     3\nProportion 0.056 0.637 0.121 0.060 0.043 0.013 0.009 0.013 0.011 0.007 0.006\n                                        \nValue         11    12    13    15    19\nFrequency      4     3     3     1     1\nProportion 0.007 0.006 0.006 0.002 0.002\n\nFor the frequency table, variable is rounded to the nearest 0\n--------------------------------------------------------------------------------\n\n\n\n\nCode\nscooby_sample |&gt;\n  select(rooby_rooby_roo) |&gt;\n  describe()\n\n\nselect(scooby_sample, rooby_rooby_roo) \n\n 1  Variables      535  Observations\n--------------------------------------------------------------------------------\nrooby_rooby_roo \n       n  missing distinct     Info     Mean      Gmd \n     535        0        8    0.782    0.729   0.7098 \n                                                          \nValue          0     1     2     3     4     5     6     7\nFrequency    210   289    19    11     3     1     1     1\nProportion 0.393 0.540 0.036 0.021 0.006 0.002 0.002 0.002\n\nFor the frequency table, variable is rounded to the nearest 0\n--------------------------------------------------------------------------------\n\n\n\n\n3.2.6 Identifying Missingness\n\n\nCode\nscooby_sample |&gt;\n  miss_var_summary() |&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nvariable\nn_miss\npct_miss\n\n\n\n\nmonster_gender\n30\n5.607477\n\n\nmonster_real\n30\n5.607477\n\n\nimdb\n15\n2.803738\n\n\nengagement\n15\n2.803738\n\n\nindex\n0\n0.000000\n\n\nnetwork\n0\n0.000000\n\n\nseason\n0\n0.000000\n\n\nrooby_rooby_roo\n0\n0.000000\n\n\nmotive\n0\n0.000000\n\n\nmonster_amount\n0\n0.000000\n\n\n\n\n\n\n\nThe variables monster_gender, monster_real, imdb and engagement have missing values."
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#print-and-save-the-final-tibble",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#print-and-save-the-final-tibble",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "3.3 Print and Save The Final Tibble",
    "text": "3.3 Print and Save The Final Tibble\n\n\nCode\nprint(scooby_sample)\n\n\n# A tibble: 535 × 10\n   index  imdb engagement network   season rooby_rooby_roo motive monster_amount\n   &lt;chr&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;            &lt;dbl&gt; &lt;fct&gt;           &lt;dbl&gt;\n 1 1       8.1        556 other_ne… seaso…               1 theft               1\n 2 2       8.1        479 other_ne… seaso…               0 theft               1\n 3 3       8          455 other_ne… seaso…               0 treas…              1\n 4 4       7.8        426 other_ne… seaso…               0 indir…              1\n 5 5       7.5        391 other_ne… seaso…               0 compe…              1\n 6 6       8.4        384 other_ne… seaso…               1 indir…              1\n 7 7       7.6        358 other_ne… seaso…               1 compe…              1\n 8 8       8.2        358 other_ne… seaso…               1 indir…              1\n 9 9       8.1        371 other_ne… seaso…               1 indir…              1\n10 10      8          346 other_ne… seaso…               0 compe…              1\n# ℹ 525 more rows\n# ℹ 2 more variables: monster_gender &lt;fct&gt;, monster_real &lt;fct&gt;\n\n\n\n\nCode\nwrite_rds(scooby_sample, file = \"scooby_sample.Rds\")"
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#analysis-1",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#analysis-1",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "5.1 Analysis 1",
    "text": "5.1 Analysis 1\n\n5.1.1 The Research Question\nPredicting an iconic catchphrase: Are the logistics of an episode of Scooby-Doo good predictors of the number of times an iconic catchphrase is spoken?\n\n\n5.1.2 The Outcome\nA numerical summary of the outcome variable was obtained.\n\n\nCode\nscooby_sample |&gt;\n  select(rooby_rooby_roo) |&gt;\n  describe()\n\n\nselect(scooby_sample, rooby_rooby_roo) \n\n 1  Variables      535  Observations\n--------------------------------------------------------------------------------\nrooby_rooby_roo \n       n  missing distinct     Info     Mean      Gmd \n     535        0        8    0.782    0.729   0.7098 \n                                                          \nValue          0     1     2     3     4     5     6     7\nFrequency    210   289    19    11     3     1     1     1\nProportion 0.393 0.540 0.036 0.021 0.006 0.002 0.002 0.002\n\nFor the frequency table, variable is rounded to the nearest 0\n--------------------------------------------------------------------------------\n\n\nMost of the values are 0s and 1s, with the other values being fewer in comparison.\n\n\n5.1.3 Visualizing the Outcome Variable\nThe outcome variable’s distribution was visualized using ggplot.\n\n\nCode\np1 &lt;- \n  ggplot(scooby_sample, aes(x = rooby_rooby_roo)) +\n  geom_histogram(bins = 25, \n  fill = \"#2b8cbe\", col = \"white\") +\n  labs( x = \"Number of times the catchphrase was said\", \n        y = \"Count\",\n        title = str_wrap(\"Distribution of the number of times the catchphrase 'Rooby-Rooby-Roo!' was spoken in each episode of Scooby-Doo\", width = 90),\n         subtitle = glue(\"Across \", nrow(scooby_sample), \n                           \" episodes/movies/specials of the entirety of the Scooby-Doo show.\"),\n         caption = \"\")\n\n\np1\n\n\n\n\n\n\n\n5.1.4 The Predictors\nThe predictors for the outcome are imdb, engagement, network and season. Each predictor variable was individually described.\n\nimdb is a continuous, quantitative variable.\n\n\n\nCode\nscooby_sample |&gt;\n  select(imdb) |&gt;\n  describe()\n\n\nselect(scooby_sample, imdb) \n\n 1  Variables      535  Observations\n--------------------------------------------------------------------------------\nimdb \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     520       15       48    0.996    7.347   0.7684      5.9      6.5 \n     .25      .50      .75      .90      .95 \n     7.1      7.4      7.7      8.2      8.4 \n\nlowest : 4.6 4.8 4.9 5   5.1, highest: 9   9.1 9.2 9.3 9.6\n--------------------------------------------------------------------------------\n\n\n\nengagement is a continuous, quantitative variable.\n\n\n\nCode\nscooby_sample |&gt;\n  select(engagement) |&gt;\n  describe()\n\n\nselect(scooby_sample, engagement) \n\n 1  Variables      535  Observations\n--------------------------------------------------------------------------------\nengagement \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     520       15      229        1    645.2     1153     17.0     22.9 \n     .25      .50      .75      .90      .95 \n    31.0     75.0    166.2    384.6   1891.4 \n\nlowest :      7      9     11     12     13, highest:   6929  12620  20226  50847 100951\n--------------------------------------------------------------------------------\n\n\n\nnetwork is a multi-categorical variable.\n\n\n\nCode\nscooby_sample |&gt;\n  tabyl(network) |&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nnetwork\nn\npercent\n\n\n\n\nabc\n218\n0.4074766\n\n\ncartoon_network\n82\n0.1532710\n\n\nwarner_brothers\n82\n0.1532710\n\n\nboomerang\n73\n0.1364486\n\n\nother_networks\n80\n0.1495327\n\n\n\n\n\n\n\nseason is a multi-categorical variable.\n\n\nCode\nscooby_sample |&gt;\n  tabyl(season) |&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nseason\nn\npercent\n\n\n\n\nseason_1\n275\n0.5140187\n\n\nseason_2\n143\n0.2672897\n\n\nseason_3_4\n58\n0.1084112\n\n\nmovie\n40\n0.0747664\n\n\nspecial_crossover\n19\n0.0355140\n\n\n\n\n\n\n\nThe outcome was then visualized against each predictor."
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#rooby_rooby_roo-vs-network",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#rooby_rooby_roo-vs-network",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "5.2 rooby_rooby_roo vs network",
    "text": "5.2 rooby_rooby_roo vs network\n\n\nCode\ntmp_data &lt;- scooby_sample\n\ntmp_data$network &lt;-  fct_recode(tmp_data$network,\n                               ABC = \"abc\",\n                               \"Cartoon Network\" = \"cartoon_network\",\n                               \"Warner Brothers\" = \"warner_brothers\",\n                               \"Boomerang\" = \"boomerang\",\n                               \"Other Networks\" = \"other_networks\")\nggplot(tmp_data, aes(x = network, y = rooby_rooby_roo)) +\n    geom_violin(fill = \"#2b8cbe\", alpha = 0.3, scale = \"width\")+ \n    geom_boxplot(fill = \"#2b8cbe\", width = 0.1,\n               outlier.color = \"red\") +\n  labs(y = \"Number of times 'Rooby-Rooby-Roo!' was said\",\n       x = \"Network/Producer\",\n       title = str_wrap(\"Mapping the distribution of the outcome variable across the networks/producers who produced Scooby-Doo.\",width = 90),\n       subtitle = \"Across 535 episodes of Scooby-Doo.\") \n\n\n\n\n\nWarner Brothers appears to have a higher number of Rooby-Rooby-Roo’s compared to the other networks."
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#rooby_rooby_roo-vs-season",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#rooby_rooby_roo-vs-season",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "5.3 rooby_rooby_roo vs season",
    "text": "5.3 rooby_rooby_roo vs season\n\n\nCode\ntmp_data &lt;- scooby_sample\n\ntmp_data$season &lt;-  fct_recode(tmp_data$season,\n                               \"Season 1\" = \"season_1\",\n                               \"Season 2\" = \"season_2\",\n                               \"Season 3 and 4\" = \"season_3_4\",\n                               \"Movie\" = \"movie\",\n                               \"Special / Crossover\" = \"special_crossover\")\nggplot(tmp_data, aes(x = season, y = rooby_rooby_roo)) +\n    geom_violin(fill = \"#2b8cbe\", alpha = 0.3, scale = \"width\")+ \n    geom_boxplot(fill = \"#2b8cbe\", width = 0.1,\n               outlier.color = \"red\") +\n  labs(y = \"Number of times 'Rooby-Rooby-Roo!' was said\",\n       x = \"Network/Producer\",\n       title = str_wrap(\"Mapping the distribution of the outcome variable across the various seasons/movies of Scooby-Doo.\",width = 90),\n       subtitle = \"Across 535 episodes of Scooby-Doo.\") \n\n\n\n\n\nMovies appear to have a higher number of Rooby-Rooby-Roos compared to the other seasons/specials/crossovers."
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#rooby_rooby_roo-vs-imdb",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#rooby_rooby_roo-vs-imdb",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "5.4 rooby_rooby_roo vs imdb",
    "text": "5.4 rooby_rooby_roo vs imdb\n\n\nCode\nggplot(tmp_data, aes(y = as.factor(rooby_rooby_roo), x = imdb)) +\n    geom_violin(fill = \"#2b8cbe\", alpha = 0.4, scale = \"width\") +\n   \n    geom_boxplot(fill = \"#2b8cbe\", width = 0.1,\n               outlier.color = \"red\") +\n  labs(y = \"Number of times 'Rooby-Rooby-Roo!' was said\",\n       x = \"IMDB Rating\",\n       title = str_wrap(\"Mapping the distribution of the outcome variable across the IMDB rating\",width = 90),\n       subtitle = \"Across 535 episodes of Scooby-Doo.\") \n\n\n\n\n\nScooby-Doo media with moreRooby-Rooby-Roo’s appear to have lower IMDB ratings."
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#rooby_rooby_roo-vs-engagement",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#rooby_rooby_roo-vs-engagement",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "5.5 rooby_rooby_roo vs engagement",
    "text": "5.5 rooby_rooby_roo vs engagement\n\n\nCode\nggplot(tmp_data, aes(x = as.factor(rooby_rooby_roo), y = engagement)) +\n  geom_violin(fill = \"#2b8cbe\", alpha = 0.4, scale = \"width\") +\n   \n    geom_boxplot(fill = \"#2b8cbe\", width = 0.1,\n               outlier.color = \"red\") +\n  scale_x_discrete(breaks = c(0,1,2,3,4,5,6,7,8,9,10)) +\n  labs(x = \"Number of times 'Rooby-Rooby-Roo!' was said\",\n       y = \"IMDB Engagement Score\",\n       title = str_wrap(\"Mapping the distribution of the outcome variable across the IMDB engagement metric.\",width = 80),\n       subtitle = \"Across 535 episodes of Scooby-Doo.\") \n\n\n\n\n\nA significant outlier is seen for the engagement score. This outlier is the live action Scooby-Doo movie, titled “Scooby-Doo”, released in 2002.\n\n\nCode\nscooby_sample |&gt;\n  filter(engagement &gt; 100000) |&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nindex\nimdb\nengagement\nnetwork\nseason\nrooby_rooby_roo\nmotive\nmonster_amount\nmonster_gender\nmonster_real\n\n\n\n\n342\n5.1\n100951\nwarner_brothers\nmovie\n3\nconquer\n10\nfemale\nreal\n\n\n\n\n\n\n\nThis live action movie was considered a cult classic soon after it’s release due to the plot being more intense that the usual fare for Scooby-Doo movies, with a fair share of more adult jokes appearing as well. The movie was rated on IMDB by over 100,000 people, indicating that it stands several degrees above the other Scooby-Doo media in terms of engagement on IMDB."
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#visualizing-the-distribution-of-the-zero-counts-across-the-network-predictor",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#visualizing-the-distribution-of-the-zero-counts-across-the-network-predictor",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "5.6 Visualizing the distribution of the zero counts across the network predictor",
    "text": "5.6 Visualizing the distribution of the zero counts across the network predictor\nThe distribution of zero counts across the network predictor was visualized.\n\n\nCode\ntmp_data &lt;- scooby_sample\n\ntmp_data$network &lt;-  fct_recode(tmp_data$network,\n                               ABC = \"abc\",\n                               \"Cartoon Network\" = \"cartoon_network\",\n                               \"Warner Brothers\" = \"warner_brothers\",\n                               \"Boomerang\" = \"boomerang\",\n                               \"Other Networks\" = \"other_networks\")\ntmp_data |&gt; group_by(network) |&gt;\n    dplyr::summarize(n = n(), \n              percent_0s = round(100*sum(rooby_rooby_roo == 0)/n,1)) |&gt;\n    ggplot(aes(y = network, x = percent_0s)) +\n    geom_label(aes(label = percent_0s ), col = \"#2b8cbe\") +\n    labs(x = \"% of episodes/movies of Scooby-Doo with zero occurences of 'Rooby-Rooby-Roo!'.\",\n         y = \"Network/Producer\",\n         title = str_wrap(\"Visualizing the percentage of episodes/movies with a zero count of `rooby_rooby_roo` across the different networks.\", width = 80))"
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#visualizing-the-distribution-of-the-zero-counts-across-the-season-predictor",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#visualizing-the-distribution-of-the-zero-counts-across-the-season-predictor",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "5.7 Visualizing the distribution of the zero counts across the season predictor",
    "text": "5.7 Visualizing the distribution of the zero counts across the season predictor\nThe distribution of zero counts across the season predictor was visualized.\n\n\nCode\ntmp_data &lt;- scooby_sample\n\ntmp_data$season &lt;-  fct_recode(tmp_data$season,\n                               \"Season 1\" = \"season_1\",\n                               \"Season 2\" = \"season_2\",\n                               \"Season 3 and 4\" = \"season_3_4\",\n                               \"Movie\" = \"movie\",\n                               \"Special / Crossover\" = \"special_crossover\")\ntmp_data |&gt; group_by(season) |&gt;\n    dplyr::summarize(n = n(), \n              percent_0s = round(100*sum(rooby_rooby_roo == 0)/n,1)) |&gt;\n    ggplot(aes(y = season, x = percent_0s)) +\n    geom_label(aes(label = percent_0s), col = \"#2b8cbe\") +\n    labs(x = \"% of episodes/movies of Scooby-Doo with zero occurences of 'Rooby-Rooby-Roo!'.\",\n         y = \"Season / Movie / Special\",\n         title = str_wrap(\"Visualizing the percentage of episodes/movies with a zero count of `rooby_rooby_roo` across different seasons/Movies/Specials of Scooby-Doo.\", width = 60))"
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#preliminary-thoughts",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#preliminary-thoughts",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "5.8 Preliminary Thoughts",
    "text": "5.8 Preliminary Thoughts\nI believe that higher IMDB scores will be associated with higher counts of Rooby-Rooby-Roo, and lower IMDB scores will be associated with lower counts of Rooby-Rooby-Roo.\nI believe that movies will have higher counts of Rooby-Rooby-Roo’s.\nI believe that media produced by Cartoon Network and Boomerang will have lower counts of Rooby-Rooby-Roo’s.\n\n5.8.1 Missingness\nA seperate dataset, containing the identifier variable, along with the outcome and the predictor variables was created and stored in the tibble scooby_q1. This tibble was then assessed for missingness using miss_var_summary.\n\n\nCode\nscooby_q1 &lt;- scooby_sample |&gt;\n  select(index,rooby_rooby_roo, imdb, engagement, network, season)\nscooby_q1 |&gt;\n  miss_var_summary() |&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nvariable\nn_miss\npct_miss\n\n\n\n\nimdb\n15\n2.803738\n\n\nengagement\n15\n2.803738\n\n\nindex\n0\n0.000000\n\n\nrooby_rooby_roo\n0\n0.000000\n\n\nnetwork\n0\n0.000000\n\n\nseason\n0\n0.000000\n\n\n\n\n\n\n\nThe variables imdb and engagement have 15 missing values each.\nThe data can be assumed to be MAR (Missing At Random), since the missing data are not randomly distributed. It cannot be assumed to be MNAR (Missing Not At Random) since there does not appear to be a relationship between the magnitude of a value and it’s missingness (or inclusion in the non-missing data), since there is no preponderance of values with lower or higher magnitude being missing.\nSingle imputation was then performed on the dataset, to account for missingness. The complete dataset, containing the data for both the analyses had it’s missing values imputed.\n\n\n5.8.2 Imputing Missing Variables\nUsing the mice function, missing values in the scooby_q1 tibble were singly imputed and stored in the q1_simp tibble.\n\n\nCode\nset.seed(4322023)\nq1_mice1 &lt;- mice(scooby_q1, m = 10, print = FALSE) \n\n\nWarning: Number of logged events: 1\n\n\nCode\nq1_simp &lt;- mice::complete(q1_mice1)\n\nmiss_var_summary(q1_simp)\n\n\n# A tibble: 6 × 3\n  variable        n_miss pct_miss\n  &lt;chr&gt;            &lt;int&gt;    &lt;dbl&gt;\n1 index                0        0\n2 rooby_rooby_roo      0        0\n3 imdb                 0        0\n4 engagement           0        0\n5 network              0        0\n6 season               0        0\n\n\n\n\n5.8.3 Splitting the data\nThe data was then split into a training (q1_train) and testing (q1_test) dataset using the initial_split function.\n\n\nCode\nset.seed(4322023)\nscoob_split &lt;- initial_split(q1_simp, prop = 0.75)\nq1_train = training(scoob_split)\nq1_test = testing(scoob_split)\ndim(q1_train)\n\n\n[1] 401   6\n\n\n\n\nCode\ndim(q1_test)\n\n\n[1] 134   6\n\n\n\n\n5.8.4 Assessing Collinearity\nThe ggpairs function was used to plot a scatterplot matrix to assess for potential collinearity.\n\n\nCode\nggpairs(q1_simp, columns = c(\"rooby_rooby_roo\", \"imdb\",\n                             \"engagement\", \"network\",\n                             \"season\"))\n\n\n\n\n\nThe variables imdb and engagement exhibit collinearity with each other and the outcome rooby-rooby-roo.\n\n\nCode\nmod_poisson_col &lt;- glm(rooby_rooby_roo ~ imdb +  engagement + network + season,data = q1_train, family = \"poisson\")\n\ncar::vif(mod_poisson_col)\n\n\n               GVIF Df GVIF^(1/(2*Df))\nimdb       1.611302  1        1.269371\nengagement 1.272811  1        1.128189\nnetwork    3.401606  4        1.165361\nseason     3.689599  4        1.177260\n\n\n\n\n5.8.5 Non-Linear Terms\nA Spearman plot was then created to assess if non-linear terms would be needed in the models.\n\n\nCode\nsp2 &lt;- spearman2(rooby_rooby_roo ~ imdb + engagement + network +season, data = q1_simp)\n\nplot(sp2)\n\n\n\n\n\nThe plot suggests the following:\n\nAn interaction term between the main effects of network and imdb, which would add 4 degrees of freedom to the model.\nA restricted cubic spline applied to imdb.\nAn interaction term between imdb and season, which would add 4 degrees of freedom to the model.\n\nThe Poisson model created for the purpose of testing collinearity was then re-fit using Glm() from the rms package to obtain the number of degrees of freedom in the model without adding a non-linear term.\n\n\nCode\nd &lt;- datadist(q1_simp)\noptions(datadist = \"d\")\n\nmod_poisson_rms &lt;- Glm(rooby_rooby_roo ~ imdb + engagement + network +season,\n                     family = poisson(), \n                     data = q1_simp, \n                     x = T, y = T)\n\nmod_poisson_rms\n\n\nGeneral Linear Model\n\nGlm(formula = rooby_rooby_roo ~ imdb + engagement + network + \n    season, family = poisson(), data = q1_simp, x = T, y = T)\n\n                      Model Likelihood    \n                            Ratio Test    \n         Obs 535    LR chi2      90.48    \nResidual d.f.524    d.f.            10    \n         g 0.496    Pr(&gt; chi2) &lt;0.0001    \n\n                         Coef    S.E.   Wald Z Pr(&gt;|Z|)\nIntercept                 1.8019 0.6048  2.98  0.0029  \nimdb                     -0.2779 0.0819 -3.39  0.0007  \nengagement                0.0000 0.0000  0.32  0.7470  \nnetwork=cartoon_network  -0.4735 0.1931 -2.45  0.0142  \nnetwork=warner_brothers  -0.0361 0.1845 -0.20  0.8447  \nnetwork=boomerang        -0.5819 0.2147 -2.71  0.0067  \nnetwork=other_networks   -0.2212 0.1684 -1.31  0.1890  \nseason=season_2          -0.1466 0.1435 -1.02  0.3068  \nseason=season_3_4         0.0265 0.1691  0.16  0.8754  \nseason=movie              0.6389 0.2305  2.77  0.0056  \nseason=special_crossover -0.1181 0.3072 -0.38  0.7008  \n\n\nA restricted cubic spline with 3 knots applied to imdb would add a single degree of freedom to the model, spending a total of 11 degrees of freedom.\n\n\n5.8.6 Fitting a Poisson Model\nA Poisson model, mod_poisson, was fit using glm().\n\n\nCode\nmod_poisson &lt;- glm(rooby_rooby_roo ~ rcs(imdb,3) +  engagement + network + season ,data = q1_train, family = \"poisson\")\n\n\n\n\n5.8.7 Model Coefficients\nThe model’s coefficients were obtained using the tidy function.\n\n\nCode\ntidy(mod_poisson) |&gt;\n  select(term, estimate, std.error, p.value) |&gt;\n  kbl(digits = 3) |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nterm\nestimate\nstd.error\np.value\n\n\n\n\n(Intercept)\n1.719\n1.075\n0.110\n\n\nrcs(imdb, 3)imdb\n-0.274\n0.152\n0.072\n\n\nrcs(imdb, 3)imdb'\n0.067\n0.253\n0.792\n\n\nengagement\n0.000\n0.000\n0.811\n\n\nnetworkcartoon_network\n-0.454\n0.246\n0.065\n\n\nnetworkwarner_brothers\n-0.053\n0.225\n0.814\n\n\nnetworkboomerang\n-0.515\n0.242\n0.034\n\n\nnetworkother_networks\n-0.188\n0.226\n0.404\n\n\nseasonseason_2\n-0.208\n0.173\n0.229\n\n\nseasonseason_3_4\n-0.048\n0.205\n0.816\n\n\nseasonmovie\n0.837\n0.277\n0.003\n\n\nseasonspecial_crossover\n-0.217\n0.353\n0.540\n\n\n\n\n\n\n\n\n\n5.8.8 Poisson Rootogram\nA rootogram was created using the rootogram function from the countreg package.\n\n\nCode\np_root &lt;- topmodels::rootogram(mod_poisson, max = 10,\n                              main = \"Rootogram for the Poisson model\")\n\n\n\n\n\nThe Poisson model (mod_poisson) under-predicts 0 counts, over-predicts 1 counts, and under-predicts 3, 4 and 5 counts, and over-predicts 6 and 7 counts.\n\n\n5.8.9 Storing Training Sample Predictions\nUsing the augment function, the training sample predictions for the mod_poisson model were stored.\n\n\nCode\nmod_poisson_aug &lt;- augment(mod_poisson, q1_train,\n                     type.predict = \"response\")\nmod_poisson_aug |&gt; \n  select(index, rooby_rooby_roo, .fitted) |&gt;\n  head(3)|&gt; kable(digits = 3) |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nindex\nrooby_rooby_roo\n.fitted\n\n\n\n\n144\n1\n0.744\n\n\n435\n0\n0.413\n\n\n345\n1\n0.706\n\n\n\n\n\n\n\n\n\n5.8.10 Summarizing Training Sample mod_poisson Fit\nThe \\(R^2\\), Root mean Squared Error (RMSE) and Mean Absolute Error were then obtained from model mod_poisson.\n\n\nCode\nmets &lt;- metric_set(rsq, rmse, mae)\nmod_poisson_summary &lt;-\nmets(mod_poisson_aug, truth = rooby_rooby_roo, estimate = .fitted) |&gt;\nmutate(model = \"mod_possion\") |&gt; relocate(model)\nmod_poisson_summary |&gt; kable(digits = 3) |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nmodel\n.metric\n.estimator\n.estimate\n\n\n\n\nmod_possion\nrsq\nstandard\n0.228\n\n\nmod_possion\nrmse\nstandard\n0.727\n\n\nmod_possion\nmae\nstandard\n0.525\n\n\n\n\n\n\n\n\n\n5.8.11 Fitting a Negative Binomial Model\nA negative binomial model was fit using the glm.nb function.\n\n\nCode\nmod_nb &lt;- MASS::glm.nb(rooby_rooby_roo ~ rcs(imdb, 3) + engagement + network +season ,data = q1_train)\n\n\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\n\nWarning in theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace =\ncontrol$trace &gt; : iteration limit reached\n\n\n\n\n5.8.12 Model Coefficients\nThe model coefficients were placed in a table.\n\n\nCode\ntidy(mod_nb) |&gt; kable(digits = c(0, 3, 3, 1, 3)) |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.720\n1.075\n1.6\n0.110\n\n\nrcs(imdb, 3)imdb\n-0.274\n0.152\n-1.8\n0.072\n\n\nrcs(imdb, 3)imdb'\n0.067\n0.253\n0.3\n0.792\n\n\nengagement\n0.000\n0.000\n0.2\n0.811\n\n\nnetworkcartoon_network\n-0.454\n0.246\n-1.8\n0.065\n\n\nnetworkwarner_brothers\n-0.053\n0.225\n-0.2\n0.814\n\n\nnetworkboomerang\n-0.515\n0.242\n-2.1\n0.034\n\n\nnetworkother_networks\n-0.188\n0.226\n-0.8\n0.404\n\n\nseasonseason_2\n-0.208\n0.173\n-1.2\n0.229\n\n\nseasonseason_3_4\n-0.048\n0.205\n-0.2\n0.816\n\n\nseasonmovie\n0.837\n0.277\n3.0\n0.003\n\n\nseasonspecial_crossover\n-0.217\n0.353\n-0.6\n0.540\n\n\n\n\n\n\n\n\n\n5.8.13 Negative Binomial Model Rootogram\n\n\nCode\nnb_root &lt;- topmodels::rootogram(mod_nb, max = 10,\nmain = \"Rootogram for the Negative Binomial Model\")\n\n\n\n\n\n\n\n5.8.14 Storing Training Sample Predictions\n\n\nCode\nmod_nb_aug &lt;- augment(mod_nb, q1_train,\n                     type.predict = \"response\")\n\n\nWarning: The `augment()` method for objects of class `negbin` is not maintained by the broom team, and is only supported through the `glm` tidier method. Please be cautious in interpreting and reporting broom output.\n\nThis warning is displayed once per session.\n\n\nCode\nmod_nb_aug |&gt; \n  select(index, rooby_rooby_roo, .fitted) |&gt;\n  head(3)|&gt; kable(digits = 3) |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nindex\nrooby_rooby_roo\n.fitted\n\n\n\n\n144\n1\n0.744\n\n\n435\n0\n0.413\n\n\n345\n1\n0.706\n\n\n\n\n\n\n\n\n\n5.8.15 Summarizing Training Sample mod_nb Fit\n\n\nCode\nmets &lt;- metric_set(rsq, rmse, mae)\nmod_nb_summary &lt;-\nmets(mod_nb_aug, truth = rooby_rooby_roo, estimate = .fitted) |&gt;\nmutate(model = \"mod_nb\") |&gt; relocate(model)\nmod_nb_summary |&gt; kable(digits = 3) |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nmodel\n.metric\n.estimator\n.estimate\n\n\n\n\nmod_nb\nrsq\nstandard\n0.228\n\n\nmod_nb\nrmse\nstandard\n0.727\n\n\nmod_nb\nmae\nstandard\n0.525\n\n\n\n\n\n\n\n\n\n5.8.16 Fitting a Zero-Inflated Poisson Model\nA zero-inflated Poisson model was fit using the zeroinfl function.\n\n\nCode\nmod_zip &lt;- pscl::zeroinfl(rooby_rooby_roo ~ rcs(imdb, 3)  + engagement + network +season,data = q1_train)\n\n\nWarning in value[[3L]](cond): system is computationally singular: reciprocal\ncondition number = 6.56872e-62FALSE\n\n\nCode\nsummary(mod_zip)\n\n\n\nCall:\npscl::zeroinfl(formula = rooby_rooby_roo ~ rcs(imdb, 3) + engagement + \n    network + season, data = q1_train)\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.4387 -0.6795  0.1373  0.4081  4.1033 \n\nCount model coefficients (poisson with log link):\n                          Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)              1.719e+00         NA      NA       NA\nrcs(imdb, 3)imdb        -2.741e-01         NA      NA       NA\nrcs(imdb, 3)imdb'        6.660e-02         NA      NA       NA\nengagement               2.802e-06         NA      NA       NA\nnetworkcartoon_network  -4.538e-01         NA      NA       NA\nnetworkwarner_brothers  -5.290e-02         NA      NA       NA\nnetworkboomerang        -5.150e-01         NA      NA       NA\nnetworkother_networks   -1.883e-01         NA      NA       NA\nseasonseason_2          -2.081e-01         NA      NA       NA\nseasonseason_3_4        -4.779e-02         NA      NA       NA\nseasonmovie              8.372e-01         NA      NA       NA\nseasonspecial_crossover -2.166e-01         NA      NA       NA\n\nZero-inflation model coefficients (binomial with logit link):\n                          Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)             -1.511e+01         NA      NA       NA\nrcs(imdb, 3)imdb        -3.225e+00         NA      NA       NA\nrcs(imdb, 3)imdb'       -1.858e+00         NA      NA       NA\nengagement               3.922e-04         NA      NA       NA\nnetworkcartoon_network   1.566e+00         NA      NA       NA\nnetworkwarner_brothers  -1.227e+00         NA      NA       NA\nnetworkboomerang         1.011e+00         NA      NA       NA\nnetworkother_networks    1.266e+00         NA      NA       NA\nseasonseason_2           4.908e-01         NA      NA       NA\nseasonseason_3_4         4.850e-01         NA      NA       NA\nseasonmovie              3.671e-02         NA      NA       NA\nseasonspecial_crossover  7.360e-01         NA      NA       NA\n\nNumber of iterations in BFGS optimization: 25 \nLog-likelihood:  -392 on 24 Df\n\n\n\n\n5.8.17 Zero-Inflated Poisson Model Rootogram\n\n\nCode\nzip_root &lt;- topmodels::rootogram(mod_zip, max = 10,\nmain = \"Rootogram for the Zero-Inflated Poisson Model\")\n\n\n\n\n\n\n\n5.8.18 Storing Training Sample Predictions\n\n\nCode\nmod_zip_aug &lt;- q1_train |&gt;\n  mutate(\".fitted\" = predict(mod_zip, type = \"response\"),\n         \".resid\" = resid(mod_zip, type = \"response\"))\nmod_zip_aug |&gt; \n  select(index, rooby_rooby_roo, .fitted, .resid) |&gt;\n  head(3)\n\n\n  index rooby_rooby_roo   .fitted     .resid\n1   144               1 0.7443436  0.2556564\n2   435               0 0.4132138 -0.4132138\n3   345               1 0.7061787  0.2938213\n\n\n\n\n5.8.19 Summarizing Training Sample mod_nb Fit\n\n\nCode\nmets &lt;- metric_set(rsq, rmse, mae)\nmod_zip_summary &lt;-\nmets(mod_zip_aug, truth = rooby_rooby_roo, estimate = .fitted) |&gt;\nmutate(model = \"mod_zip\") |&gt; relocate(model)\nmod_zip_summary |&gt; kable(digits = 3) |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nmodel\n.metric\n.estimator\n.estimate\n\n\n\n\nmod_zip\nrsq\nstandard\n0.228\n\n\nmod_zip\nrmse\nstandard\n0.727\n\n\nmod_zip\nmae\nstandard\n0.525\n\n\n\n\n\n\n\n\n\n5.8.20 Fitting a Zero-Inflated Negative Binomial Model\nA zero-inflated Negative Binomial Model was then fit.\n\n\nCode\nmod_zinb &lt;- pscl::zeroinfl(rooby_rooby_roo ~ rcs(imdb, 3)  + engagement + network + season, dist = \"negbin\", data = q1_train)\n\n\nWarning in value[[3L]](cond): system is computationally singular: reciprocal\ncondition number = 2.1577e-46FALSE\n\n\n\n\nCode\nsummary(mod_zinb)\n\n\n\nCall:\npscl::zeroinfl(formula = rooby_rooby_roo ~ rcs(imdb, 3) + engagement + \n    network + season, data = q1_train, dist = \"negbin\")\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.3855 -0.6603  0.1403  0.3362  4.1881 \n\nCount model coefficients (negbin with log link):\n                          Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)              1.257e+00         NA      NA       NA\nrcs(imdb, 3)imdb        -2.070e-01         NA      NA       NA\nrcs(imdb, 3)imdb'       -4.830e-02         NA      NA       NA\nengagement               5.965e-06         NA      NA       NA\nnetworkcartoon_network   3.438e-01         NA      NA       NA\nnetworkwarner_brothers   9.493e-02         NA      NA       NA\nnetworkboomerang        -3.765e-01         NA      NA       NA\nnetworkother_networks   -1.481e-01         NA      NA       NA\nseasonseason_2          -1.322e-01         NA      NA       NA\nseasonseason_3_4        -8.629e-02         NA      NA       NA\nseasonmovie              6.345e-01         NA      NA       NA\nseasonspecial_crossover -4.164e-01         NA      NA       NA\nLog(theta)               1.663e+01         NA      NA       NA\n\nZero-inflation model coefficients (binomial with logit link):\n                          Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)             -72.139488         NA      NA       NA\nrcs(imdb, 3)imdb          5.234140         NA      NA       NA\nrcs(imdb, 3)imdb'        -4.966800         NA      NA       NA\nengagement               -0.006803         NA      NA       NA\nnetworkcartoon_network   36.255230         NA      NA       NA\nnetworkwarner_brothers   -7.081453         NA      NA       NA\nnetworkboomerang         32.389288         NA      NA       NA\nnetworkother_networks   -31.492774         NA      NA       NA\nseasonseason_2            1.735211         NA      NA       NA\nseasonseason_3_4         -4.228295         NA      NA       NA\nseasonmovie               2.998970         NA      NA       NA\nseasonspecial_crossover -96.148898         NA      NA       NA\n\nTheta = 16665909.2848 \nNumber of iterations in BFGS optimization: 103 \nLog-likelihood: -384.7 on 25 Df\n\n\n\n\n5.8.21 Rootogram\n\n\nCode\nzinb_root &lt;- topmodels::rootogram(mod_zinb, max = 10,\nmain = \"Rootogram for Zero-Inflated Negative Binomial Model\")\n\n\n\n\n\n\n\n5.8.22 Storing Training Sample Predictions\n\n\nCode\nmod_zinb_aug &lt;- q1_train |&gt;\n  mutate(\".fitted\" = predict(mod_zinb, type = \"response\"),\n         \".resid\" = resid(mod_zinb, type = \"response\"))\nmod_zinb_aug |&gt; \n  select(index, rooby_rooby_roo, .fitted, .resid) |&gt;\n  head(3)\n\n\n  index rooby_rooby_roo   .fitted     .resid\n1   144               1 0.7525684  0.2474316\n2   435               0 0.4927507 -0.4927507\n3   345               1 0.8279808  0.1720192\n\n\n\n\n5.8.23 Summarizing Training Sample mod_nb Fit\n\n\nCode\nmets &lt;- metric_set(rsq, rmse, mae)\nmod_zinb_summary &lt;-\nmets(mod_zinb_aug, truth = rooby_rooby_roo, estimate = .fitted) |&gt;\nmutate(model = \"mod_zinb\") |&gt; relocate(model)\nmod_zinb_summary |&gt; kable(digits = 3) |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nmodel\n.metric\n.estimator\n.estimate\n\n\n\n\nmod_zinb\nrsq\nstandard\n0.289\n\n\nmod_zinb\nrmse\nstandard\n0.698\n\n\nmod_zinb\nmae\nstandard\n0.499\n\n\n\n\n\n\n\n\n\n5.8.24 Comparing Rootograms\n\n\nCode\nplot(p_root)\n\n\n\n\n\n\n\nCode\nplot(nb_root)\n\n\n\n\n\n\n\nCode\nplot(zip_root)\n\n\n\n\n\n\n\nCode\nplot(zinb_root)\n\n\n\n\n\n\n\n5.8.25 Vuong’s Procedure Comparing the Negative Binomial Model and the Poisson Model\n\n\nCode\nvuong(mod_nb, mod_poisson)\n\n\nVuong Non-Nested Hypothesis Test-Statistic: \n(test-statistic is asymptotically distributed N(0,1) under the\n null that the models are indistinguishible)\n-------------------------------------------------------------\n              Vuong z-statistic             H_A  p-value\nRaw                    -2.29614 model2 &gt; model1 0.010834\nAIC-corrected          -2.29614 model2 &gt; model1 0.010834\nBIC-corrected          -2.29614 model2 &gt; model1 0.010834\n\n\n\n\n5.8.26 Vuong’s Procedure Comparing the Zero-Inflated Poisson Model and the Poisson Model\n\n\nCode\nvuong(mod_zip, mod_poisson)\n\n\nVuong Non-Nested Hypothesis Test-Statistic: \n(test-statistic is asymptotically distributed N(0,1) under the\n null that the models are indistinguishible)\n-------------------------------------------------------------\n              Vuong z-statistic             H_A p-value\nRaw               -1.996332e-01 model2 &gt; model1 0.42088\nAIC-corrected     -8.766005e+05 model2 &gt; model1 &lt; 2e-16\nBIC-corrected     -2.627154e+06 model2 &gt; model1 &lt; 2e-16\n\n\n\n\n5.8.27 Vuong’s Procedure Comparing the ZINB Model and the Poisson Model\n\n\nCode\nvuong(mod_zinb, mod_poisson)\n\n\nVuong Non-Nested Hypothesis Test-Statistic: \n(test-statistic is asymptotically distributed N(0,1) under the\n null that the models are indistinguishible)\n-------------------------------------------------------------\n              Vuong z-statistic             H_A   p-value\nRaw                    1.609132 model1 &gt; model2  0.053794\nAIC-corrected         -1.030014 model2 &gt; model1  0.151502\nBIC-corrected         -6.300337 model2 &gt; model1 1.485e-10\n\n\n\n\n5.8.28 Vuong’s Procedure Comparing the ZINB Model and the Negative Binomial Model\n\n\nCode\nvuong(mod_zinb, mod_nb)\n\n\nVuong Non-Nested Hypothesis Test-Statistic: \n(test-statistic is asymptotically distributed N(0,1) under the\n null that the models are indistinguishible)\n-------------------------------------------------------------\n              Vuong z-statistic             H_A    p-value\nRaw                    1.609863 model1 &gt; model2   0.053714\nAIC-corrected         -1.029515 model2 &gt; model1   0.151619\nBIC-corrected         -6.300301 model2 &gt; model1 1.4853e-10\n\n\n\n\n5.8.29 Testing Sample Validations\nTesting sample validations were obtained by using the predict function.\n\n\nCode\ntest_poisson &lt;- predict(mod_poisson, newdata = q1_test,\ntype.predict = \"response\")\ntest_nb &lt;- predict(mod_nb, newdata = q1_test,\ntype.predict = \"response\")\ntest_zip &lt;- predict(mod_zip, newdata = q1_test,\ntype.predict = \"response\")\ntest_zinb &lt;- predict(mod_zinb, newdata = q1_test,\ntype.predict = \"response\")\n\n\n\n\nCode\ntest_res &lt;- bind_cols(q1_test,\n                      pre_poisson = test_poisson, \n                      pre_nb = test_nb,\n                      pre_zip = test_zip, \n                      pre_zinb = test_zinb)\nnames(test_res)\n\n\n [1] \"index\"           \"rooby_rooby_roo\" \"imdb\"            \"engagement\"     \n [5] \"network\"         \"season\"          \"pre_poisson\"     \"pre_nb\"         \n [9] \"pre_zip\"         \"pre_zinb\"       \n\n\nThe metrics of the testing sample’s fit were obtained.\n\n\nCode\npoisson_sum &lt;- mets(test_res, truth = rooby_rooby_roo, estimate = pre_poisson) |&gt;\n  mutate(model = \"mod_poisson\")\n\nnb_sum &lt;- mets(test_res, truth = rooby_rooby_roo, estimate = pre_nb) |&gt;\nmutate(model = \"mod_nb\")\n\nzip_sum &lt;- mets(test_res, truth = rooby_rooby_roo, estimate = pre_zip) |&gt;\nmutate(model = \"mod_zip\")\n\nzinb_sum &lt;- mets(test_res, truth = rooby_rooby_roo, estimate = pre_zinb) |&gt;\nmutate(model = \"mod_zinb\")\n\ntest_sum &lt;- bind_rows(poisson_sum, nb_sum, zip_sum, zinb_sum)|&gt;\n  pivot_wider(names_from = model,\n              values_from = .estimate)\ntest_sum |&gt;\nselect(-.estimator) |&gt; kable(dig = 3) |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\n.metric\nmod_poisson\nmod_nb\nmod_zip\nmod_zinb\n\n\n\n\nrsq\n0.210\n0.210\n0.077\n0.168\n\n\nrmse\n1.345\n1.345\n0.705\n0.686\n\n\nmae\n1.210\n1.210\n0.535\n0.521\n\n\n\n\n\n\n\n\n\n5.8.30 Selecting a Final Model\nThe final winning model was the Poisson model, because:\n\nIt has the highest testing sample \\(R^2\\).\nFrom the Vuong test, the other models do not appear to be a significant improvement over the Poisson model.\nThe rootograms for the Poisson model are not very different from the other three models.\n\nA poisson model called mod_poisson_imp was fit on the entire imputed dataset.\n\n\nCode\nmod_poisson_imp &lt;- glm(rooby_rooby_roo ~ rcs(imdb,3) +  engagement + network + season ,data = q1_simp, family = \"poisson\")\nsummary(mod_poisson_imp)\n\n\n\nCall:\nglm(formula = rooby_rooby_roo ~ rcs(imdb, 3) + engagement + network + \n    season, family = \"poisson\", data = q1_simp)\n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)              1.782e+00  9.109e-01   1.956  0.05047 . \nrcs(imdb, 3)imdb        -2.750e-01  1.297e-01  -2.121  0.03393 * \nrcs(imdb, 3)imdb'       -6.195e-03  2.095e-01  -0.030  0.97641   \nengagement               1.803e-06  5.675e-06   0.318  0.75069   \nnetworkcartoon_network  -4.710e-01  2.116e-01  -2.225  0.02606 * \nnetworkwarner_brothers  -3.598e-02  1.845e-01  -0.195  0.84539   \nnetworkboomerang        -5.810e-01  2.166e-01  -2.683  0.00731 **\nnetworkother_networks   -2.185e-01  1.921e-01  -1.137  0.25555   \nseasonseason_2          -1.463e-01  1.438e-01  -1.017  0.30907   \nseasonseason_3_4         2.636e-02  1.692e-01   0.156  0.87621   \nseasonmovie              6.397e-01  2.319e-01   2.758  0.00581 **\nseasonspecial_crossover -1.186e-01  3.077e-01  -0.385  0.69998   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 469.87  on 534  degrees of freedom\nResidual deviance: 379.39  on 523  degrees of freedom\nAIC: 1084.7\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\n5.8.31 Rootogram\nThe rootogram for the final model was obtained.\n\n\nCode\npoisson_imp_root &lt;- topmodels::rootogram(mod_poisson_imp, max = 10,\nmain = \"Rootogram for Final Poisson Model\")\n\n\n\n\n\n\n\n5.8.32 Storing Fitted Values and Residuals\nThe fitted values were stored and the \\(R^2\\) of the final was obtained.\n\n\nCode\nsm_poiss1 &lt;- augment(mod_poisson_imp, q1_simp,\n                     type.predict = \"response\")\n\nsm_poiss1 |&gt; \n    select(rooby_rooby_roo, .fitted) |&gt;\n    head()\n\n\n# A tibble: 6 × 2\n  rooby_rooby_roo .fitted\n            &lt;dbl&gt;   &lt;dbl&gt;\n1               1   0.512\n2               0   0.512\n3               0   0.526\n4               0   0.557\n5               0   0.606\n6               1   0.470\n\n\n\n\nCode\n# The correlation of observed and fitted values\n(poiss_r &lt;- with(sm_poiss1, cor(rooby_rooby_roo, .fitted)))\n\n\n[1] 0.4658293\n\n\nCode\npoiss_r^2\n\n\n[1] 0.2169969\n\n\nThe glance function was used to obtain the log-likelihood value, and the AIC and the BIC.\n\n\nCode\nglance(mod_poisson_imp) |&gt; round(3)\n\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1          470.     534  -530. 1085. 1136.     379.         523   535\n\n\n\n\n\nLog-Likelihood\nAIC\nBIC\n\\(R^2\\)\n\n\n\n\n530.352\n1084.70\n1136.10\n0.22\n\n\n\nThe \\(R^2\\) is 0.22, indicating that the model does not do a very good job of explaining the variation seen in the data.\n\n\n5.8.33 Diagnostic Plots\nA plot of the residuals versus the fitted values of rooby-rooby-roo for the final model were obtained.\n\n\nCode\nggplot(sm_poiss1, aes(x = .fitted, y = .resid)) +\n    geom_point() +\n    labs(title = \"Residuals vs. Fitted `rooby_rooby_roo`\",\n         subtitle = \"Original Poisson Regression model\")\n\n\n\n\n\nThe pattern seen here corroborates the pattern seen in the rootogram, with an underfitting of 0 values and an overfitting of 1 values.\nglm.diag.plots was used to obtain the other diagnostic plots.\n\n\nCode\nglm.diag.plots(mod_poisson_imp)\n\n\n\n\n\nThe plots show that the model residuals follow a somewhat normal distribution, with some significant outliers. There are no points that exert an undue influence on the model.\n\n\nCode\npar(mfrow = c(2,2))\nplot(mod_poisson_imp, which = 1)\nplot(mod_poisson_imp, which = 2)\nplot(mod_poisson_imp, which = 3)\nplot(mod_poisson_imp, which = 4)\n\n\n\n\n\n326 shows up as a outlier in all of these plots, along with 342."
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#outlier-diagnostics",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#outlier-diagnostics",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "5.9 Outlier Diagnostics",
    "text": "5.9 Outlier Diagnostics\n\n\nCode\nscooby_raw |&gt;\n  filter(index == 326) |&gt;\n  select(title,rooby_rooby_roo, engagement, imdb, network, season) |&gt;\n  kable() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\ntitle\nrooby_rooby_roo\nengagement\nimdb\nnetwork\nseason\n\n\n\n\nDawn of the Spooky Shuttle Scare\n1\n37\n7.1\nABC\n3\n\n\n\n\n\n\n\nThe first outlier is an episode from Season 3 produced by ABC, with a single rooby-rooby-roo.\n\n\nCode\nscooby_raw |&gt;\n  filter(index == 342) |&gt;\n  select(title,rooby_rooby_roo, engagement, imdb, network, season) |&gt;\n  kable(dig = 3) |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\ntitle\nrooby_rooby_roo\nengagement\nimdb\nnetwork\nseason\n\n\n\n\nScooby-Doo\n3\n100951\n5.1\nWarner Bros. Picture\nMovie\n\n\n\n\n\n\n\nThe second outlier is the live action Scooby-Doo movie, titled “Scooby-Doo” which was very popular at the time it released, and was considered a cult classic. Over 10,000 people voted to rate the movie on IMDB, which is higher than the engagement value for the other Scooby-Doo media."
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#testing-for-overdispersion",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#testing-for-overdispersion",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "5.10 Testing for Overdispersion",
    "text": "5.10 Testing for Overdispersion\nThe Poisson distribution assumes that the variance of the distribution should be equal to it’s mean. Overdispersion occurs when the observed variance of the data is more than what would be expected under a Poisson distributio, and can indicate variability in the data that is not being explained by the model.\n\n\nCode\ncat(\"overdispersion ratio is \", sum(z^2)/ (n - k), \"\\n\")\n\n\noverdispersion ratio is  0.6754214 \n\n\nCode\ncat(\"p value of overdispersion test is \", \n    pchisq(sum(z^2), df = n-k, lower.tail = FALSE), \"\\n\")\n\n\np value of overdispersion test is  1 \n\n\nIt does not look like the data has the issue of overdispersion from the large p value, and the data can be assumed to have a Poisson distribution."
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#model-coefficients-2",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#model-coefficients-2",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "5.11 Model Coefficients",
    "text": "5.11 Model Coefficients\nThe final model’s coefficients were obtained.\n\n\nCode\ntidy(mod_poisson_imp) |&gt;\n  select(term, estimate, std.error, p.value) |&gt;\n  kbl(digits = 3) |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nterm\nestimate\nstd.error\np.value\n\n\n\n\n(Intercept)\n1.782\n0.911\n0.050\n\n\nrcs(imdb, 3)imdb\n-0.275\n0.130\n0.034\n\n\nrcs(imdb, 3)imdb'\n-0.006\n0.210\n0.976\n\n\nengagement\n0.000\n0.000\n0.751\n\n\nnetworkcartoon_network\n-0.471\n0.212\n0.026\n\n\nnetworkwarner_brothers\n-0.036\n0.185\n0.845\n\n\nnetworkboomerang\n-0.581\n0.217\n0.007\n\n\nnetworkother_networks\n-0.218\n0.192\n0.256\n\n\nseasonseason_2\n-0.146\n0.144\n0.309\n\n\nseasonseason_3_4\n0.026\n0.169\n0.876\n\n\nseasonmovie\n0.640\n0.232\n0.006\n\n\nseasonspecial_crossover\n-0.119\n0.308\n0.700\n\n\n\n\n\n\n\n\n5.11.1 Odds Ratios\nThis was then converted into Odds ratios.\n\n\nCode\ntidy(mod_poisson_imp, exponentiate = TRUE, conf.int = TRUE, conf.level = 0.95) |&gt;\n  select(term, estimate, std.error, \n         low95 = conf.low, high95 = conf.high, p = p.value) |&gt;\n  kable(digits = 3)|&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nterm\nestimate\nstd.error\nlow95\nhigh95\np\n\n\n\n\n(Intercept)\n5.940\n0.911\n0.968\n34.596\n0.050\n\n\nrcs(imdb, 3)imdb\n0.760\n0.130\n0.591\n0.983\n0.034\n\n\nrcs(imdb, 3)imdb'\n0.994\n0.210\n0.653\n1.485\n0.976\n\n\nengagement\n1.000\n0.000\n1.000\n1.000\n0.751\n\n\nnetworkcartoon_network\n0.624\n0.212\n0.407\n0.935\n0.026\n\n\nnetworkwarner_brothers\n0.965\n0.185\n0.665\n1.372\n0.845\n\n\nnetworkboomerang\n0.559\n0.217\n0.359\n0.842\n0.007\n\n\nnetworkother_networks\n0.804\n0.192\n0.547\n1.162\n0.256\n\n\nseasonseason_2\n0.864\n0.144\n0.648\n1.140\n0.309\n\n\nseasonseason_3_4\n1.027\n0.169\n0.730\n1.418\n0.876\n\n\nseasonmovie\n1.896\n0.232\n1.205\n2.993\n0.006\n\n\nseasonspecial_crossover\n0.888\n0.308\n0.461\n1.556\n0.700"
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#the-outcome-1",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#the-outcome-1",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "6.1 The Outcome",
    "text": "6.1 The Outcome\nThe outcome variable is motive, describing the motive of the antagonist of each episode/movie of Scooby-Doo.\n\n\nCode\nscooby_sample |&gt;\n  tabyl(motive) |&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nmotive\nn\npercent\n\n\n\n\ncompetition\n168\n0.3140187\n\n\ntheft\n124\n0.2317757\n\n\nindirect_self_interest\n118\n0.2205607\n\n\ntreasure\n54\n0.1009346\n\n\nconquer\n42\n0.0785047\n\n\nmisc_motive\n29\n0.0542056"
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#the-predictors-1",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#the-predictors-1",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "6.2 The Predictors",
    "text": "6.2 The Predictors\nThe predictors are monster_amount, monster_gender, monster_real.\n\nmonster_amount is a continuous quantitative variable describing the number of monsters that appear in each episode/movie.\n\n\n\nCode\ndescribe(scooby_sample$monster_amount)\n\n\nscooby_sample$monster_amount \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     535        0       16    0.739    1.944    1.781        0        1 \n     .25      .50      .75      .90      .95 \n       1        1        2        4        7 \n                                                                            \nValue          0     1     2     3     4     5     6     7     8     9    10\nFrequency     30   341    65    32    23     7     5     7     6     4     3\nProportion 0.056 0.637 0.121 0.060 0.043 0.013 0.009 0.013 0.011 0.007 0.006\n                                        \nValue         11    12    13    15    19\nFrequency      4     3     3     1     1\nProportion 0.007 0.006 0.006 0.002 0.002\n\nFor the frequency table, variable is rounded to the nearest 0\n\n\n\nmonster_gender describes if the monster/monsters present in the episode were either male or female(in terms of the pronouns the Mystery Inc. gang used to refer to them). This variable had a large number of levels, and was collapsed to male if all of the monsters were male, and female if any of the monsters present in the episode/movie were female.\n\n\n\nCode\nscooby_sample |&gt;\n  tabyl(monster_gender) |&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nmonster_gender\nn\npercent\nvalid_percent\n\n\n\n\nmale\n427\n0.7981308\n0.8455446\n\n\nfemale\n78\n0.1457944\n0.1544554\n\n\nNA\n30\n0.0560748\nNA\n\n\n\n\n\n\n\n\nmonster_real describes if the monster was actually real, or someone in a costume, or a mind-controlled person, etc.\n\n\n\nCode\nscooby_sample |&gt;\n  tabyl(monster_real) |&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nmonster_real\nn\npercent\nvalid_percent\n\n\n\n\nnot_real\n402\n0.7514019\n0.7960396\n\n\nreal\n103\n0.1925234\n0.2039604\n\n\nNA\n30\n0.0560748\nNA"
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#missingness-1",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#missingness-1",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "6.3 Missingness",
    "text": "6.3 Missingness\nA seperate dataset, containing the identifier variable, along with the outcome and the predictor variables was created and stored in the tibble scooby_q2. This tibble was then assessed for missingness using miss_var_summary.\n\n\nCode\nscooby_q2 &lt;- scooby_sample |&gt;\n  select(index,motive, monster_amount, monster_gender, monster_real)\nscooby_q2 |&gt;\n  miss_var_summary() |&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nvariable\nn_miss\npct_miss\n\n\n\n\nmonster_gender\n30\n5.607477\n\n\nmonster_real\n30\n5.607477\n\n\nindex\n0\n0.000000\n\n\nmotive\n0\n0.000000\n\n\nmonster_amount\n0\n0.000000\n\n\n\n\n\n\n\nThe variables monster_gender and monster_real have 15 missing values each.\nThe data can be assumed to be MAR (Missing At Random), since the missing data are not randomly distributed. It cannot be assumed to be MNAR (Missing Not At Random) since there does not appear to be a relationship between the magnitude of a value and it’s missingness (or inclusion in the non-missing data), since there is no preponderance of values with lower or higher magnitude being missing.\nSingle imputation was then performed on the dataset, to account for missingness. The complete dataset, containing the data for both the analyses had it’s missing values imputed."
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#imputing-missing-variables-1",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#imputing-missing-variables-1",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "6.4 Imputing Missing Variables",
    "text": "6.4 Imputing Missing Variables\n\n\nCode\nset.seed(4322023)\nq2_mice1 &lt;- mice(scooby_q2, m = 10, print = FALSE) \n\n\nWarning: Number of logged events: 1\n\n\nCode\nq2_simp &lt;- mice::complete(q2_mice1)\n\nmiss_var_summary(q2_simp)\n\n\n# A tibble: 5 × 3\n  variable       n_miss pct_miss\n  &lt;chr&gt;           &lt;int&gt;    &lt;dbl&gt;\n1 index               0        0\n2 motive              0        0\n3 monster_amount      0        0\n4 monster_gender      0        0\n5 monster_real        0        0"
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#visualizing-data-exploration",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#visualizing-data-exploration",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "6.5 Visualizing Data Exploration",
    "text": "6.5 Visualizing Data Exploration\n\n\nCode\nmosaic::favstats(monster_amount ~ motive, data = q2_simp) |&gt;\n  kable(digits = 2) |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\n\n\n\n\nmotive\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n\ncompetition\n0\n1\n1.0\n1.00\n19\n1.58\n2.09\n168\n0\n\n\ntheft\n0\n1\n1.0\n1.00\n15\n1.56\n1.82\n124\n0\n\n\nindirect_self_interest\n0\n1\n1.0\n2.00\n9\n1.60\n1.34\n118\n0\n\n\ntreasure\n1\n1\n1.0\n2.00\n7\n1.70\n1.28\n54\n0\n\n\nconquer\n1\n3\n5.5\n9.75\n13\n6.17\n3.96\n42\n0\n\n\nmisc_motive\n0\n0\n1.0\n2.00\n5\n1.38\n1.42\n29\n0\n\n\n\n\n\n\n\n\n6.5.1 motive vs monster_amount\n\n\nCode\ntmp_data_2 &lt;- q2_simp\n\ntmp_data_2$motive &lt;-  fct_recode(tmp_data_2$motive,\n                               \"Competition\" = \"competition\",\n                               \"Theft\" = \"theft\",\n                               \"Indirect Self Interest\" = \"indirect_self_interest\",\n                               \"Treasure\" = \"treasure\",\n                               \"Conquer\" = \"conquer\",\n                               \"Miscellaneous Motives\" = \"misc_motive\")\nggplot(tmp_data_2, aes(x = motive, y = monster_amount, fill = motive))  +\n  geom_violin(fill = \"#2b8cbe\", alpha = 0.3, scale = \"width\")+ \n  geom_boxplot(fill = \"#2b8cbe\", width = 0.1,\n               outlier.color = \"red\")+\n  labs(y = \"Number of monsters per episode/movie\",\n       x = \"Motive of the antagonist\",\n       title = str_wrap(\"Mapping the motives of the major antagonist of every episode/movie against the number of monsters across the various seasons/movies of Scooby-Doo.\",width = 80),\n       subtitle = \"Across 535 episodes of Scooby-Doo.\")  \n\n\n\n\n\nAntagonists with a motivation to conquer appear to star in episodes with more monsters, compared to antagonists with other motivations.\n\n\n6.5.2 motive vs monster_real\n\n\nCode\ntmp_data_2 &lt;- tmp_data_2 |&gt;\n  mutate(monster_real = fct_recode(monster_real,\n                               \"Monster was not real\" = \"not_real\",\n                               \"Monster was real\" = \"real\"))\n\nggplot(tmp_data_2, aes(motive))+\n  geom_bar(fill = \"#2b8cbe\") + \n  facet_wrap(~ monster_real)+\n  theme(axis.text.x = element_text(angle = 45, hjust=1)) +\n  labs(y = \"Count\",\n       x = \"Motive of the antagonist\",\n       title = str_wrap(\"Mapping the motives of the major antagonist of every episode/movie against the nature of reality of the monsters across the various seasons/movies of Scooby-Doo.\",width = 80),\n       subtitle = \"Across 535 episodes of Scooby-Doo.\") \n\n\n\n\n\nIt appears that for episodes with monsters that are not real, competition is the most numerous motivation. In episodes with monsters that are real, conquering appears to be the most numerous motivation.\n\n\n6.5.3 motive vs monster_gender\n\n\nCode\ntmp_data_2 &lt;- tmp_data_2 |&gt;\n  mutate(monster_gender = fct_recode(monster_gender,\n                               \"Male\" = \"male\",\n                               \"Female\" = \"female\"))\nggplot(tmp_data_2, aes(motive))+\n  geom_bar(fill = \"#2b8cbe\") + \n  facet_wrap(~ monster_gender)+\n  theme(axis.text.x = element_text(angle = 45, hjust=1)) + \n  labs(y = \"Count\",\n       x = \"Motive of the antagonist\",\n       title = str_wrap(\"Mapping the motives of the major antagonist of every episode/movie against the gender of the monsters across the various seasons/movies of Scooby-Doo.\",width = 80),\n       subtitle = \"Across 535 episodes of Scooby-Doo.\") \n\n\n\n\n\nEpisodes with only male monsters have competition as the most numerous outcome, and episodes that have female monsters also have competition as the most numerous outcome."
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#assessing-collinearity-1",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#assessing-collinearity-1",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "6.6 Assessing collinearity",
    "text": "6.6 Assessing collinearity\nThe ggpairs function was used to plot a scatterplot matrix to assess for potential collinearity.\n\n\nCode\nggpairs(q2_simp, columns = c(\"motive\", \"monster_real\",\n                             \"monster_gender\", \"monster_amount\"))\n\n\n\n\n\nNo collinearity was seen from the scatterplot."
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#non-linear-terms-1",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#non-linear-terms-1",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "6.7 Non-Linear Terms",
    "text": "6.7 Non-Linear Terms\nA Spearman plot was then created to assess if non-linear terms would be needed in the models.\n\n\nCode\nsp2 &lt;- spearman2(motive ~ monster_amount + monster_real + monster_gender, data = q2_simp)\n\nplot(sp2)\n\n\n\n\n\nThe Spearman plot shows that if adding non-linear terms were to improve the fit of the model, the term most likely to add the most predictive power with a non-linear term applied to it would be monster_real.\nHowever, no non-linear term was selected, given that adding non-linear terms would result in spending of too many degrees of freedom."
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#fitting-the-multinomial-model",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#fitting-the-multinomial-model",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "6.8 Fitting the Multinomial Model",
    "text": "6.8 Fitting the Multinomial Model\nA multinomial model was fit using the multinom function and was designated as multinom_model.\n\n\nCode\nmultinom_model &lt;-multinom(motive ~ monster_amount + monster_real + monster_gender ,data=q2_simp) \n\n\n# weights:  30 (20 variable)\ninitial  value 958.591316 \niter  10 value 806.559330\niter  20 value 761.294580\niter  30 value 760.258265\nfinal  value 760.257932 \nconverged\n\n\nCode\nsummary(multinom_model)\n\n\nCall:\nmultinom(formula = motive ~ monster_amount + monster_real + monster_gender, \n    data = q2_simp)\n\nCoefficients:\n                       (Intercept) monster_amount monster_realreal\ntheft                   -0.2984806    -0.03866893        0.3795771\nindirect_self_interest  -0.4247981    -0.13144830        1.9282423\ntreasure                -1.2076838     0.08470106       -0.6903529\nconquer                 -4.2541198     0.18107415        4.7542887\nmisc_motive             -2.1112197    -0.46152297        3.2876692\n                       monster_genderfemale\ntheft                            0.20634400\nindirect_self_interest           0.06788956\ntreasure                        -0.27912724\nconquer                         -0.08231355\nmisc_motive                      1.11501804\n\nStd. Errors:\n                       (Intercept) monster_amount monster_realreal\ntheft                    0.1697542     0.08281784        0.4887687\nindirect_self_interest   0.1767427     0.08597467        0.4165262\ntreasure                 0.2190608     0.09981712        0.8402710\nconquer                  0.6053196     0.08056674        0.7138281\nmisc_motive              0.3661798     0.19340172        0.5299548\n                       monster_genderfemale\ntheft                             0.3499198\nindirect_self_interest            0.3704725\ntreasure                          0.5111410\nconquer                           0.5731102\nmisc_motive                       0.5255567\n\nResidual Deviance: 1520.516 \nAIC: 1560.516"
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#model-summary",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#model-summary",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "6.9 Model Summary",
    "text": "6.9 Model Summary\n\n\nCode\ntidy(multinom_model, exponentiate = FALSE) |&gt; kable(digits = 3)|&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\ny.level\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\ntheft\n(Intercept)\n-0.298\n0.170\n-1.758\n0.079\n\n\ntheft\nmonster_amount\n-0.039\n0.083\n-0.467\n0.641\n\n\ntheft\nmonster_realreal\n0.380\n0.489\n0.777\n0.437\n\n\ntheft\nmonster_genderfemale\n0.206\n0.350\n0.590\n0.555\n\n\nindirect_self_interest\n(Intercept)\n-0.425\n0.177\n-2.403\n0.016\n\n\nindirect_self_interest\nmonster_amount\n-0.131\n0.086\n-1.529\n0.126\n\n\nindirect_self_interest\nmonster_realreal\n1.928\n0.417\n4.629\n0.000\n\n\nindirect_self_interest\nmonster_genderfemale\n0.068\n0.370\n0.183\n0.855\n\n\ntreasure\n(Intercept)\n-1.208\n0.219\n-5.513\n0.000\n\n\ntreasure\nmonster_amount\n0.085\n0.100\n0.849\n0.396\n\n\ntreasure\nmonster_realreal\n-0.690\n0.840\n-0.822\n0.411\n\n\ntreasure\nmonster_genderfemale\n-0.279\n0.511\n-0.546\n0.585\n\n\nconquer\n(Intercept)\n-4.254\n0.605\n-7.028\n0.000\n\n\nconquer\nmonster_amount\n0.181\n0.081\n2.248\n0.025\n\n\nconquer\nmonster_realreal\n4.754\n0.714\n6.660\n0.000\n\n\nconquer\nmonster_genderfemale\n-0.082\n0.573\n-0.144\n0.886\n\n\nmisc_motive\n(Intercept)\n-2.111\n0.366\n-5.766\n0.000\n\n\nmisc_motive\nmonster_amount\n-0.462\n0.193\n-2.386\n0.017\n\n\nmisc_motive\nmonster_realreal\n3.288\n0.530\n6.204\n0.000\n\n\nmisc_motive\nmonster_genderfemale\n1.115\n0.526\n2.122\n0.034"
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#fitting-a-proportional-odds-model",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#fitting-a-proportional-odds-model",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "6.10 Fitting a Proportional Odds Model",
    "text": "6.10 Fitting a Proportional Odds Model\nA proportional odds model was fit using the polr function.\n\n\nCode\npolr_model &lt;- polr(motive~ monster_real + monster_amount + monster_gender ,data=q2_simp, Hess = TRUE)\n\nsummary(polr_model)\n\n\nCall:\npolr(formula = motive ~ monster_real + monster_amount + monster_gender, \n    data = q2_simp, Hess = TRUE)\n\nCoefficients:\n                       Value Std. Error t value\nmonster_realreal     1.95262    0.23359  8.3593\nmonster_amount       0.06793    0.03869  1.7558\nmonster_genderfemale 0.07528    0.21936  0.3432\n\nIntercepts:\n                                Value   Std. Error t value\ncompetition|theft               -0.4092  0.1140    -3.5890\ntheft|indirect_self_interest     0.6615  0.1148     5.7641\nindirect_self_interest|treasure  1.8288  0.1406    13.0087\ntreasure|conquer                 2.6939  0.1795    15.0048\nconquer|misc_motive              3.8677  0.2454    15.7593\n\nResidual Deviance: 1631.342 \nAIC: 1647.342"
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#testing-the-proportional-odds-assumption",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#testing-the-proportional-odds-assumption",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "6.11 Testing the Proportional Odds Assumption",
    "text": "6.11 Testing the Proportional Odds Assumption\nA likelihood ratio test was performed. The multinomial model fits 5 intercepts and 20 slopes, for a total of 25 parameters. The proportional odds model fits 5 intercepts and 4 slopes, for a total of 9 parameters. The difference between them is 16.\nG is a chi-square statistic, calculated as the difference in deviance. Deviance = -2 log likelihood.\n\n\nCode\nLL_1 &lt;- logLik(polr_model)\nLL_1m &lt;- logLik(multinom_model)\n(G &lt;- -2 * (LL_1[1] - LL_1m[1]))\n\n\n[1] 110.8262\n\n\nThe p value associated with the chi-square statistic, with a difference of 16 degrees of freedom, is calculated.\n\n\nCode\npchisq(G, 16, lower.tail = FALSE)\n\n\n[1] 3.12363e-16\n\n\nThe p value suggests that the proportional odds model does not fit the data as well as the multinomial model does."
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#comparing-metrics",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#comparing-metrics",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "6.12 Comparing Metrics",
    "text": "6.12 Comparing Metrics\n\n\n\n\n\n\n\n\n\nModel\nEffective Degrees of Freedom\nDeviance\nAIC\n\n\n\n\nMultinomial Model\n20\n1515.427\n1555.427\n\n\nPolynomial Model\n8\n1624.988\n1640.988\n\n\n\nBased on the results of testing the proportional odds assumption, and the model metrics, the multinomial model appears to be a better fit for the data than the proportional odds model."
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#the-final-model",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#the-final-model",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "6.13 The Final Model",
    "text": "6.13 The Final Model\n\n\nCode\nsummary(multinom_model)\n\n\nCall:\nmultinom(formula = motive ~ monster_amount + monster_real + monster_gender, \n    data = q2_simp)\n\nCoefficients:\n                       (Intercept) monster_amount monster_realreal\ntheft                   -0.2984806    -0.03866893        0.3795771\nindirect_self_interest  -0.4247981    -0.13144830        1.9282423\ntreasure                -1.2076838     0.08470106       -0.6903529\nconquer                 -4.2541198     0.18107415        4.7542887\nmisc_motive             -2.1112197    -0.46152297        3.2876692\n                       monster_genderfemale\ntheft                            0.20634400\nindirect_self_interest           0.06788956\ntreasure                        -0.27912724\nconquer                         -0.08231355\nmisc_motive                      1.11501804\n\nStd. Errors:\n                       (Intercept) monster_amount monster_realreal\ntheft                    0.1697542     0.08281784        0.4887687\nindirect_self_interest   0.1767427     0.08597467        0.4165262\ntreasure                 0.2190608     0.09981712        0.8402710\nconquer                  0.6053196     0.08056674        0.7138281\nmisc_motive              0.3661798     0.19340172        0.5299548\n                       monster_genderfemale\ntheft                             0.3499198\nindirect_self_interest            0.3704725\ntreasure                          0.5111410\nconquer                           0.5731102\nmisc_motive                       0.5255567\n\nResidual Deviance: 1520.516 \nAIC: 1560.516"
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#p-values",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#p-values",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "6.14 p Values",
    "text": "6.14 p Values\n\n\nCode\nz &lt;- summary(multinom_model)$coefficients/summary(multinom_model)$standard.errors\np &lt;- (1 - pnorm(abs(z), 0, 1)) * 2\np |&gt; kable(dig = 3) |&gt;\n  kable_classic_2(font_size = 24,full_width = F) \n\n\n\n\n\n\n(Intercept)\nmonster_amount\nmonster_realreal\nmonster_genderfemale\n\n\n\n\ntheft\n0.079\n0.641\n0.437\n0.555\n\n\nindirect_self_interest\n0.016\n0.126\n0.000\n0.855\n\n\ntreasure\n0.000\n0.396\n0.411\n0.585\n\n\nconquer\n0.000\n0.025\n0.000\n0.886\n\n\nmisc_motive\n0.000\n0.017\n0.000\n0.034"
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#exponentiated-coefficients",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#exponentiated-coefficients",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "6.15 Exponentiated Coefficients",
    "text": "6.15 Exponentiated Coefficients\n\n\nCode\nexp(coef(multinom_model, conf_int = TRUE)) |&gt; kable(dig = 3) |&gt;\n  kable_classic_2(font_size = 24,full_width = F) \n\n\n\n\n\n\n(Intercept)\nmonster_amount\nmonster_realreal\nmonster_genderfemale\n\n\n\n\ntheft\n0.742\n0.962\n1.462\n1.229\n\n\nindirect_self_interest\n0.654\n0.877\n6.877\n1.070\n\n\ntreasure\n0.299\n1.088\n0.501\n0.756\n\n\nconquer\n0.014\n1.199\n116.081\n0.921\n\n\nmisc_motive\n0.121\n0.630\n26.780\n3.050"
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#exponentiated-coefficients-with-p-values",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#exponentiated-coefficients-with-p-values",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "6.16 Exponentiated Coefficients with p Values",
    "text": "6.16 Exponentiated Coefficients with p Values\n\n\nCode\ndf &lt;- data.frame(\n  motive = c(\"theft\", \"indirect_self_interest\", \"treasure\", \"conquer\", \"misc_motive\"),\n  intercept = c(0.741, 0.637, 0.299, 0.014, 0.102),\n  intercept_pvalue = c(0.078, 0.011, 0.000, 0.000, 0.000),\n  monster_amount = c(0.958, 0.882, 1.087, 1.209, 0.660),\n  monster_amount_pvalue = c(0.602, 0.141, 0.404, 0.019, 0.021),\n  monster_realreal = c(1.627, 6.949, 0.509, 115.167, 33.966),\n  monster_realreal_pvalue = c(0.306, 0.000, 0.419, 0.000, 0.000),\n  monster_genderfemale = c(1.229, 1.158, 0.757, 0.961, 2.556),\n  monster_genderfemale_pvalue = c(0.555, 0.689, 0.586, 0.945, 0.085)\n)\n\ndf |&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F) \n\n\n\n\n\nmotive\nintercept\nintercept_pvalue\nmonster_amount\nmonster_amount_pvalue\nmonster_realreal\nmonster_realreal_pvalue\nmonster_genderfemale\nmonster_genderfemale_pvalue\n\n\n\n\ntheft\n0.741\n0.078\n0.958\n0.602\n1.627\n0.306\n1.229\n0.555\n\n\nindirect_self_interest\n0.637\n0.011\n0.882\n0.141\n6.949\n0.000\n1.158\n0.689\n\n\ntreasure\n0.299\n0.000\n1.087\n0.404\n0.509\n0.419\n0.757\n0.586\n\n\nconquer\n0.014\n0.000\n1.209\n0.019\n115.167\n0.000\n0.961\n0.945\n\n\nmisc_motive\n0.102\n0.000\n0.660\n0.021\n33.966\n0.000\n2.556\n0.085"
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#exponentiated-coefficients-with-confidence-intervals",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#exponentiated-coefficients-with-confidence-intervals",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "6.17 Exponentiated Coefficients with Confidence Intervals",
    "text": "6.17 Exponentiated Coefficients with Confidence Intervals\n\n\nCode\ntidy(multinom_model,conf.int = TRUE, conf.level = 0.95, exponentiate = TRUE) |&gt;\n  kbl(dig = 3) |&gt;\n  kable_classic_2(font_size = 24,full_width = F)  \n\n\n\n\n\ny.level\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\ntheft\n(Intercept)\n0.742\n0.170\n-1.758\n0.079\n0.532\n1.035\n\n\ntheft\nmonster_amount\n0.962\n0.083\n-0.467\n0.641\n0.818\n1.132\n\n\ntheft\nmonster_realreal\n1.462\n0.489\n0.777\n0.437\n0.561\n3.810\n\n\ntheft\nmonster_genderfemale\n1.229\n0.350\n0.590\n0.555\n0.619\n2.440\n\n\nindirect_self_interest\n(Intercept)\n0.654\n0.177\n-2.403\n0.016\n0.462\n0.925\n\n\nindirect_self_interest\nmonster_amount\n0.877\n0.086\n-1.529\n0.126\n0.741\n1.038\n\n\nindirect_self_interest\nmonster_realreal\n6.877\n0.417\n4.629\n0.000\n3.040\n15.559\n\n\nindirect_self_interest\nmonster_genderfemale\n1.070\n0.370\n0.183\n0.855\n0.518\n2.212\n\n\ntreasure\n(Intercept)\n0.299\n0.219\n-5.513\n0.000\n0.195\n0.459\n\n\ntreasure\nmonster_amount\n1.088\n0.100\n0.849\n0.396\n0.895\n1.324\n\n\ntreasure\nmonster_realreal\n0.501\n0.840\n-0.822\n0.411\n0.097\n2.603\n\n\ntreasure\nmonster_genderfemale\n0.756\n0.511\n-0.546\n0.585\n0.278\n2.060\n\n\nconquer\n(Intercept)\n0.014\n0.605\n-7.028\n0.000\n0.004\n0.047\n\n\nconquer\nmonster_amount\n1.199\n0.081\n2.248\n0.025\n1.023\n1.404\n\n\nconquer\nmonster_realreal\n116.081\n0.714\n6.660\n0.000\n28.652\n470.298\n\n\nconquer\nmonster_genderfemale\n0.921\n0.573\n-0.144\n0.886\n0.300\n2.832\n\n\nmisc_motive\n(Intercept)\n0.121\n0.366\n-5.766\n0.000\n0.059\n0.248\n\n\nmisc_motive\nmonster_amount\n0.630\n0.193\n-2.386\n0.017\n0.431\n0.921\n\n\nmisc_motive\nmonster_realreal\n26.780\n0.530\n6.204\n0.000\n9.478\n75.668\n\n\nmisc_motive\nmonster_genderfemale\n3.050\n0.526\n2.122\n0.034\n1.089\n8.543"
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#storing-and-graphing-predicted-probabilities",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#storing-and-graphing-predicted-probabilities",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "6.18 Storing and graphing Predicted Probabilities",
    "text": "6.18 Storing and graphing Predicted Probabilities\nThe predicted probabilities were stored, tabulated, and then pivoted. This resulting table was used to create graphs of predicted probabilities of the outcome across the different predictors."
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#predicted-probabilities-and-monster_amount",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#predicted-probabilities-and-monster_amount",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "6.19 Predicted Probabilities and monster_amount",
    "text": "6.19 Predicted Probabilities and monster_amount\nThe predicted probabilities for each motive were graphed against the monster_amount variable.\n\n\nCode\nggplot(q2_simp_fits_long, aes(x = monster_amount, y = prob, shape = response, col = response)) +\n  geom_point(size = 3) +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(title = str_wrap(\"Comparing predicted probabilities of the different antagonist motive types against the number of monsters of each installment of Scooby-Doo Media.\", width = 80),\n       subtitle = \"Using a multinomial logistic regression model.\",\n       x = \"Number of monsters per installment\",\n       y = \"Predicted Probability\")\n\n\n\n\n\nHigher monster counts are association with higher predicted probability of conquer being the model’s prediction for the antagonist’s motivation, and lower monster counts are associated with higher predicted probability of misc_motive being the model’s prediction for the antagonist’s motivation."
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#predicted-probabilities-and-monster_real",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#predicted-probabilities-and-monster_real",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "6.20 Predicted Probabilities and monster_real",
    "text": "6.20 Predicted Probabilities and monster_real\n\n\nCode\nggplot(q2_simp_fits_long, aes(x = monster_real, y = prob,\n                              col = response,shape = response)) +\n  geom_point(size = 3) +\n  geom_jitter(width = 0.25) +\n  scale_fill_brewer(palette = \"Set1\")+\n  labs(title = str_wrap(\"Comparing predicted probabilities of the different antagonist motive types against whether the monster(s) of each installment of Scooby-Doo Media was real or not.\", width = 80),\n       subtitle = \"Using a multinomial logistic regression model.\",\n       x = \"Whether the monster was real or not\",\n       y = \"Predicted Probability\")\n\n\n\n\n\nMonsters who are real have a higher predicted probability of having conquering as a motivation, along with having a miscellaneous motivation. Monsters who are not real have a higher predicted probability of having a competition be their motivation, with a very low probability of conquering being their motivation."
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#predicted-probabilities-and-monster_gender",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#predicted-probabilities-and-monster_gender",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "6.21 Predicted Probabilities and monster_gender",
    "text": "6.21 Predicted Probabilities and monster_gender\n\n\nCode\nggplot(q2_simp_fits_long, aes(x = monster_gender, y = prob,\n                              col = response,shape = response)) +\n  geom_point(size = 3) +\n  geom_jitter(width = 0.25) + \n  scale_fill_brewer(palette = \"Set1\")  +\n  labs(title = str_wrap(\"Comparing predicted probabilities of the different antagonist motive types against whether the monster(s) of each installment of Scooby-Doo Media included a female monster.\", width = 80),\n       subtitle = str_wrap(\"Using a multinomial logistic regression model.\", width = 80),\n       x = \"Whether the monster(s) included a female monster.\",\n       y = \"Predicted Probability\") \n\n\n\n\n\nMonster gender does not appear to cause a significant effect on the predicted probability of the motivation types."
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#analysis-1-1",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#analysis-1-1",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "7.1 Analysis 1",
    "text": "7.1 Analysis 1\n\n\nCode\ntidy(mod_poisson_imp, exponentiate = TRUE, conf.int = TRUE, conf.level = 0.95) |&gt;\n  select(term, estimate, std.error, \n         low95 = conf.low, high95 = conf.high, p = p.value) |&gt;\n  kable(digits = 3)|&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nterm\nestimate\nstd.error\nlow95\nhigh95\np\n\n\n\n\n(Intercept)\n5.940\n0.911\n0.968\n34.596\n0.050\n\n\nrcs(imdb, 3)imdb\n0.760\n0.130\n0.591\n0.983\n0.034\n\n\nrcs(imdb, 3)imdb'\n0.994\n0.210\n0.653\n1.485\n0.976\n\n\nengagement\n1.000\n0.000\n1.000\n1.000\n0.751\n\n\nnetworkcartoon_network\n0.624\n0.212\n0.407\n0.935\n0.026\n\n\nnetworkwarner_brothers\n0.965\n0.185\n0.665\n1.372\n0.845\n\n\nnetworkboomerang\n0.559\n0.217\n0.359\n0.842\n0.007\n\n\nnetworkother_networks\n0.804\n0.192\n0.547\n1.162\n0.256\n\n\nseasonseason_2\n0.864\n0.144\n0.648\n1.140\n0.309\n\n\nseasonseason_3_4\n1.027\n0.169\n0.730\n1.418\n0.876\n\n\nseasonmovie\n1.896\n0.232\n1.205\n2.993\n0.006\n\n\nseasonspecial_crossover\n0.888\n0.308\n0.461\n1.556\n0.700\n\n\n\n\n\n\n\n\n7.1.1 The relationship between the Rooby-Rooby-Roo count and the producing network\nCartoon Network - Scooby-Doo media being produced by Cartoon Network results in a 37.6%(95%CI: 6.5% to 59.3%) decrease in count of Rooby-Rooby-Roo’s per episode compared to being produced by ABC , holding everything else constant.\nScooby-Doo media produced by Cartoon Network is more likely to have a lower count of Rooby-Rooby-Roos per episode when compared to Scooby-Doo media produced by ABC, provided they have the same IMDB rating, the same IMDB engagement score, and are from the same season.\nBoomerang Episodes - Scooby-Doo media being produced by Boomerang results in a 44.1% (95%CI: 15.8% to 64.1%) decrease in count of Rooby-Rooby-Roo’s per episode compared to being produced by ABC, holding everything else constant.\nScooby-Doo media produced by Boomerang is more likely to have a lower count of Rooby-Rooby-Roos per episode when compared to Scooby-Doo media produced by ABC, provided they have the same IMDB rating, the same IMDB engagement score, and are from the same season.\n\n\n7.1.2 The relationship between the Rooby-Rooby-Roo count and the type of Scooby-Doo media\nMovies: The form of Scooby-Doo media being a movie results in a 89.6%(95%CI: 20.5% to 199.3%) increase in count of Rooby-Rooby-Roo’s compared to season 1 episodes, holding everything else constant.\nMovies are more likely to have a higher count of Rooby-Rooby-Roo’s when compared to Season 1 episodes, provided they are both produced by the same network/producer, have the same IMDB score, and have the same IMDB engagement rating."
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#analysis-2-1",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#analysis-2-1",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "7.2 Analysis 2",
    "text": "7.2 Analysis 2\n\n\nCode\ndf |&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nmotive\nintercept\nintercept_pvalue\nmonster_amount\nmonster_amount_pvalue\nmonster_realreal\nmonster_realreal_pvalue\nmonster_genderfemale\nmonster_genderfemale_pvalue\n\n\n\n\ntheft\n0.741\n0.078\n0.958\n0.602\n1.627\n0.306\n1.229\n0.555\n\n\nindirect_self_interest\n0.637\n0.011\n0.882\n0.141\n6.949\n0.000\n1.158\n0.689\n\n\ntreasure\n0.299\n0.000\n1.087\n0.404\n0.509\n0.419\n0.757\n0.586\n\n\nconquer\n0.014\n0.000\n1.209\n0.019\n115.167\n0.000\n0.961\n0.945\n\n\nmisc_motive\n0.102\n0.000\n0.660\n0.021\n33.966\n0.000\n2.556\n0.085\n\n\n\n\n\n\n\n\n\nCode\ntidy(multinom_model,conf.int = TRUE, conf.level = 0.95, exponentiate = TRUE) |&gt;\n  kbl(dig = 3) |&gt;\n  kable_classic_2(font_size = 24,full_width = F)  \n\n\n\n\n\ny.level\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\ntheft\n(Intercept)\n0.742\n0.170\n-1.758\n0.079\n0.532\n1.035\n\n\ntheft\nmonster_amount\n0.962\n0.083\n-0.467\n0.641\n0.818\n1.132\n\n\ntheft\nmonster_realreal\n1.462\n0.489\n0.777\n0.437\n0.561\n3.810\n\n\ntheft\nmonster_genderfemale\n1.229\n0.350\n0.590\n0.555\n0.619\n2.440\n\n\nindirect_self_interest\n(Intercept)\n0.654\n0.177\n-2.403\n0.016\n0.462\n0.925\n\n\nindirect_self_interest\nmonster_amount\n0.877\n0.086\n-1.529\n0.126\n0.741\n1.038\n\n\nindirect_self_interest\nmonster_realreal\n6.877\n0.417\n4.629\n0.000\n3.040\n15.559\n\n\nindirect_self_interest\nmonster_genderfemale\n1.070\n0.370\n0.183\n0.855\n0.518\n2.212\n\n\ntreasure\n(Intercept)\n0.299\n0.219\n-5.513\n0.000\n0.195\n0.459\n\n\ntreasure\nmonster_amount\n1.088\n0.100\n0.849\n0.396\n0.895\n1.324\n\n\ntreasure\nmonster_realreal\n0.501\n0.840\n-0.822\n0.411\n0.097\n2.603\n\n\ntreasure\nmonster_genderfemale\n0.756\n0.511\n-0.546\n0.585\n0.278\n2.060\n\n\nconquer\n(Intercept)\n0.014\n0.605\n-7.028\n0.000\n0.004\n0.047\n\n\nconquer\nmonster_amount\n1.199\n0.081\n2.248\n0.025\n1.023\n1.404\n\n\nconquer\nmonster_realreal\n116.081\n0.714\n6.660\n0.000\n28.652\n470.298\n\n\nconquer\nmonster_genderfemale\n0.921\n0.573\n-0.144\n0.886\n0.300\n2.832\n\n\nmisc_motive\n(Intercept)\n0.121\n0.366\n-5.766\n0.000\n0.059\n0.248\n\n\nmisc_motive\nmonster_amount\n0.630\n0.193\n-2.386\n0.017\n0.431\n0.921\n\n\nmisc_motive\nmonster_realreal\n26.780\n0.530\n6.204\n0.000\n9.478\n75.668\n\n\nmisc_motive\nmonster_genderfemale\n3.050\n0.526\n2.122\n0.034\n1.089\n8.543\n\n\n\n\n\n\n\n\n7.2.1 The relationship between monster motivation and the monster count\nFor two installments of Scooby-Doo media,“EP-1” and “EP-2”, with the same nature of the monster, and the same gender of the monster, if “EP-1” has one more monster than “EP-2”, EP-1’s predicted odds of the antagonist’s motive being “Conquer” rather than “Competition” is 1.21 times(95% confidence interval - 1.03 to 1.42) EP-2’s odds of the antagonist’s motive being “Conquer” rather than “Competition”.\nEpisodes with more monsters have a meaningfully higher odds of the antagonist having a motive of “Conquer” rather than “Competition”, provided the monster gender and the reality of the monster remain same.\nFor two installments of Scooby-Doo media,“EP-1” and “EP-2”, with the same nature of the monster, and the same gender of the monster, if “EP-1” has one more monster than “EP-2”, EP-1’s predicted odds of the antagonist’s motive being “Miscellaneous” rather than “Competition” is only 0.66 times(95% confidence interval - 0.46 to 0.94) EP-2’s odds of the antagonist’s motive being “Miscellaneous” rather than “Competition”.\nEpisodes with more monsters have a meaningfully lower odds of the antagonist having a motive of “Miscellaneous” rather than “Competition”, provided the monster gender and the reality of the monster remain the same.\n\n\n7.2.2 The relationship between monster motivation and the nature of the monster’s reality\nFor two installments of Scooby-Doo media,“EP-1” and “EP-2”, with the same number of monsters, and the same gender of the monster, and the monsters in EP-2 are not real and EP-1 has a real monster, EP-1’s predicted odds of the antagonist’s motive being “Indirect Self Interest” rather than “Competition” is 6.95 times (95% confidence interval - 3.11 to 15.54) EP-2’s odds of the antagonist’s motive being “Indirect Self Interest” rather than “Competition”.\nEpisodes with real monsters have a meaningfully higher odds of the antagonist having a motive of “Indirect Self Interest” rather than “Competition”, provided the monster gender and the number of monsters remain the same.\nFor two installments of Scooby-Doo media,“EP-1” and “EP-2”, with the same number of monsters, and the the same gender of the monster, and the monsters in EP-2 are not real and EP-1 has a real monster, EP-1’s predicted odds of the antagonist’s motive being “Conquer” rather than “Competition” is 115.167 times(95% confidence interval - 28.60 to 463.70) EP-2’s odds of the antagonist’s motive being “Conquer” rather than “Competition”.\nEpisodes with real monsters have a meaningfully higher odds of the antagonist having a motive of “Conquer” rather than “Competition”, provided the monster gender and the number of monsters remain the same.\nFor two installments of Scooby-Doo media,“EP-1” and “EP-2”, with the same number of monsters, and the the same gender of the monster, and the monsters in EP-2 are not real and EP-1 has a real monster, EP-1’s predicted odds of the antagonist’s motive being “Miscellaneous” rather than “Competition” is 33.966 times(95% confidence interval - 12.10 to 95.74) EP-2’s odds of the antagonist’s motive being “Miscellaneous” rather than “Competition”.\nEpisodes with real monsters have a meaningfully higher odds of the antagonist having a motive of “Miscellaneous” rather than “Competition”, provided the monster gender and the number of monsters remain the same.\n\n\n7.2.3 The relationship between monster motivation and the gender of the monsters\nHaving a female monster in an installment of Scooby-Doo does not have a significant impact on the motive of the antagonist of that installment when compared to an installment with no female monsters, provided the number of monsters and the nature of the monster remains same across both installments."
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#answering-my-research-questions",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#answering-my-research-questions",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "7.3 Answering My Research Questions",
    "text": "7.3 Answering My Research Questions"
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#to-answer-my-first-question.",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#to-answer-my-first-question.",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "7.4 To Answer My First Question….",
    "text": "7.4 To Answer My First Question….\nPredicting an iconic catchphrase: Are the logistics of an episode of Scooby-Doo good predictors of the number of times an iconic catchphrase is spoken?\nThe network/producer that produces a particular installment of Scooby-Doo has a meaningful impact on the number of times Rooby-Rooby-Roo is spoken. Cartoon Network and Boomerang produce media that is less likely to have a higher count of Rooby-Rooby-Roo’s compared to ABC, which is the network that has produced the most Scooby-Doo media.(Provided the IMDB logistics and the type of installment remain constant.)\nGiven that there is a meaningful correlation, the producing network of Scooby-Doo media can be considered a decent predictor of the number of times Rooby-Rooby-Roo’s are spoken per installment.\nThe nature of the type of installment of Scooby-Doo media has a meaningful impact on the number of times Rooby-Rooby-Roo is spoken.\nMovies are more likely to have a higher count of Rooby-Rooby-Roo’s compared to Season 1 episodes, provided the two installments have the same IMDB logistics and the same producing network.\nGiven that there is a meaningful correlation, the nature of the type of installment of Scooby-Doo media can be considered a decent predictor of the number of times Rooby-Rooby-Roo’s are spoken per installment."
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#to-answer-my-second-question.",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#to-answer-my-second-question.",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "7.5 To Answer My Second Question….",
    "text": "7.5 To Answer My Second Question….\nMonster Motivation: Is it possible to predict the motive of the major antagonist of an episode of Scooby-Doo, based on the nature of the monster the antagonist appears as?\nThe number of monsters per installment of Scooby-Doo are a good predictor of the motive of the major antagonist. Episodes with higher monster counts are more likely to have antagonists that want to conquer, and episodes with lower monster counts are more likely to have antagonists that have more nebulous motivations, provided the monster’s gender and nature of reality remain constant.\nThe nature of reality of the monster(s) in an installment of Scooby-Doo are a good predictor of the motive of the major antagonist. Episodes with real monsters (as opposed to humans in a costume, or brainwashed/mind-controlled humans) are more likely to have antagonists that want to conquer, have more nebulous motivations, or act in their own self interest in an indirect manner, provided the monster’s gender and monster count remain constant.\nMonster gender does not have a meaningful impact on the motivation of the antagonist of an installment of Scooby-Doo, provided the monster’s nature of reality and monster count remain constant. (No gender bias here!)"
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#references",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#references",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "8.1 References",
    "text": "8.1 References\nThe data comes from Kaggle, and was part of Tidy Tuesday’s dataset for 2021-07-13, and can be found here.\nThe data was manually aggregated by user plummye on the Kaggle website."
  },
  {
    "objectID": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#acknowledgments",
    "href": "portfolio_projects/SCOOBY_study/Naveen_Kannan_Project_B_Portfolio.html#acknowledgments",
    "title": "Scooby Doo analytics: Quantifying what’s groovy about a pop culture icon.",
    "section": "8.2 Acknowledgments",
    "text": "8.2 Acknowledgments\nI would like to thank Stephanie Merlino for being a fantastic TA, and for answering any and every question I had for her without hesitation. She was a big part of why Project B felt like something I could accomplish."
  },
  {
    "objectID": "portfolio_projects/NHANES_study/Study-1.html",
    "href": "portfolio_projects/NHANES_study/Study-1.html",
    "title": "Analysing the relationship between Blood Cholesterol levels, Physical Activity, Excessive Drinking and Depression.",
    "section": "",
    "text": "Code\nsource(\"Love-boost.R\")\n\nlibrary(knitr)\nlibrary(rmdformats)\nlibrary(nhanesA)\nlibrary(kableExtra)\nlibrary(naniar)\nlibrary(janitor)\nlibrary(glue)\nlibrary(rcompanion)\nlibrary(gt)\nlibrary(ggridges)\nlibrary(patchwork)\nlibrary(broom)\nlibrary(tidyverse)\n\n## Global options\noptions(max.print=\"100\")\nopts_chunk$set(comment=NA)\nopts_knit$set(width=75)\nknitr::opts_chunk$set(comment = NA)\n\n\n\n\n\nThe nhanes package was used to read the data and store in in the form of an .Rds file for each of the data sets being used.\n\n\nCode\n## Demographic Data\n#demo_raw &lt;- nhanes('P_DEMO') |&gt; tibble()\n#saveRDS(demo_raw, \"C:\\\\Users\\\\navee\\\\Desktop\\\\PQHS 431\\\\Project B\\\\data\\\\P_DEMO.Rds\")\ndemo_raw &lt;- readRDS(\"data/P_DEMO.Rds\")\n\n## Alcohol Data\n#alc_raw &lt;- nhanes('P_ALQ') |&gt; tibble()\n#saveRDS(alc_raw, \"C:\\\\Users\\\\navee\\\\Desktop\\\\PQHS 431\\\\Project B\\\\data\\\\P_ALQ.Rds\")\nalc_raw &lt;- readRDS(\"data/P_ALQ.Rds\")\n\n## Depression data\n#depr_raw &lt;- nhanes('P_DPQ') |&gt; tibble()\n#saveRDS(depr_raw, \"C:\\\\Users\\\\navee\\\\Desktop\\\\PQHS 431\\\\Project B\\\\data\\\\P_DPQ.Rds\")\ndepr_raw &lt;- readRDS(\"data/P_DPQ.Rds\")\n\n## Physical activity data\n#phys_raw &lt;- nhanes('P_PAQ') |&gt; tibble()\n#saveRDS(phys_raw, \"C:\\\\Users\\\\navee\\\\Desktop\\\\PQHS 431\\\\Project B\\\\data\\\\P_PAQ.Rds\")\nphys_raw &lt;- readRDS(\"data/P_PAQ.Rds\")\n\n## Cholesterol data\n#chol_raw &lt;- nhanes('P_HDL') |&gt; tibble()\n#saveRDS(chol_raw, \"C:\\\\Users\\\\navee\\\\Desktop\\\\PQHS 431\\\\Project B\\\\data\\\\P_HDL.Rds\")\nchol_raw &lt;- readRDS(\"data/P_HDL.Rds\")\n\n\nThe tibbles were then merged on the SEQN variable.\n\n\nCode\n## Merging the tibbles\nNEW &lt;- left_join(demo_raw, chol_raw, by = \"SEQN\")\nNEW2 &lt;- left_join(NEW, alc_raw, by = \"SEQN\")\nNEW3 &lt;- left_join(NEW2, depr_raw, by = \"SEQN\")\ncombined_tib &lt;- left_join(NEW3, phys_raw, by = \"SEQN\")"
  },
  {
    "objectID": "portfolio_projects/NHANES_study/Study-1.html#loading-packages",
    "href": "portfolio_projects/NHANES_study/Study-1.html#loading-packages",
    "title": "Analysing the relationship between Blood Cholesterol levels, Physical Activity, Excessive Drinking and Depression.",
    "section": "",
    "text": "Code\nsource(\"Love-boost.R\")\n\nlibrary(knitr)\nlibrary(rmdformats)\nlibrary(nhanesA)\nlibrary(kableExtra)\nlibrary(naniar)\nlibrary(janitor)\nlibrary(glue)\nlibrary(rcompanion)\nlibrary(gt)\nlibrary(ggridges)\nlibrary(patchwork)\nlibrary(broom)\nlibrary(tidyverse)\n\n## Global options\noptions(max.print=\"100\")\nopts_chunk$set(comment=NA)\nopts_knit$set(width=75)\nknitr::opts_chunk$set(comment = NA)"
  },
  {
    "objectID": "portfolio_projects/NHANES_study/Study-1.html#ingesting-data",
    "href": "portfolio_projects/NHANES_study/Study-1.html#ingesting-data",
    "title": "Analysing the relationship between Blood Cholesterol levels, Physical Activity, Excessive Drinking and Depression.",
    "section": "",
    "text": "The nhanes package was used to read the data and store in in the form of an .Rds file for each of the data sets being used.\n\n\nCode\n## Demographic Data\n#demo_raw &lt;- nhanes('P_DEMO') |&gt; tibble()\n#saveRDS(demo_raw, \"C:\\\\Users\\\\navee\\\\Desktop\\\\PQHS 431\\\\Project B\\\\data\\\\P_DEMO.Rds\")\ndemo_raw &lt;- readRDS(\"data/P_DEMO.Rds\")\n\n## Alcohol Data\n#alc_raw &lt;- nhanes('P_ALQ') |&gt; tibble()\n#saveRDS(alc_raw, \"C:\\\\Users\\\\navee\\\\Desktop\\\\PQHS 431\\\\Project B\\\\data\\\\P_ALQ.Rds\")\nalc_raw &lt;- readRDS(\"data/P_ALQ.Rds\")\n\n## Depression data\n#depr_raw &lt;- nhanes('P_DPQ') |&gt; tibble()\n#saveRDS(depr_raw, \"C:\\\\Users\\\\navee\\\\Desktop\\\\PQHS 431\\\\Project B\\\\data\\\\P_DPQ.Rds\")\ndepr_raw &lt;- readRDS(\"data/P_DPQ.Rds\")\n\n## Physical activity data\n#phys_raw &lt;- nhanes('P_PAQ') |&gt; tibble()\n#saveRDS(phys_raw, \"C:\\\\Users\\\\navee\\\\Desktop\\\\PQHS 431\\\\Project B\\\\data\\\\P_PAQ.Rds\")\nphys_raw &lt;- readRDS(\"data/P_PAQ.Rds\")\n\n## Cholesterol data\n#chol_raw &lt;- nhanes('P_HDL') |&gt; tibble()\n#saveRDS(chol_raw, \"C:\\\\Users\\\\navee\\\\Desktop\\\\PQHS 431\\\\Project B\\\\data\\\\P_HDL.Rds\")\nchol_raw &lt;- readRDS(\"data/P_HDL.Rds\")\n\n\nThe tibbles were then merged on the SEQN variable.\n\n\nCode\n## Merging the tibbles\nNEW &lt;- left_join(demo_raw, chol_raw, by = \"SEQN\")\nNEW2 &lt;- left_join(NEW, alc_raw, by = \"SEQN\")\nNEW3 &lt;- left_join(NEW2, depr_raw, by = \"SEQN\")\ncombined_tib &lt;- left_join(NEW3, phys_raw, by = \"SEQN\")"
  },
  {
    "objectID": "portfolio_projects/NHANES_study/Study-1.html#the-question",
    "href": "portfolio_projects/NHANES_study/Study-1.html#the-question",
    "title": "Analysing the relationship between Blood Cholesterol levels, Physical Activity, Excessive Drinking and Depression.",
    "section": "4.1 The Question",
    "text": "4.1 The Question\nAmong adults of ages 21-79 participating in NHANES 2017-18, is there a significant difference in the values of HDL cholesterol values of participants across their self-reported participation or lack thereof in vigorous weekly physical activity?"
  },
  {
    "objectID": "portfolio_projects/NHANES_study/Study-1.html#describing-the-data",
    "href": "portfolio_projects/NHANES_study/Study-1.html#describing-the-data",
    "title": "Analysing the relationship between Blood Cholesterol levels, Physical Activity, Excessive Drinking and Depression.",
    "section": "4.2 Describing the Data",
    "text": "4.2 Describing the Data\nThe Outcome Variable\nThe outcome variable is LBDHDD, which is a continuous quantitative variable. It describes the Direct HDL-Cholesterol levels of the participants in mg/dL(milligrams of HDL-Cholesterol per deciliter of blood). The values range from 5 to 189 mg/dL, from the documentation of the variable available at https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/P_HDL.htm#LBDHDD.\nThe Binary Categorical Exposure Variable\nThe categorical variable is PAQ650, which is a Binary Categorical Variable. It describes whether or not the participant performs any vigorous-intensity sports, fitness, or recreational activities that cause large increases in breathing or heart rate like running or basketball for at least 10 minutes continuously, in a typical week. It has the following factors :\n“1” - Yes “0” - No\nThe samples here are independent, as the levels of the exposure variable PAQ650 are mutually exclusive and exhaustive, with only 1 of two possible answers that can apply to each participant.\n\n4.2.1 Numerical Summary of Direct HDL- Cholesterol across weekly vigorous activity groups\n\n\nCode\nmosaic::favstats(LBDHDD ~ PAQ650, data = final_tib) |&gt;\n  kable(digits = 2) |&gt; \n  kable_classic_2(font_size = 28, full_width = F)\n\n\n\n\n\nPAQ650\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n\n0\n23\n42\n51\n62\n138\n53.42\n15.57\n1492\n0\n\n\n1\n10\n44\n53\n65\n166\n55.96\n16.90\n1165\n0\n\n\n\n\n\n\n\nThe numerical summary indicates that there are differences in the distribution of the outcome variable across the vigorous activity groups. The range of HDL cholesterol goes to a higher level in the group that answered yes, and the mean and median HDL levels in the group are higher than that of the group that answered no, although it is yet to be seen if this difference is statistically meaningful. Furthermore, 1492 participants answered no, while 1165 participants answered yes, indicating that the study design is imbalanced.\n\n\nCode\ntemp1 &lt;- final_tib |&gt; \n  mutate(PAQ650_f = fct_recode(factor(PAQ650),\n                              \"No vigorous weekly physical activity\" = \"0\",\n                              \"Atleast once per week\" = \"1\")) |&gt; \n    select(PAQ650_f,LBDHDD)\nggplot(temp1, aes(x = LBDHDD, y = PAQ650_f, fill = PAQ650_f, height = after_stat(density))) +\n    ggridges::geom_density_ridges(scale = 2) +\n    scale_fill_brewer(palette = \"Blues\") +\n    guides(fill = \"none\") +\n    labs(title = str_wrap(\"Direct HDL-Cholesterol levels of participants split according to their self-reported participation in vigorous weekly physical activity\", width = 70),\n         x = \"Direct HDL-Cholesterol (mg/dL)\",\n         y = \"Self reported participation in weekly vigorous physical activity\",\n       subtitle = str_wrap(glue(\"Across \", nrow(temp1), \n                           \" participants from NHANES 2017-18 data with complete data. \"), width = 70),\n       caption = \"Source: NHANES 2017-18.\") \n\n\nPicking joint bandwidth of 3.28\n\n\n\n\n\nVisualizing the levels of direct HDL-cholesterol among study participants across weekly exercise activity shows that there is a difference in HDL-cholesterol levels, albeit slight. Individuals who participate in vigorous physical activity atleast once a week appear to have higher HDL-cholesterol levels than individuals who do not participate in weekly vigorous physical activity.\n\n\nCode\np1 &lt;- \n  ggplot(temp1, aes(x = LBDHDD)) +\n  geom_histogram(bins = 20, \n  fill = \"#2b8cbe\", col = \"white\") +\n  facet_grid(PAQ650_f ~ ., labeller = 'label_value') +\n  labs( x = \"Direct HDL-Cholesterol (mg/dL)\", y = \"\")\n\np2 &lt;- \n  ggplot(temp1, aes(sample = LBDHDD)) + \n  geom_qq(col = \"#2b8cbe\") + geom_qq_line(col = \"red\") +\n  facet_grid(PAQ650_f ~ ., labeller = 'label_value') +\n  labs(x = \"\", y = \"Direct HDL-Cholesterol (mg/dL)\")\n\np3 &lt;- \n  ggplot(temp1, aes(x = \"\", y = LBDHDD)) +\n  geom_violin(fill = \"#2b8cbe\", alpha = 0.3) + \n  geom_boxplot(fill = \"#2b8cbe\", width = 0.3,\n               outlier.color = \"red\") +\n  facet_grid(PAQ650_f ~ ., labeller = 'label_value') +\n  labs(y = \"Direct HDL-Cholesterol (mg/dL)\", x = \"\") + coord_flip()\n\np2 + p1 - p3 +\n  plot_layout(ncol = 1, height = c(3, 2)) + \n  plot_annotation(title = str_wrap(\"Distribution of Direct HDL-Cholesterol (mg/dL), split according to the self reported weekly vigorous physical activity of the participant\", width = 100),\n         subtitle = glue(\"Across \", nrow(temp1), \n                           \" participants. Data taken from NHANES 2017-18\"))\n\n\n\n\n\nThe distribution of Direct HDL-Cholesterol values across the two different groups of weekly vigorous activity demonstrates that the distribution has a significant right skew, from the shape of the Q-Q plots, the shape of the histogram, and the outliers detected in the boxplot and violinplot.\n\n\nCode\nfinal_tib |&gt;\n  summarise(mean(LBDHDD), median(LBDHDD), sd(LBDHDD),\n              skew1 = (mean(LBDHDD) - median(LBDHDD))/sd(LBDHDD)) |&gt;\n    kbl(digits = 2) |&gt;kable_classic_2(font_size = 28, full_width = F)\n\n\n\n\n\nmean(LBDHDD)\nmedian(LBDHDD)\nsd(LBDHDD)\nskew1\n\n\n\n\n54.53\n52\n16.21\n0.16\n\n\n\n\n\n\n\nThe numerical value of the skew also indicates that the data is right-skewed. From the data visualization and the numerical summaries, the right-skewed, non-normal distribution of the outcome variable indicates that the bootstrap method is ideal in order to compare the difference in means across the two groups."
  },
  {
    "objectID": "portfolio_projects/NHANES_study/Study-1.html#main-analysis",
    "href": "portfolio_projects/NHANES_study/Study-1.html#main-analysis",
    "title": "Analysing the relationship between Blood Cholesterol levels, Physical Activity, Excessive Drinking and Depression.",
    "section": "4.3 Main Analysis",
    "text": "4.3 Main Analysis\nGiven the non-normality of the outcome variable’s distribution and the imbalance in the study design, the Bootstrap method would be the ideal choice to estimate the difference in means across the two groups. After setting a seed, the bootdif function from the love-boost script was used to perform a bootstrapped analysis of the difference in mean HDL levels between the weekly vigorous exercise groups.\n\n\nCode\nset.seed(4312022)\n\nbootdif(final_tib$LBDHDD, final_tib$PAQ650, conf.level = 0.90)\n\n\nMean Difference            0.05            0.95 \n       2.545718        1.491790        3.637311"
  },
  {
    "objectID": "portfolio_projects/NHANES_study/Study-1.html#conclusions",
    "href": "portfolio_projects/NHANES_study/Study-1.html#conclusions",
    "title": "Analysing the relationship between Blood Cholesterol levels, Physical Activity, Excessive Drinking and Depression.",
    "section": "4.4 Conclusions",
    "text": "4.4 Conclusions\nThe 90% confidence interval for the difference in the means of Direct HDL-Cholesterol levels goes from 1.49 mg/dL to 3.64 mg/dL, which are a reasonable range of estimates for the true difference of the mean of Direct HDL-Cholesterol levels across the two weekly vigorous activity groups. The point estimate of the difference in means is 2.55 mg/dL. This confidence interval does not include zero.\nFrom the fact that we are using a 90% confidence interval, we can be 90% sure that this method of creating a confidence interval will produce a result containing the true difference in means. (This means that if bootstrapping was done 100 times to create 100 different intervals, 90 of those intervals will have the true difference in mean Direct HDL-Cholesterol levels.)\nThere is a statistically detectable difference in Direct HDL-Cholesterol levels across the two groups of weekly vigorous physical activity.\nThe mean blood HDL cholesterol for individuals who reported weekly vigorous physical activity was 2.55 mg/dL higher than that of individuals who did not report weekly vigorous physical activity, with a 90% CI of (1.49, 3.64 mg/dL).\nHowever, while there is a statistical difference, the mean difference varies from 1.49 to 3.64 mg/dL. This difference may not be medically significant."
  },
  {
    "objectID": "portfolio_projects/NHANES_study/Study-1.html#the-question-1",
    "href": "portfolio_projects/NHANES_study/Study-1.html#the-question-1",
    "title": "Analysing the relationship between Blood Cholesterol levels, Physical Activity, Excessive Drinking and Depression.",
    "section": "5.1 The Question",
    "text": "5.1 The Question\nAmong adults of ages 21-79 participating in NHANES 2017-18, are differences in severity of feeling down, depressed, or hopeless associated with differences in the number of minutes of daily sedentary activity among NHANES 2017-18 participants?"
  },
  {
    "objectID": "portfolio_projects/NHANES_study/Study-1.html#describing-the-data-1",
    "href": "portfolio_projects/NHANES_study/Study-1.html#describing-the-data-1",
    "title": "Analysing the relationship between Blood Cholesterol levels, Physical Activity, Excessive Drinking and Depression.",
    "section": "5.2 Describing the Data",
    "text": "5.2 Describing the Data\nThe Outcome Variable\nThe outcome variable is PAD680, which is a continuous quantitative variable. It describes the participants time spent sitting on a typical day in Minutes.\nThe Multi-Categorical Exposure Variable\nThe categorical variable is DPQ020, which is a Multi-Categorical Variable. It describes how often has the participant been bothered by the following problems: feeling down, depressed, or hopeless over the past 2 weeks. It has the following factors :\n4 levels :\n0 = Not at all\n1 = Several days\n2 = More than half the days\n3 = Nearly every day\n\n\nCode\nmosaic::favstats(PAD680 ~ DPQ020, data = final_tib) |&gt; kbl() |&gt; \n  kable_classic_2(font_size = 28)\n\n\n\n\n\nDPQ020\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n\n0\n10\n180\n300\n480\n1080\n336.9990\n197.0243\n2042\n0\n\n\n1\n2\n180\n300\n480\n960\n331.5556\n196.0190\n450\n0\n\n\n2\n20\n180\n240\n360\n900\n294.8454\n186.7324\n97\n0\n\n\n3\n15\n120\n300\n420\n960\n301.4706\n208.0203\n68\n0\n\n\n\n\n\n\n\nThe above table depicts the summary statistics of PAD680 values across the levels of DPQ020. From the summary statistics alone, there appears to be a differences in the mean PAD680 value across the different levels of DPQ020. However, numbers alone are not enough to make any conclusions, and the data needs to be visualized.\nUsing ggplot and ggridges, the distribution of PAD680 values across the levels of DPQ020 was visualized, and given a color scale to help the different categories stand out visually.\n\n\nCode\ntemp2 &lt;- final_tib |&gt; \n  mutate(DPQ020_f = fct_recode(factor(DPQ020),\n                              \"Not at all\" = \"0\",\n                              \"Several days\" = \"1\",\n                              \"More than half the days\" = \"2\",\n                              \"Nearly every day\" = \"3\")) |&gt; \n    select(DPQ020_f,PAD680)\nggplot(temp2, aes(x = PAD680, y = DPQ020_f, fill = DPQ020_f, height = after_stat(density))) +\n    ggridges::geom_density_ridges(scale = 2) +\n    scale_fill_brewer(palette = \"RdYlGn\", direction = -1) +\n    guides(fill = \"none\") +\n    labs(title = str_wrap(\"Minutes of sedentary activity of NHANES participants per day split according to how often they felt down, depressed or hopeless over the past 2 weeks.\", width = 70),\n         x = \"Minutes of sedentary activity during the day (Minutes)\",\n         y = str_wrap(\"How often did the participant feel down, depressed or hopeless over the past 2 weeks?\",width = 70),\n       subtitle = str_wrap(glue(\"Across \", nrow(temp2), \n                           \" participants from NHANES 2017-18 data with complete data. \"), width = 70),\n       caption = \"Source: NHANES 2017-18.\") \n\n\nPicking joint bandwidth of 54.9\n\n\n\n\n\nThe above graphic visualizes the distribution of DPQ020 (Minutes of sedentary activity of NHANES participants per day) according to the PAD680 (How often did the participant feel down, depressed or hopeless over the past 2 weeks?) category of the participant. From looking at the plot, it does appear that categories 2 (More than half the days) and 3 (Nearly every day) have a larger proportion of minutes of sedentary activity.\n\n\nCode\nggplot(temp2, aes(x = PAD680, y = DPQ020_f, fill = DPQ020_f)) +\n  geom_violin(alpha = 0.3) +\n  geom_boxplot(width = 0.3) +\n  scale_fill_brewer(palette = \"RdYlGn\", direction = -1) +\n  coord_flip() +\n  guides(fill = \"none\") +\n  theme_bw() +\n  labs(title = str_wrap(\"Minutes of sedentary activity of NHANES participants per day split according to how often they felt down, depressed or hopeless over the past 2 weeks.\", width = 70),\n       y = str_wrap(\"How often did the participant feel down, depressed or hopeless over the past 2 weeks?\",width = 70),\n       x = \"Minutes of sedentary activity per day\",\n       caption = \"Data taken from NHANES 2017-18\")\n\n\n\n\n\nThe above plot demonstrates the variance of PAD680 values across the 4 categories of DPQ020. PAD680 appears to have more or less equal variance across the 4 categories.\n\n\nCode\np1 &lt;- \n  ggplot(temp2, aes(x = PAD680)) +\n  geom_histogram(bins = 20, \n  fill = \"#2b8cbe\", col = \"white\") +\n  facet_grid(DPQ020_f ~ ., labeller = 'label_value') +\n  labs( x = \"Minutes of sedentary activity during the day (Minutes)\", y = \"\")\n\np2 &lt;- \n  ggplot(temp2, aes(sample = PAD680)) + \n  geom_qq(col = \"#2b8cbe\") + geom_qq_line(col = \"red\") +\n  facet_grid(DPQ020_f ~ ., labeller = 'label_value') +\n  labs(x = \"\", y = \"Minutes of sedentary activity during the day (Minutes)\")\n\np3 &lt;- \n  ggplot(temp2, aes(x = \"\", y = PAD680)) +\n  geom_violin(fill = \"#2b8cbe\", alpha = 0.3) + \n  geom_boxplot(fill = \"#2b8cbe\", width = 0.3,\n               outlier.color = \"red\") +\n  facet_grid(DPQ020_f ~ ., labeller = 'label_value') +\n  labs(y = \"Minutes of sedentary activity during the day (Minutes)\", x = \"\") + coord_flip()\n\np2 + p1 - p3 +\n  plot_layout(ncol = 1, height = c(3, 2)) + \n  plot_annotation(title = str_wrap(\"Distribution of Minutes of Sedentary Activity of NHANES participants per day\", width = 90),\n         subtitle = str_wrap(glue(\"Across \", nrow(temp2), \n                           \" participants, split according to how often they felt down, depressed or hopeless over the past 2 weeks. Data taken from NHANES 2017-18.\"), width = 90))\n\n\n\n\n\nThese plots demonstrate the distribution of PAD680. The distribution is skewed to the right, and does not appear to have a normal distribution.\n\n\nCode\nfinal_tib |&gt;\n  summarise(mean(PAD680), median(PAD680), sd(PAD680),\n              skew1 = (mean(PAD680) - median(PAD680))/sd(PAD680)) |&gt;\n    kbl(digits = 2) |&gt;kable_classic_2(font_size = 28, full_width = F)\n\n\n\n\n\nmean(PAD680)\nmedian(PAD680)\nsd(PAD680)\nskew1\n\n\n\n\n333.63\n300\n196.9\n0.17\n\n\n\n\n\n\n\nThe numerical summaries also demonstrate the PAD680 does not have a normal distribution; it is right-skewed. There is a marked difference between the mean and the median, and the numerical value of the skew also demonstrates a right skew."
  },
  {
    "objectID": "portfolio_projects/NHANES_study/Study-1.html#main-analysis-1",
    "href": "portfolio_projects/NHANES_study/Study-1.html#main-analysis-1",
    "title": "Analysing the relationship between Blood Cholesterol levels, Physical Activity, Excessive Drinking and Depression.",
    "section": "5.3 Main Analysis",
    "text": "5.3 Main Analysis\nGiven that PAD680 does not have a normal distribution, even though the distribution of PAD680 across DPQ020 has equal variance, the assumptions of an ANOVA test would be violated if it were to be performed on these two variables.\nThe three main assumptions of an ANOVA test are as follows.\n\nNormality\n\nEach sample was drawn from a normally distributed population.\n\nEqual variance\n\nThe variances of the populations that the samples come from are equal.\n\nIndependence\n\nThe observations in each group are independent of each other and the observations within groups were obtained by a random sample.\nThus, the Kruskal-Wallis rank sum test would be preferable to the ANOVA test to estimate if there is a significant difference in the mean value of PAD680 across the 4 categories of DPQ020, especially because of the significant right-skew of the data. The Kruskal-Wallis test is non-parametric, which means that it does not need any distributional assumption.\n\n\nCode\nkruskal.test(PAD680 ~ DPQ020, data = final_tib)\n\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  PAD680 by DPQ020\nKruskal-Wallis chi-squared = 6.7478, df = 3, p-value = 0.08038\n\n\nThe results of the Kruskal-Wallis test do not indicate a meaningful difference in the values of PAD680 across the levels of DPQ020, given that the p value is 0.080, with a significance value of 0.05."
  },
  {
    "objectID": "portfolio_projects/NHANES_study/Study-1.html#conclusions-1",
    "href": "portfolio_projects/NHANES_study/Study-1.html#conclusions-1",
    "title": "Analysing the relationship between Blood Cholesterol levels, Physical Activity, Excessive Drinking and Depression.",
    "section": "5.4 Conclusions",
    "text": "5.4 Conclusions\nFrom the results of the Kruskal-Wallis test, among the study population taken from NHANES 2017-18, there is no statistically detectable difference in the number of minutes of daily sedentary activity across the different levels of how often they felt down, depressed or hopeless over the past 2 weeks."
  },
  {
    "objectID": "portfolio_projects/NHANES_study/Study-1.html#the-question-2",
    "href": "portfolio_projects/NHANES_study/Study-1.html#the-question-2",
    "title": "Analysing the relationship between Blood Cholesterol levels, Physical Activity, Excessive Drinking and Depression.",
    "section": "6.1 The Question",
    "text": "6.1 The Question\nAmong adults of ages 21-79 participating in NHANES 2017-18, is there an association between a period of heavy drinking (described as having 4 or more alcoholic beverages daily) and a lack of weekly vigorous physical activity?"
  },
  {
    "objectID": "portfolio_projects/NHANES_study/Study-1.html#describing-the-data-2",
    "href": "portfolio_projects/NHANES_study/Study-1.html#describing-the-data-2",
    "title": "Analysing the relationship between Blood Cholesterol levels, Physical Activity, Excessive Drinking and Depression.",
    "section": "6.2 Describing the Data",
    "text": "6.2 Describing the Data\nThe two binary categorical variables under study are:\nThe Outcome Variable\nPAQ650 - It describes whether or not the participant performs any vigorous-intensity sports, fitness, or recreational activities that cause large increases in breathing or heart rate like running or basketball for at least 10 minutes continuously, in a typical week. It has the following levels:\n1 = Yes\n0 = No\nThe Exposure Variable\nALQ151 - It describes whether or not the participant had a period in their life where they drank 4 or more drinks of any alcoholic beverage almost every day. It has the following levels:\n1 = Yes\n0 = No\nUsing the mutate function, the variables ALQ151 and PAQ650 were given different factor names to aid in understanding their meaning.\n\n\nCode\ntemp3&lt;- final_tib |&gt;\n  mutate(ALQ151 = fct_recode(factor(ALQ151),\n                                \"Yes\" = \"1\",\n                                \"No\" = \"0\"),\n         PAQ650 = fct_recode(factor(PAQ650),\n                                \"Yes\" = \"1\",\n                                \"No\" = \"0\")) |&gt;\n  select(PAQ650, ALQ151)\n\ntemp3 &lt;- temp3 |&gt;\n  mutate(ALQ151 = fct_relevel(ALQ151, \"Yes\"),\n         PAQ650 = fct_relevel(PAQ650, \"No\", \"Yes\"))\n\n\nThe tabyl function was used to create a table of the two variables.\n\n\nCode\ntable3 &lt;- temp3 |&gt; tabyl(ALQ151, PAQ650)|&gt; \n    adorn_totals(where = c(\"row\")) |&gt;\n    adorn_percentages(denominator = \"row\") |&gt;\n    adorn_pct_formatting() |&gt;\n    adorn_ns(position = \"front\")\n\n\nUsing the gt package in conjunction with the glue package, a meaningful, visually appealing 2x2 table was created.\n\n\nCode\ngt_table3 &lt;- gt(table3)\ngt_table3 &lt;- gt_table3 |&gt;\n  tab_header(\n    title = md(\"**Exploring the relationship between heavy drinking and a lack of weekly vigorous physical activity.**\"),\n    subtitle = glue(\"Across \", nrow(temp3), \n                           \" participants with complete data.\")\n  ) |&gt;\n  tab_spanner(\n    label = \"Does the participant perform any vigorous-intensity activities that cause large increases in breathing or heart rate for at least 10 minutes continuously, in a typical week?\",\n    columns = c(Yes, No)\n  ) |&gt;\n  cols_label(\"ALQ151\" = \"Did the participant have a period in their life where they drank 4 or more drinks of any alcoholic beverage almost every day?\") |&gt;\n  cols_label(\"No\" = md(\"**No**\")) |&gt;\n  cols_label(\"Yes\" = md(\"**Yes**\")) |&gt;\n  tab_source_note(\n    source_note = md(\"**Source:** _National Health and Nutrition Examination Survey, 2017-2018_\")\n  )\ngt_table3\n\n\n\n\n\n\n  \n    \n      Exploring the relationship between heavy drinking and a lack of weekly vigorous physical activity.\n    \n    \n      Across 2657 participants with complete data.\n    \n    \n      Did the participant have a period in their life where they drank 4 or more drinks of any alcoholic beverage almost every day?\n      \n        Does the participant perform any vigorous-intensity activities that cause large increases in breathing or heart rate for at least 10 minutes continuously, in a typical week?\n      \n    \n    \n      Yes\n      No\n    \n  \n  \n    Yes\n131 (37.9%)\n215 (62.1%)\n    No\n1,034 (44.7%)\n1,277 (55.3%)\n    Total\n1,165 (43.8%)\n1,492 (56.2%)\n  \n  \n    \n      Source: National Health and Nutrition Examination Survey, 2017-2018"
  },
  {
    "objectID": "portfolio_projects/NHANES_study/Study-1.html#main-analysis-2",
    "href": "portfolio_projects/NHANES_study/Study-1.html#main-analysis-2",
    "title": "Analysing the relationship between Blood Cholesterol levels, Physical Activity, Excessive Drinking and Depression.",
    "section": "6.3 Main Analysis",
    "text": "6.3 Main Analysis\nUsing the twobytwo function from the love-boost script, an analysis of the two-by-two table was conducted.\n\n\nCode\ntwobytwo(215+2, 131+2, 1277+2, 1034+2, \"Drank 4 or more alcoholic drinks daily\",\"Did not have such a period\",\"No weekly physical activity\",\"Had weekly physical activity\", conf.level = 0.90)\n\n\n2 by 2 table analysis: \n------------------------------------------------------ \nOutcome   : No weekly physical activity \nComparing : Drank 4 or more alcoholic drinks daily vs. Did not have such a period \n\n                                       No weekly physical activity\nDrank 4 or more alcoholic drinks daily                         217\nDid not have such a period                                    1279\n                                       Had weekly physical activity\nDrank 4 or more alcoholic drinks daily                          133\nDid not have such a period                                     1036\n                                          P(No weekly physical activity)\nDrank 4 or more alcoholic drinks daily                            0.6200\nDid not have such a period                                        0.5525\n                                       90% conf. interval\nDrank 4 or more alcoholic drinks daily    0.5765   0.6617\nDid not have such a period                0.5354   0.5694\n\n                                   90% conf. interval\n             Relative Risk: 1.1222    1.0407   1.2101\n         Sample Odds Ratio: 1.3216    1.0888   1.6041\nConditional MLE Odds Ratio: 1.3215    1.0822   1.6163\n    Probability difference: 0.0675    0.0209   0.1125\n\n             Exact P-value: 0.0179 \n        Asymptotic P-value: 0.0179 \n------------------------------------------------------"
  },
  {
    "objectID": "portfolio_projects/NHANES_study/Study-1.html#conclusions-2",
    "href": "portfolio_projects/NHANES_study/Study-1.html#conclusions-2",
    "title": "Analysing the relationship between Blood Cholesterol levels, Physical Activity, Excessive Drinking and Depression.",
    "section": "6.4 Conclusions",
    "text": "6.4 Conclusions\n\n6.4.1 The Relative Risk\nIndividuals who have had a period of their life where they drank 4 or more alcoholic drinks a day have a 1.12 times higher risk(90% CI-1.04 to 1.21) of having no weekly vigorous physical activity when compared to individuals who did not have such a period.\n\n\n6.4.2 The Odds Ratio\nIndividuals who do not report weekly vigorous physical activity have 1.32 times higher odds(90% CI-1.09 to 1.60) of having had a period in their lives where they drank 4 or more alcoholic drinks a day when compared to individuals who did report weekly vigorous physical activity.\n\n\n6.4.3 The Risk Difference\nThe difference between in the probability of having no weekly vigorous physical activity for individuals who have had a period of their life where they drank 4 or more alcoholic drinks versus individuals who did not have such a period is 0.068 (90% CI - 0.021 to 0.113)."
  },
  {
    "objectID": "portfolio_projects/NHANES_study/Study-1.html#the-question-3",
    "href": "portfolio_projects/NHANES_study/Study-1.html#the-question-3",
    "title": "Analysing the relationship between Blood Cholesterol levels, Physical Activity, Excessive Drinking and Depression.",
    "section": "7.1 The Question",
    "text": "7.1 The Question\nAmong adults of ages 21-79 participating in NHANES 2017-18, is there an association between having had a period of heavy drinking (described as having 4 or more alcoholic beverages daily) and symptoms of depression (how often has the participant been bothered by the following problems: feeling down, depressed, or hopeless)?"
  },
  {
    "objectID": "portfolio_projects/NHANES_study/Study-1.html#describing-the-data-3",
    "href": "portfolio_projects/NHANES_study/Study-1.html#describing-the-data-3",
    "title": "Analysing the relationship between Blood Cholesterol levels, Physical Activity, Excessive Drinking and Depression.",
    "section": "7.2 Describing the Data",
    "text": "7.2 Describing the Data\nUsing the mutate function, the variables under study and their factors were re-leveled and renamed in order to create context and meaning for the data.\nThe tabyl function was then used to create a dataset containing the different factors of DPQ020 as the rows, and the two factors of ALQ151 as the columns.\n\n\nCode\ntemp4 &lt;- final_tib |&gt;\n  select(DPQ020, ALQ151) |&gt;\n  mutate(DPQ020 = fct_recode(factor(DPQ020),\n                               \"Not at all\" = \"0\",\n                               \"Several Days\" = \"1\",\n                               \"More than half the days\" = \"2\",\n                               \"Nearly every day\" = \"3\"),\n         ALQ151 = fct_recode(factor(ALQ151),\n                               \"No\" = \"0\",\n                               \"Yes\" = \"1\"))\ntable4 &lt;- temp4 |&gt;\n  tabyl(DPQ020, ALQ151)|&gt;\n  adorn_totals(where = c(\"row\")) |&gt;\n    adorn_percentages(denominator = \"row\") |&gt;\n    adorn_pct_formatting() |&gt;\n    adorn_ns(position = \"front\")\n\n\nThe gt package was used to create a meaningful, visually appealing table for the data present in table4, in conjunction with the glue package.\n\n\nCode\ngt_table4 &lt;- gt(table4)\ngt_table4 &lt;- gt_table4 |&gt;\n  tab_header(\n    title = md(\"**Exploring the relationship between depression and excessive drinking.**\"),\n    subtitle = glue(\"Across \", nrow(temp4), \n                           \" participants with complete data.\")\n  )|&gt;\n  tab_spanner(\n    label = \"Did the participant have a period in their life where they drank 4 or more drinks of any alcoholic beverage almost every day?\",\n    columns = c(Yes, No)\n  ) |&gt;\n  cols_label(\"DPQ020\" = \"Over the last 2 weeks, how often has the participant been bothered by the following problems: feeling down, depressed, or hopeless\") |&gt;\n  cols_label(\"Yes\" = md(\"**Yes**\")) |&gt;\n  cols_label(\"No\" = md(\"**No**\")) |&gt;\n  tab_source_note(\n    source_note = md(\"**Source:** _National Health and Nutrition Examination Survey, 2017-2018_\")\n  )\ngt_table4\n\n\n\n\n\n\n  \n    \n      Exploring the relationship between depression and excessive drinking.\n    \n    \n      Across 2657 participants with complete data.\n    \n    \n      Over the last 2 weeks, how often has the participant been bothered by the following problems: feeling down, depressed, or hopeless\n      \n        Did the participant have a period in their life where they drank 4 or more drinks of any alcoholic beverage almost every day?\n      \n    \n    \n      Yes\n      No\n    \n  \n  \n    Not at all\n238 (11.7%)\n1,804 (88.3%)\n    Several Days\n67 (14.9%)\n383 (85.1%)\n    More than half the days\n16 (16.5%)\n81 (83.5%)\n    Nearly every day\n25 (36.8%)\n43 (63.2%)\n    Total\n346 (13.0%)\n2,311 (87.0%)\n  \n  \n    \n      Source: National Health and Nutrition Examination Survey, 2017-2018"
  },
  {
    "objectID": "portfolio_projects/NHANES_study/Study-1.html#main-analysis-3",
    "href": "portfolio_projects/NHANES_study/Study-1.html#main-analysis-3",
    "title": "Analysing the relationship between Blood Cholesterol levels, Physical Activity, Excessive Drinking and Depression.",
    "section": "7.3 Main Analysis",
    "text": "7.3 Main Analysis\nThe Pearson Chi-Square test of Independence was conducted to assess if there is an association between the rows and the columns of the contingency table.\n\n\nCode\ntable4a &lt;- table(temp4$DPQ020, temp4$ALQ151)\nchisq.test(table4a)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  table4a\nX-squared = 39.629, df = 3, p-value = 1.277e-08\n\n\nFrom the p-value of the above Chi-squared test, we can assume that there is a definite association between the rows and columns of the contingency table.\nCramer’s V\nFrom the Chi-squared test, we know that there is an association between the rows and the columns of the contingency table. However, to identify the strength of the association, the Cramer’s V test, which is a test of statistical strength, can be conducted.\n\n\nCode\ncramerV(table4a, ci = TRUE) \n\n\n  Cramer.V lower.ci upper.ci\n1   0.1221  0.07406   0.1801\n\n\nThe point estimate of the Cramer’s V value is 0.122, with 95% confidence intervals ranging from 0.074 to 0.180.\nFrom the Cramer’s V test, given that there are 3 degrees of freedom, the association between depression and excessive drinking is not particularly strong, although it is definitely present.\n\n7.3.1 Chi-Square Test Assumptions\n\nThe levels of the variables are mutually exclusive.\nEach subject may contribute data to one and only one cell in the χ2.\nThe study groups must be independent.\n\nIn addition to these, the Cochran’s assumptions must also be met, which are:\n\nNo cells with 0 counts\nAt least 80% of the cells in our table have counts of 5 or higher\nThe Expected counts in each cell of the table should be 5 or more"
  },
  {
    "objectID": "portfolio_projects/NHANES_study/Study-1.html#conclusions-3",
    "href": "portfolio_projects/NHANES_study/Study-1.html#conclusions-3",
    "title": "Analysing the relationship between Blood Cholesterol levels, Physical Activity, Excessive Drinking and Depression.",
    "section": "7.4 Conclusions",
    "text": "7.4 Conclusions\n11.7% of participants who reported never feeling down, depressed, or hopeless over the past 2 weeks had a period in their life where they drank 4 or more drinks of any alcoholic beverage almost every day.\nThe proportion of people who have a history of excessive drinking increases with the severity of the self reported symptoms of depression.\n36.8% of participants who felt down, depressed or hopeless nearly every day had a period in their life where they drank 4 or more drinks of any alcoholic beverage almost every day.\nAmong adults of ages 21-79 participating in NHANES 2017-18, individuals who have had a period of heavy drinking (described as having 4 or more alcoholic beverages daily) have a greater degree of association with symptoms of depression (how often has the participant been bothered by the following problems: feeling down, depressed, or hopeless) compared to individuals who have not had a period of heavy drinking. Stronger feelings of depression are associated more strongly with a history of having had a period of excessive drinking."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Naveen Kannan",
    "section": "",
    "text": "I’m a Database Engineer at Trailhead Biosystems, Cleveland. My research interests include Deep Learning, Bioinformatics, Biostatistics and Geospatial Analytics. In my spare time I enjoy art, swimming, hiking and cooking.\nThis is my personal little nerd cave. I like to share the work I do on my blog. I hope you find something useful here!"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Naveen Kannan",
    "section": "Education",
    "text": "Education\nMS in Biomedical and Health Informatics\nCase Western Reserve University | Cleveland - Ohio\nJanuary 2022 - August 2023\nMBBS (Bachelor of Medicine and Bachelor of Surgery)\nMadras Medical College | Chennai, India\nSeptember 2016 - March 2020"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Naveen Kannan",
    "section": "Experience",
    "text": "Experience\nDatabase Engineer\nTrailhead Biosystems, Cleveland, Ohio\nJuly 2024 - Present\n\nResearch Associate\nDepartment of Population and Quantitative Health Sciences, Case Western Reserve University\nMay 2023 - July 2024\n\nGraduate Research Assistant\nDepartment of Population and Quantitative Health Sciences, Case Western Reserve University\nJanuary 2023 - May 2023\n\nGraduate Research Assistant\nGIS Health and Hazards Lab, Department of Population and Quantitative Health Sciences, Case Western Reserve University\nMay 2022 - December 2022\n\nMedical Observer\nKauvery Hospitals, Chennai, India\nOctober 2020 - March 2021\n\nJunior Resident Doctor\nDepartment of Psychiatry, Saveetha Medical College, India\nJuly 2020 - September 2020\n\nIntern Doctor\nMadras Medical College, Chennai, India\nMarch 2019 - March 2020"
  },
  {
    "objectID": "blog_main.html",
    "href": "blog_main.html",
    "title": "My Blog Posts.",
    "section": "",
    "text": "Secure SFTP Backups with pgBackRest for PostgreSQL: A Step-by-Step Guide\n\n\n\n\n\n\n\nPostgreSQL\n\n\npgBackRest\n\n\nDatabase Backups\n\n\nLinux Administration\n\n\nDevOps\n\n\nSecurity\n\n\n\n\nA guide to configuring pgBackRest to securely perform SFTP based backups of your PostgreSQL server.\n\n\n\n\n\n\nMay 14, 2025\n\n\nNaveen Kannan\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to PXE boot servers.\n\n\n\n\n\n\n\nComputing\n\n\nsysadmin\n\n\nDnsmasq\n\n\nDHCP\n\n\nPXE\n\n\nTFTP\n\n\nBIOS\n\n\n\n\nA basic introduction to Pre-Execution Environment Servers.\n\n\n\n\n\n\nMar 18, 2024\n\n\nNaveen Kannan\n\n\n\n\n\n\n  \n\n\n\n\nIntegrating HDFS and PostgreSQL through Apache Spark.\n\n\n\n\n\n\n\nComputing\n\n\nHDFS\n\n\nPostgreSQL\n\n\nSpark\n\n\nRDBMS\n\n\nBig Data\n\n\n\n\nA guide to leveraging Spark’s inferSchema tool in conjunction with the HDFS to streamline PostgreSQL database schema/table creation.\n\n\n\n\n\n\nMar 18, 2024\n\n\nNaveen Kannan\n\n\n\n\n\n\n  \n\n\n\n\nMamba implementation in Scientific Pipelines.\n\n\n\n\n\n\n\nComputing\n\n\nContainers\n\n\nMamba\n\n\nConda\n\n\n\n\nA guide to using Mamba (over Conda) for pipeline building.\n\n\n\n\n\n\nNov 18, 2023\n\n\nNaveen Kannan\n\n\n\n\n\n\n  \n\n\n\n\nMoving Docker’s Data directory to another location.\n\n\n\n\n\n\n\nComputing\n\n\nContainers\n\n\nDiscussion\n\n\n\n\nA guide to transferring the Docker data storage to another location while preserving the existing images and containers.\n\n\n\n\n\n\nOct 30, 2023\n\n\nNaveen Kannan\n\n\n\n\n\n\n  \n\n\n\n\nInstalling and configuring the HIVE metastore with a MySQL backend.\n\n\n\n\n\n\n\nComputing\n\n\nClusters\n\n\nDiscussion\n\n\nHIVE\n\n\nSpark\n\n\nSQL\n\n\nDatabase\n\n\n\n\nA guide to configuring Hive Metastore to use a MySQL server as the backend RDBMS for metadata storage, while enabling Spark to connect to the Metastore.\n\n\n\n\n\n\nSep 22, 2023\n\n\nNaveen Kannan\n\n\n\n\n\n\n  \n\n\n\n\nUsing Ansible to install Hive on a Spark cluster.\n\n\n\n\n\n\n\nComputing\n\n\nClusters\n\n\nDiscussion\n\n\nAnsible\n\n\nHive\n\n\nSpark\n\n\n\n\nA detailed view of Ansible playbooks with a highly relevant example.\n\n\n\n\n\n\nSep 4, 2023\n\n\nNaveen Kannan\n\n\n\n\n\n\n  \n\n\n\n\nInstalling and configuring Hadoop and Spark on a 4 node cluster.\n\n\n\n\n\n\n\nComputing\n\n\nClusters\n\n\nDiscussion\n\n\nHadoop\n\n\nSpark\n\n\nHDFS\n\n\nYARN\n\n\n\n\nA guide to installing Hadoop and Spark on a 4 node cluster, while configuring and setting up HDFS, YARN and MapReduce.\n\n\n\n\n\n\nAug 21, 2023\n\n\nNaveen Kannan\n\n\n\n\n\n\n  \n\n\n\n\nUsing Ansible to remotely configure a cluster.\n\n\n\n\n\n\n\nComputing\n\n\nClusters\n\n\nDiscussion\n\n\nAnsible\n\n\nDocker\n\n\nContainers\n\n\n\n\nUsing a containerized instance of Ansible to remotely connect to a cluster and perform a simple ping task to confirm connection.\n\n\n\n\n\n\nJun 24, 2023\n\n\nNaveen Kannan\n\n\n\n\n\n\n  \n\n\n\n\nDocker, Singularity, and HPC.\n\n\n\n\n\n\n\nComputing\n\n\nHPC\n\n\nContainers\n\n\nDiscussion\n\n\nDocker\n\n\nSingularity\n\n\n\n\nA brief rundown of Docker and Singularity, and their relevance in HPC environments.\n\n\n\n\n\n\nJun 17, 2023\n\n\nNaveen Kannan\n\n\n\n\n\n\n  \n\n\n\n\nSLURM and HPC.\n\n\n\n\n\n\n\nComputing\n\n\nHPC\n\n\nDiscussion\n\n\nClusters\n\n\n\n\nAn introduction to SLURM in the context of HPC clusters.\n\n\n\n\n\n\nJun 15, 2023\n\n\nNaveen Kannan\n\n\n\n\n\n\n  \n\n\n\n\np values, Statistical Significance, and the magic number.\n\n\n\n\n\n\n\nStatistics\n\n\nDiscussion\n\n\n\n\nA brief exploration on p-values.\n\n\n\n\n\n\nMar 22, 2023\n\n\nNaveen Kannan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Naveen Kannan",
    "section": "",
    "text": "I’m a Database Engineer at Trailhead Biosystems, Cleveland. My research interests include Deep Learning, Bioinformatics, Biostatistics and Geospatial Analytics. In my spare time I enjoy art, swimming, hiking and cooking.\nThis is my personal little nerd cave. I like to share the work I do on my blog. I hope you find something useful here!"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Naveen Kannan",
    "section": "Education",
    "text": "Education\nMS in Biomedical and Health Informatics\nCase Western Reserve University | Cleveland - Ohio\nJanuary 2022 - August 2023\nMBBS (Bachelor of Medicine and Bachelor of Surgery)\nMadras Medical College | Chennai, India\nSeptember 2016 - March 2020"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Naveen Kannan",
    "section": "Experience",
    "text": "Experience\nDatabase Engineer\nTrailhead Biosystems, Cleveland, Ohio\nJuly 2024 - Present\n\nResearch Associate\nDepartment of Population and Quantitative Health Sciences, Case Western Reserve University\nMay 2023 - July 2024\n\nGraduate Research Assistant\nDepartment of Population and Quantitative Health Sciences, Case Western Reserve University\nJanuary 2023 - May 2023\n\nGraduate Research Assistant\nGIS Health and Hazards Lab, Department of Population and Quantitative Health Sciences, Case Western Reserve University\nMay 2022 - December 2022\n\nMedical Observer\nKauvery Hospitals, Chennai, India\nOctober 2020 - March 2021\n\nJunior Resident Doctor\nDepartment of Psychiatry, Saveetha Medical College, India\nJuly 2020 - September 2020\n\nIntern Doctor\nMadras Medical College, Chennai, India\nMarch 2019 - March 2020"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Naveen Kannan’s CV",
    "section": "",
    "text": "Naveen Kannan\n\nUnited States Permanent Resident - Cleveland, Ohio\n\nData Scientist with extensive experience working with R, Python, GIS, and SQL. Expertise in visualizing data and synthesizing data into high-impact graphics and plots using ggplot. Able to create accessible and readable maps utilizing big spatial data and GIS. Able to execute and automate data extraction and analysis utilizing data mining and algorithmic techniques. Extensive experience in creating and interpreting different types of regression models. Significant experience conducting statistical analysis on healthcare data. Adept at synthesizing complex data into actionable reporting, proposals, and specification writing.\n\n\n\nSkills\n\nPython: PANDAS, Keras, Numpy, scikitlearn, PySpark\nR: Advanced Statistical Analysis, Regression Analysis, R Markdown\nggplot: Creating High-impact plots, graphics, and tables\nSQL: PostgreSQL querying\nLinux: Familiarity with CLI, shell scripting, file system management\nDocker: Experience with creating, building and running docker containers\nAlgorithmic and Data Mining techniques\nScientific Writing for Publications\nExcellent knowledge of big data technologies like Hadoop, Spark and NoSQL databases"
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Geospatial Analytics",
    "section": "",
    "text": "This dashboard maps the environmental impact of the 2023 Norfolk Southern train derailment in East Palestine, Ohio on the states of Ohio, West Virginia and Pennsylvania across a two month period.\nThis project was coded entirely by me, with the interactive dashboard built in R using Shiny apps, and geo-visualization performed using GIS programming with Python and R.\nData for the project was obtained by coding scripts that scraped the internet for government created, publicly available datasets. Primarily, data was obtained from the USGS (United States Geological Survey) and AirNow.\nFind it here.\nShiny Widgets and Dashboard are dynamic, so I cannot directly render the dashboard here. However, it’s easily available at the above link!\n\n\n\n\n\nA screencap of the dashboard. It allows for visualization of a choropleth map for the zipcodes within a 30 mile buffer zone around the train derailment site in East Palestine, Ohio. It allows for visualization of the trend of the changes in AQI values across the months of January and February, 2023.\n\n\n\n\n\nThis part of the dashboard is a choropleth map that displays the air quality indices of the counties within a 30 mile buffer zone around the derailment site.\nThis map is interactive. When a county is clicked, it displays the air quality trend for the county across the study period, with indicators showing the selected date and the date of the derailment, in the form of a time series plot.\n\nThe dashboard has a date slider, allowing for visualization of the choropleth map and it’s accompanying time series plot on the selected date.\n\n\n\n\n\nThis dashboard was built as part of the final project for PQHS 427 at Case Western Reserve University, Department of Population Health and Quantitative Health Sciences.\nFind the GitHub Repo here. This includes the datasets I used.\n\n\n\nThe following maps have been made using ArcGIS and open source applications such as QGIS.\n\n\n\n\n\n\n\n\n\n\n\n\n\nI worked with the GIS Health and Hazards Lab in the Department of Population and Quantitative Health Sciences to work on mapping refugee camp sites in the Democratic Republic of Congo following the eruption of Mount Nyiragongo on Saturday, May 2021.\nI created Python scripts for a multitude of tasks, including object detection and video frame parsing. These Python scripts can be found on my Github page, at the following link:\nhttps://github.com/naveenk2022/GIS-repository\nThe paper containing a detailed description of the work done is currently being worked on, and I will share details and images when that happens!"
  },
  {
    "objectID": "portfolio.html#mapping-the-environmental-impact-of-the-2023-norfolk-southern-train-derailment-in-east-palestine-ohio",
    "href": "portfolio.html#mapping-the-environmental-impact-of-the-2023-norfolk-southern-train-derailment-in-east-palestine-ohio",
    "title": "Geospatial Analytics",
    "section": "",
    "text": "This dashboard maps the environmental impact of the 2023 Norfolk Southern train derailment in East Palestine, Ohio on the states of Ohio, West Virginia and Pennsylvania across a two month period.\nThis project was coded entirely by me, with the interactive dashboard built in R using Shiny apps, and geo-visualization performed using GIS programming with Python and R.\nData for the project was obtained by coding scripts that scraped the internet for government created, publicly available datasets. Primarily, data was obtained from the USGS (United States Geological Survey) and AirNow.\nFind it here.\nShiny Widgets and Dashboard are dynamic, so I cannot directly render the dashboard here. However, it’s easily available at the above link!\n\n\n\n\n\nA screencap of the dashboard. It allows for visualization of a choropleth map for the zipcodes within a 30 mile buffer zone around the train derailment site in East Palestine, Ohio. It allows for visualization of the trend of the changes in AQI values across the months of January and February, 2023.\n\n\n\n\n\nThis part of the dashboard is a choropleth map that displays the air quality indices of the counties within a 30 mile buffer zone around the derailment site.\nThis map is interactive. When a county is clicked, it displays the air quality trend for the county across the study period, with indicators showing the selected date and the date of the derailment, in the form of a time series plot.\n\nThe dashboard has a date slider, allowing for visualization of the choropleth map and it’s accompanying time series plot on the selected date.\n\n\n\n\n\nThis dashboard was built as part of the final project for PQHS 427 at Case Western Reserve University, Department of Population Health and Quantitative Health Sciences.\nFind the GitHub Repo here. This includes the datasets I used."
  },
  {
    "objectID": "portfolio.html#arcgis-and-qgis",
    "href": "portfolio.html#arcgis-and-qgis",
    "title": "Geospatial Analytics",
    "section": "",
    "text": "The following maps have been made using ArcGIS and open source applications such as QGIS."
  },
  {
    "objectID": "portfolio.html#python-and-geospatial-analytics",
    "href": "portfolio.html#python-and-geospatial-analytics",
    "title": "Geospatial Analytics",
    "section": "",
    "text": "I worked with the GIS Health and Hazards Lab in the Department of Population and Quantitative Health Sciences to work on mapping refugee camp sites in the Democratic Republic of Congo following the eruption of Mount Nyiragongo on Saturday, May 2021.\nI created Python scripts for a multitude of tasks, including object detection and video frame parsing. These Python scripts can be found on my Github page, at the following link:\nhttps://github.com/naveenk2022/GIS-repository\nThe paper containing a detailed description of the work done is currently being worked on, and I will share details and images when that happens!"
  },
  {
    "objectID": "portfolio.html#an-analysis-of-the-population-assessment-of-tobacco-and-health-path-study.",
    "href": "portfolio.html#an-analysis-of-the-population-assessment-of-tobacco-and-health-path-study.",
    "title": "Geospatial Analytics",
    "section": "An analysis of the Population Assessment of Tobacco and Health (PATH) Study.",
    "text": "An analysis of the Population Assessment of Tobacco and Health (PATH) Study.\nThe Population Assessment of Tobacco and Health (PATH) Study began originally surveying 45,971 adult and youth respondents. The study sampled over 150,000 mailing addresses across the United States to create a national sample of tobacco users and non-users, and is a collaboration between the National Institute on Drug Abuse (NIDA), National Institutes of Health (NIH), and the Center for Tobacco Products (CTP), Food and Drug Administration (FDA).\nThis project looks at predicting e-cigarette use among adults in the United States.\nI asked, and answered, the following questions:\nE-Cigarette Perception, Smoking Habits, and their association with Heavy E-Cigarette Use:\nAre smoking e-cigarettes that contain nicotine, the perception of the purported healthiness of e-cigarettes when compared to smoking regular cigarettes, and smoking habits strong predictors of heavy e-cigarette use in adulthood?\nRegular e-cigarette use and it’s associated factors:\nAre factors such as using flavored e-cigarettes, or needing to use e-cigarettes immediately after waking up, or using e-cigarettes with nicotine, or use of e-cigarettes as a healthier alternative to regular cigarettes associated with regular e-cigarette use?\nFind it here."
  },
  {
    "objectID": "portfolio.html#nhanes-national-health-and-nutrition-examination-survey",
    "href": "portfolio.html#nhanes-national-health-and-nutrition-examination-survey",
    "title": "Geospatial Analytics",
    "section": "NHANES (National Health and Nutrition Examination Survey)",
    "text": "NHANES (National Health and Nutrition Examination Survey)\nThe National Health and Nutrition Examination Survey (NHANES) is a program of studies designed to assess the health and nutritional status of adults and children in the United States. The survey is unique in that it combines interviews and physical examinations. NHANES is a major program of the National Center for Health Statistics (NCHS). NCHS is part of the Centers for Disease Control and Prevention (CDC) and has the responsibility for producing vital and health statistics for the Nation.\nThis projectAnalysing the relationship between Blood Cholesterol levels, Physical Activity, Excessive Drinking and Depression, and can be found here.\nI asked and answering the following question:\nAmong adults of ages 21-79 participating in NHANES 2017-18, is there a significant difference in the values of HDL cholesterol values of participants across their self-reported participation or lack thereof in vigorous weekly physical activity?"
  },
  {
    "objectID": "portfolio.html#scooby-doo-analytics",
    "href": "portfolio.html#scooby-doo-analytics",
    "title": "Geospatial Analytics",
    "section": "Scooby-Doo analytics",
    "text": "Scooby-Doo analytics\n“Scooby-Doo” is a television series that has been airing for over 50 years. Centered around Mystery Inc.,a group of iconic mystery solving detectives, including Fred, Daphne, Velma, Shaggy, and the titular Scooby-Doo, a talking dog with a penchant for consuming ridiculously tall sandwiches and Scooby snacks.\nThe data comes from Kaggle, and was part of Tidy Tuesday’s dataset for 2021-07-13, and can be found here.\nI decided to use a more light-hearted dataset for this. However, I did perform thorough analysis of the data I had!\nI asked, and answered, the following questions:\nPredicting an iconic catchphrase: Are the logistics of an episode of Scooby-Doo good predictors of the number of times an iconic catchphrase is spoken?\nMonster Motivation: Is it possible to predict the motive of the major antagonist of an episode of Scooby-Doo, based on the nature of the monster the antagonist appears as?\n\n\n\nThe Mystery Inc. Gang. From left to right, Velma, Shaggy, Scooby Doo, Fred, and Daphne.\n\n\nFind the project here!"
  },
  {
    "objectID": "portfolio.html#creating-a-publication-ready-table-1",
    "href": "portfolio.html#creating-a-publication-ready-table-1",
    "title": "Geospatial Analytics",
    "section": "Creating a Publication-Ready Table 1",
    "text": "Creating a Publication-Ready Table 1\nThe following table was created from a simulated dataset containing data on individuals with a hypertension diagnosis, receiving primary care at two primary health practices.\n\n\n\n\n\n\n  \n    \n      Table 1. Baseline characteristics of 864 individuals with a diagnosis of hypertension, receiving primary care at Highland and Sycamore practices.\n    \n    \n      Data taken from a simulated dataset.\n    \n    \n      Characteristics\n      Highland\n      Sycamore\n      p\n    \n  \n  \n    \n      Demographic Characteristics\n    \n    no.1\n432\n432\n\n    Age - Years(Median, [IQR])2\n55.0 [47.8, 63.0]\n64.5 [56.0, 73.0]\n&lt;0.001\n    Race - no.(%)3,1\n\n\n&lt;0.001\n       White\n367 (85.0)\n53 (13.2)\n\n       AA_Black\n52 (12.0)\n335 (83.5)\n\n       Asian\n12 (2.8)\n3 (0.7)\n\n       Other\n1 (0.2)\n10 (2.5)\n\n    Hispanic/Latino Ethnicity - no. (%)3,1\n93 (21.5)\n0 (0.0)\n&lt;0.001\n    Male sex - no. (%)1\n178 (41.2)\n148 (34.3)\n0.042\n    Insurance Provider - no. (%)1\n\n\n&lt;0.001\n       Medicaid\n161 (37.3)\n0 (0.0)\n\n       Medicare\n137 (31.7)\n192 (44.4)\n\n       Commercial\n74 (17.1)\n240 (55.6)\n\n       Uninsured\n60 (13.9)\n0 (0.0)\n\n    \n      Health Characteristics\n    \n    Body Mass Index - kg/m^2(Median, [IQR])3,4,2\n31.9 [27.3, 37.7]\n30.7 [26.8, 36.4]\n0.064\n    BMI Category - no. (%)3,1\n\n\n0.234\n       Underweight\n4 (0.9)\n2 (0.5)\n\n       Healthy Weight\n53 (12.5)\n69 (16.0)\n\n       Overweight\n103 (24.2)\n116 (26.9)\n\n       Obese\n265 (62.4)\n245 (56.7)\n\n    Systolic Blood Pressure - mm/Hg(Median, [IQR])2,5\n130.5 [118.8, 142.2]\n130.0 [120.0, 138.0]\n0.373\n    Diastolic Blood Pressure - mm/Hg(Median, [IQR])2,5\n79.0 [72.0, 84.0]\n74.0 [67.0, 81.0]\n&lt;0.001\n  \n  \n  \n    \n      1 Numbers are No. (%) unless otherwise noted.\n    \n    \n      2 IQR = Interquantile Range.\n    \n    \n      3 These variables are missing no more than 4% of their values.\n    \n    \n      4 kg/m^2 = Kilograms per square Meter.\n    \n    \n      5 mm/Hg = Millimeters of mercury."
  },
  {
    "objectID": "portfolio.html#ihpba-international-hepato-pancreato-biliary-association-2016",
    "href": "portfolio.html#ihpba-international-hepato-pancreato-biliary-association-2016",
    "title": "Geospatial Analytics",
    "section": "IHPBA (International Hepato-Pancreato Biliary Association) 2016",
    "text": "IHPBA (International Hepato-Pancreato Biliary Association) 2016\nI presented my paper(Kannan, N., Vellaisamy, R., Govindarajan, M., & Gounder, K. D. (2016). Pellagra following pancreaticoduodenectomy for malignant pancreatic carcinoid with pluripotent hormonal potential. HPB, 18, e381-e382.) at IHPBA’s 12th World Congress at Sao Paulo, Brazil."
  },
  {
    "objectID": "portfolio.html#ashg-american-society-of-human-genetics-2023",
    "href": "portfolio.html#ashg-american-society-of-human-genetics-2023",
    "title": "Geospatial Analytics",
    "section": "ASHG (American Society of Human Genetics) 2023",
    "text": "ASHG (American Society of Human Genetics) 2023\nAnnotation and scoring of the deleteriousness of individual genetic variants in the 4th release of the Alzheimer’s Disease Sequencing Project. (PB4451)\nNaveen Kannan1, Nicholas Wheeler1, Genome Center for Alzheimer’s Disease, Li-San Wang2, Yuk Yee Leung2, William S. Bush1\n\nCleveland Institute for Computational Biology, Department for Population and Quantitative Health Sciences, Case Western Reserve University, Cleveland, Ohio 44106, USA.\n\nDepartment of Pathology and Laboratory Medicine, Penn Neurodegeneration Genomics Center, Perelman School of Medicine, University of Pennsylvania, Philadelphia, Pennsylvania 19104, USA.\n\nPresented at the Annual Meeting of The American Society of Human Genetics, November 3, 2023 in Washington DC.\n\n\n\nWith my amazing professor, Dr William S Bush!"
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(comment = NA) \nlibrary(broom)\nlibrary(janitor) \nlibrary(naniar)\nlibrary(glue)\nlibrary(rsample)\nlibrary(yardstick)\nlibrary(GGally)\nlibrary(mice)\nlibrary(car)\nlibrary(pROC)\nlibrary(patchwork)\nlibrary(rms)\nlibrary(simputation)\nlibrary(kableExtra)\nlibrary(tidyverse)\ntheme_set(theme_bw())"
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#r-packages-and-setup",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#r-packages-and-setup",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(comment = NA) \nlibrary(broom)\nlibrary(janitor) \nlibrary(naniar)\nlibrary(glue)\nlibrary(rsample)\nlibrary(yardstick)\nlibrary(GGally)\nlibrary(mice)\nlibrary(car)\nlibrary(pROC)\nlibrary(patchwork)\nlibrary(rms)\nlibrary(simputation)\nlibrary(kableExtra)\nlibrary(tidyverse)\ntheme_set(theme_bw())"
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#loading-the-raw-data",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#loading-the-raw-data",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "3.1 Loading the Raw Data",
    "text": "3.1 Loading the Raw Data\nAccess to the data requires users to log-in to the ICPSR website. Thus, a direct download is not possible. This data can be freely downloaded by the public, and therefore a local copy of the data in the form of a tsv file was read using read_tsv from the tidyverse.\n\n\nCode\n#data_raw &lt;- read_tsv(\"36498-1001-Data.tsv\", show_col_types = FALSE)\n#saveRDS(data_raw, \"36498-1001-Data.Rds\")\ndata_raw = readRDS(\"36498-1001-Data.Rds\")"
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#cleaning-the-data",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#cleaning-the-data",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "3.2 Cleaning the Data",
    "text": "3.2 Cleaning the Data\n\n3.2.1 Selecting Variables We’ll Use\n\n\nCode\ndata_var &lt;- data_raw |&gt;\n  select(PERSONID,R01_AE1022,R01_AE0103,R01R_A_AE1006,R01_AE1062,R01_AE1100,R01_AE1099,R01_AE9004,R01_AE9010,R01_AE1050,R01R_A_MINFIRST_ECIG, R01_AE1003)\n\n\n\n\n3.2.2 Dealing with Missingness\nThe DS1001 dataset provides negative values for variables that are missing. There are several different subtypes of missing data:\n\nData removed due to respondent request\nData not ascertained\nRespondent answer is “Don’t Know”\nRespondent refused to answer\nQuestion inapplicable to respondent\n\nAll of these values are negative in the original DS1001 dataset, and none of the variables under study have negative values. It is also not possible for the variables under study to have negative values. Therefore, all negative values were replaced with NA to ensure missing data are correctly identified.\n\n\nCode\ndata_var &lt;- data_var|&gt; \n  mutate(across(everything(), function(x){replace(x, which(x&lt;0), NA)}))\n\n\n\n\n3.2.3 Changing Variable Names\nThe variables were renamed to have better, more readable names.\n\n\nCode\ndata_var &lt;- data_var|&gt;\n  mutate(ecig_30day = as.integer(R01_AE1022),\n         puff_num = as.integer(R01_AE0103),\n         age_range = factor(R01R_A_AE1006),\n         less_harm = factor(R01_AE1062),\n         ecig_reg = factor(R01_AE1100),\n         harm_perc = factor(R01_AE1099),\n         price_paid = factor(R01_AE9004),\n         personid = as.character(PERSONID),\n         ecig_nico = as.factor(R01_AE9010),\n         ecig_flavored = as.factor(R01_AE1050),\n         min_num = as.integer(R01R_A_MINFIRST_ECIG),\n         ecig_smoker = factor(R01_AE1003)) |&gt;\n  select(personid, ecig_30day,puff_num,age_range,less_harm,ecig_reg,harm_perc,price_paid,ecig_nico, ecig_flavored, min_num, ecig_smoker)\n\ndata_var &lt;- data_var |&gt;\n  clean_names() \n\n\n\n\n3.2.4 Sampling the Data\nThe dataset was filtered to select only individuals who were users of e-cigarettes at the time of study. The dataset was then filtered to have complete cases on both the outcomes under study, namely ecig_30day and ecig_reg.\n\n\nCode\ndata_outcome &lt;- data_var |&gt;\n  filter(ecig_smoker == 1 | ecig_smoker == 2)|&gt; # Excluding individuals who did not use e-cigarettes at the time of Wave 1 \n  filter(complete.cases(ecig_30day, ecig_reg)) |&gt;\n  select(personid, ecig_30day,puff_num,age_range,less_harm,ecig_reg,harm_perc,price_paid,ecig_nico, ecig_flavored, min_num, ecig_smoker)\ndim(data_outcome)\n\n\n[1] 3000   12\n\n\n3000 respondents have information on both the outcome variables under study. After setting a seed for reproducibility, a random sample of 1200 respondents was taken from the data_outcome tibble. This was also done to ensure that the same respondents are analyzed for both regression models.\n\n\nCode\nset.seed(2023) \ndata_complete &lt;- slice_sample(data_outcome, n = 1200) \n\n\nOnce a random sample was taken, the tabyl function was used to ensure that all individuals in the tibble data_complete were individuals who had used e-cigarettes.\n\n\nCode\ndata_complete |&gt;\n  tabyl(ecig_smoker) |&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\necig_smoker\nn\npercent\n\n\n\n\n1\n41\n0.0341667\n\n\n2\n1159\n0.9658333\n\n\n3\n0\n0.0000000\n\n\n\n\n\n\n\nFrom the documentation for the original variable name for ecig_smoker(R01_AE1003), the question asked was, ‘Do you now use e-cigarettes?’, with three possible levels.\n\n\n\nValue\nLabel\nFrequency\n\n\n\n\n1\n1 = Every day\n41\n\n\n2\n2 = Some days\n1159\n\n\n3\n3 = Not at all\n0\n\n\n\nThe above table shows the distribution of the responses to the question, demonstrating that all the individuals under study are individuals who were using e-cigarettes at the time of the Wave 1 questionnaire. Individuals who did not use e-cigarettes at the time are not included in the random sample of 1200 respondents.\n\n\n3.2.5 Working with Categorical Predictors\nThe tabyl function was used to check if all factor variables have atleast 30 observations at each level.\n\n\nCode\ndata_complete |&gt; \n  tabyl(age_range)|&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nage_range\nn\npercent\nvalid_percent\n\n\n\n\n1\n54\n0.0450000\n0.0453020\n\n\n2\n365\n0.3041667\n0.3062081\n\n\n3\n276\n0.2300000\n0.2315436\n\n\n4\n181\n0.1508333\n0.1518456\n\n\n5\n169\n0.1408333\n0.1417785\n\n\n6\n147\n0.1225000\n0.1233221\n\n\nNA\n8\n0.0066667\nNA\n\n\n\n\n\n\n\n\n\nCode\ndata_complete |&gt; \n  tabyl(less_harm) |&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nless_harm\nn\npercent\nvalid_percent\n\n\n\n\n1\n955\n0.7958333\n0.7998325\n\n\n2\n239\n0.1991667\n0.2001675\n\n\nNA\n6\n0.0050000\nNA\n\n\n\n\n\n\n\n\n\nCode\ndata_complete |&gt; \n  tabyl(ecig_reg) |&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\necig_reg\nn\npercent\n\n\n\n\n1\n369\n0.3075\n\n\n2\n831\n0.6925\n\n\n\n\n\n\n\n\n\nCode\ndata_complete |&gt; \n  tabyl(harm_perc) |&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nharm_perc\nn\npercent\nvalid_percent\n\n\n\n\n1\n822\n0.6850000\n0.6919192\n\n\n2\n343\n0.2858333\n0.2887205\n\n\n3\n23\n0.0191667\n0.0193603\n\n\nNA\n12\n0.0100000\nNA\n\n\n\n\n\n\n\nThe harm_perc variable was found to have a level 3 which only had 23 observations. Factor levels 2 and 3 for harm_perc were collapsed into a single category.\n\n\nCode\ndata_complete$harm_perc &lt;- fct_collapse(data_complete$harm_perc,\n  '2' = c(\"2\", \"3\"),\n  '1' = \"1\")\n\n\nNow, harm_perc has only 2 levels, each with more than 30 distinct values.\n\n\nCode\ndata_complete |&gt; \n  tabyl(harm_perc)|&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nharm_perc\nn\npercent\nvalid_percent\n\n\n\n\n1\n822\n0.685\n0.6919192\n\n\n2\n366\n0.305\n0.3080808\n\n\nNA\n12\n0.010\nNA\n\n\n\n\n\n\n\n\n\nCode\ndata_complete |&gt; \n  tabyl(price_paid)|&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nprice_paid\nn\npercent\nvalid_percent\n\n\n\n\n1\n189\n0.1575000\n0.2342007\n\n\n2\n260\n0.2166667\n0.3221809\n\n\n3\n328\n0.2733333\n0.4064436\n\n\n4\n30\n0.0250000\n0.0371747\n\n\nNA\n393\n0.3275000\nNA\n\n\n\n\n\n\n\n\n\nCode\ndata_complete |&gt; \n  tabyl(ecig_nico)|&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\necig_nico\nn\npercent\nvalid_percent\n\n\n\n\n1\n320\n0.2666667\n0.8695652\n\n\n2\n48\n0.0400000\n0.1304348\n\n\nNA\n832\n0.6933333\nNA\n\n\n\n\n\n\n\n\n\nCode\ndata_complete |&gt; \n  tabyl(ecig_flavored)|&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\necig_flavored\nn\npercent\nvalid_percent\n\n\n\n\n1\n246\n0.2050\n0.6666667\n\n\n2\n123\n0.1025\n0.3333333\n\n\nNA\n831\n0.6925\nNA\n\n\n\n\n\n\n\nThe other factor variables were found to have at least 30 observations at each level. Furthermore, none of the categorical variables have more than 6 levels.\n\n\n3.2.6 Working with Quantitative Variables\n\n\nCode\ndata_complete |&gt; \n  select(personid, ecig_30day, puff_num, min_num) |&gt;\n  summarize(across(personid:min_num, ~ n_distinct(.)))\n\n\n# A tibble: 1 × 4\n  personid ecig_30day puff_num min_num\n     &lt;int&gt;      &lt;int&gt;    &lt;int&gt;   &lt;int&gt;\n1     1200         28       29      31\n\n\nThe quantitative variables ecig_30day,puff_num and min_num have at least 10 different, ordered, observed values.\n\n\nCode\ndata_complete |&gt; \n  select(personid, ecig_30day, puff_num, min_num) |&gt;\n  miss_var_summary() |&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nvariable\nn_miss\npct_miss\n\n\n\n\nmin_num\n853\n71.08333\n\n\npuff_num\n849\n70.75000\n\n\npersonid\n0\n0.00000\n\n\necig_30day\n0\n0.00000\n\n\n\n\n\n\n\nThe miss_var_summary function from the naniar package was used to identify the number of missing values for the continuous variables under study.\n\n\n3.2.7 Renaming the factors\nThe categorical variables had their factors renamed to make the data more readable. The final tidy tibble was designated as data_final.\n\n\nCode\ndata_final &lt;- data_complete|&gt;\n  mutate(age_range = fct_recode(factor(age_range),\n                                \"&lt;18\" = \"1\",\n                                \"18to24\" = \"2\",\n                                \"25to34\" = \"3\",\n                                \"35to44\" = \"4\",\n                                \"45to54\" = \"5\",\n                                \"&gt;55\" = \"6\"),\n         less_harm = fct_recode(factor(less_harm),\n                                \"Yes\" = \"1\",\n                                \"No\" = \"2\"),\n         ecig_reg = fct_recode(factor(ecig_reg),\n                               \"Yes\" = \"1\",\n                               \"No\" = \"2\"),\n         harm_perc = fct_recode(factor(harm_perc),\n                                \"less_harm\" = \"1\",\n                                \"same_or_more\" = \"2\"),\n         price_paid = fct_recode(factor(price_paid),\n                                 \"Below$10\" = \"1\",\n                                 \"$10to$20\" = \"2\",\n                                 \"$21to$100\" = \"3\",\n                                 \"&gt;$100\" = \"4\"),\n         ecig_nico = fct_recode(factor(ecig_nico),\n                                \"Yes\" = \"1\",\n                                \"No\" = \"2\"),\n         ecig_flavored = fct_recode(factor(ecig_flavored),\n                                \"Yes\" = \"1\",\n                                \"No\" = \"2\"))\n\n\n\n\n3.2.8 Splitting the Data\nOnce a random sample of 1200 respondents was taken, the data was separated into two tibbles for the purpose of describing the data under study for the two models. data_log denotes the dataset for the logistic regression, and data_lin denotes the dataset for the linear regression.\n\n\nCode\ndata_lin &lt;- data_final|&gt;\n  select(personid,ecig_30day,puff_num,age_range,less_harm,ecig_nico)\ndata_log &lt;- data_final|&gt;\n  select(personid,ecig_reg,min_num,harm_perc,price_paid,ecig_nico,ecig_flavored)"
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#listing-the-tibble",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#listing-the-tibble",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "4.1 Listing the Tibble",
    "text": "4.1 Listing the Tibble\nThe tidy tibble, data_final, was listed.\n\n\nCode\ndata_final \n\n\n# A tibble: 1,200 × 12\n   personid   ecig_30day puff_num age_range less_harm ecig_reg harm_perc   \n   &lt;chr&gt;           &lt;int&gt;    &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;     &lt;fct&gt;    &lt;fct&gt;       \n 1 P000319879          1       NA &lt;18       Yes       No       less_harm   \n 2 P000255041          0       NA 25to34    Yes       Yes      less_harm   \n 3 P000413552          2       NA 35to44    Yes       No       same_or_more\n 4 P000239368          1       NA 45to54    Yes       No       less_harm   \n 5 P000334022          1       NA 35to44    Yes       No       less_harm   \n 6 P000151148         25       NA 45to54    Yes       Yes      less_harm   \n 7 P000423889          2       NA 45to54    No        No       same_or_more\n 8 P000264820         24        4 35to44    No        No       less_harm   \n 9 P000321341          2       NA 45to54    No        No       same_or_more\n10 P000200073          2       NA 25to34    No        No       same_or_more\n# ℹ 1,190 more rows\n# ℹ 5 more variables: price_paid &lt;fct&gt;, ecig_nico &lt;fct&gt;, ecig_flavored &lt;fct&gt;,\n#   min_num &lt;int&gt;, ecig_smoker &lt;fct&gt;"
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#size-and-identifiers",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#size-and-identifiers",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "4.2 Size and Identifiers",
    "text": "4.2 Size and Identifiers\nThere are 1200 rows, with 12 columns in the final tibble data_final. One of these columns\n\n\nCode\ndim(data_final)\n\n\n[1] 1200   12\n\n\nThe personid variable provides the identifier for each row in the tidy tibble. The n_distinct function was used to demonstrate 1200 individual values for personid in data_final, indicating that each row has a unique personid value.\n\n\nCode\nn_distinct(data_final$personid)\n\n\n[1] 1200"
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#save-the-tibble",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#save-the-tibble",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "4.3 Save The Tibble",
    "text": "4.3 Save The Tibble\nThe saveRDS function was used to save the tidy tibble in a .rds file.\n\n\nCode\nwrite_rds(data_final, file = \"data_final.Rds\")"
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#defining-the-variables",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#defining-the-variables",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "5.1 Defining the Variables",
    "text": "5.1 Defining the Variables\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nOriginal DS1001 code\nDescription | Type | Analytic Role | Missing values\n\n\npersonid\nPERSONID\nUnique Participant Identifier\nCharacter variable\nUnique Identifier\n0\n\n\n\necig_30day\nR01_AE1022\nOn how many of the past 30 days did the participant use an e-cigarette?\nQuantitative Variable\nLinear Regression Outcome\n0\n\n\n\npuff_num\nR01_AE0103\nHow many puffs from an e-cigarette did the participant take the last time they smoked an e-cigarette?\nQuantitative Variable\nPredictor\n849\n\n\n\nage_range\nR01R_A_AE1006\nHow old was the participant when they first used an e-cigarette, even one or two times?\n6 levels:&lt;18,18to24,\n25to34,35to44,\n45to54,&gt;55\nMulti-Categorical Variable with 6 factors\nPredictor\n8\n\n\n\nless_harm\nR01_AE1062\nDid the participant use / used to use e-cigarettes because they might be / have been less harmful to them?\n2 levels: yes, no\nBinary Categorical Variable\nPredictor\n6\n\n\n\necig_reg\nR01_AE1100\nHas the participant ever used e-cigarettes fairly regularly?\n2 levels:yes,no\nBinary Categorical Variable\nLogistic regression Outcome\n0\n\n\n\nharm_perc\nR01_AE1099\nTo the participant, is using e-cigarettes less harmful, about the same, or more harmful than smoking cigarettes?\n2 levels:1,2\nBinary Categorical Variable\nPredictor\n12\n\n\n\nprice_paid\nR01_AE9004\nAbout how much did the participant pay for their e-cigarette?\n4 levels: Below$10,$10to$20,\n$21t$100, &gt;$100\nMulti-Categorical Variable with 4 factors\nPredictor\n393\n\n\n\necig_nico\nR01_AE9010\nDoes the e-cigarette the participant usually uses contain nicotine?\n2 levels:yes,no\nBinary Categorical Variable\nPredictor\n832\n\n\n\necig_flavored\nR01_AE9010\nIs the regular / last brand of e-cigarettes used by the participant flavored to taste like menthol, mint,clove, spice, candy, fruit, chocolate, alcohol or other sweets?\n2 levels:yes,no\nBinary Categorical Variable\nPredictor\n831\n\n\n\nmin_num\nR01R_A_MINFIRST_ECIG\nAmount of time from waking to smoking first e-cigarette of any given day (in minutes).\nQuantitative Variable\nPredictor\n853\n\n\n\n\nThe levels of the categorical variables and their description are as follows:\nage_range\nHow old was the participant when they first used an e-cigarette, even one or two times?\n\n\n\nLevel\nDescription\nDistinct Observations in Level\n\n\n\n\n&lt;18\nLess than 18 years old\n54\n\n\n18to24\n18 to 24 years old\n365\n\n\n25to34\n25 to 34 years old\n276\n\n\n35to44\n35 to 44 years old\n181\n\n\n45to54\n45 to 54 years old\n169\n\n\n&gt;55\n55 years old or older\n147\n\n\n\nless_harm\nDid the participant use / used to use e-cigarettes because they might be / have been less harmful to them?\n\n\n\nLevel\nDescription\nDistinct Observations in Level\n\n\n\n\nyes\nYes\n955\n\n\nno\nNo\n239\n\n\n\necig_reg\nHas the participant ever used e-cigarettes fairly regularly?\n\n\n\nLevel\nDescription\nDistinct Observations in Level\n\n\n\n\nyes\nYes\n369\n\n\nno\nNo\n831\n\n\n\nharm_perc\nTo the participant, is using e-cigarettes less harmful, about the same, or more harmful than smoking cigarettes?\n\n\n\n\n\n\n\n\nLevel\nDescription\nDistinct Observations in Level\n\n\n\n\n1\nLess harmful\n822\n\n\n2\nAbout the Same / More harmful\n366\n\n\n\nprice_paid\nAbout how much did the participant pay for their e-cigarette?\n\n\n\nLevel\nDescription\nDistinct Observations in Level\n\n\n\n\nBelow$10\nLess than $10\n189\n\n\n$10to$20\n$10 to $20\n260\n\n\n$21to$100\n$21 to $100\n328\n\n\n&gt;$100\nMore than $100\n30\n\n\n\necig_nico\nDoes the e-cigarette the participant usually uses contain nicotine?\n\n\n\nLevel\nDescription\nDistinct Observations in Level\n\n\n\n\nyes\nYes\n320\n\n\nno\nNo\n48\n\n\n\necig_flavored\nIs the regular/last brand of e-cigarettes used by the participant flavored to taste like menthol, mint, clove, spice, candy, fruit, chocolate, alcohol or other sweets?\n\n\n\nLevel\nDescription\nDistinct Observations in Level\n\n\n\n\nyes\nYes\n246\n\n\nno\nNo\n123"
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#numerical-description",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#numerical-description",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "5.2 Numerical Description",
    "text": "5.2 Numerical Description\nThe describe function from the Hmisc package was used on the entire tibble.\n\n\nCode\ndescribe(data_final)\n\n\ndata_final \n\n 12  Variables      1200  Observations\n--------------------------------------------------------------------------------\npersonid \n       n  missing distinct \n    1200        0     1200 \n\nlowest : P000000176 P000000318 P000000457 P000001710 P000001806\nhighest: P000497409 P000498677 P000499021 P000499448 P000499846\n--------------------------------------------------------------------------------\necig_30day \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    1200        0       28    0.972     5.11    6.663        0        0 \n     .25      .50      .75      .90      .95 \n       0        2        6       15       20 \n\nlowest :  0  1  2  3  4, highest: 25 26 28 29 30\n--------------------------------------------------------------------------------\npuff_num \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     351      849       28     0.99    10.77    10.15        2        3 \n     .25      .50      .75      .90      .95 \n       4        6       10       25       30 \n\nlowest :   1   2   3   4   5, highest:  50  51  60  75 100\n--------------------------------------------------------------------------------\nage_range \n       n  missing distinct \n    1192        8        6 \n                                                    \nValue         &lt;18 18to24 25to34 35to44 45to54    &gt;55\nFrequency      54    365    276    181    169    147\nProportion  0.045  0.306  0.232  0.152  0.142  0.123\n--------------------------------------------------------------------------------\nless_harm \n       n  missing distinct \n    1194        6        2 \n                  \nValue      Yes  No\nFrequency  955 239\nProportion 0.8 0.2\n--------------------------------------------------------------------------------\necig_reg \n       n  missing distinct \n    1200        0        2 \n                      \nValue        Yes    No\nFrequency    369   831\nProportion 0.308 0.693\n--------------------------------------------------------------------------------\nharm_perc \n       n  missing distinct \n    1188       12        2 \n                                    \nValue         less_harm same_or_more\nFrequency           822          366\nProportion        0.692        0.308\n--------------------------------------------------------------------------------\nprice_paid \n       n  missing distinct \n     807      393        4 \n                                                  \nValue       Below$10  $10to$20 $21to$100     &gt;$100\nFrequency        189       260       328        30\nProportion     0.234     0.322     0.406     0.037\n--------------------------------------------------------------------------------\necig_nico \n       n  missing distinct \n     368      832        2 \n                    \nValue       Yes   No\nFrequency   320   48\nProportion 0.87 0.13\n--------------------------------------------------------------------------------\necig_flavored \n       n  missing distinct \n     369      831        2 \n                      \nValue        Yes    No\nFrequency    246   123\nProportion 0.667 0.333\n--------------------------------------------------------------------------------\nmin_num \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     347      853       30    0.993    149.1      182        3        5 \n     .25      .50      .75      .90      .95 \n      15       60      240      360      522 \n\nlowest :    1    2    3    4    5, highest:  720  780  840  900 1020\n--------------------------------------------------------------------------------\necig_smoker \n       n  missing distinct \n    1200        0        2 \n                      \nValue          1     2\nFrequency     41  1159\nProportion 0.034 0.966\n--------------------------------------------------------------------------------"
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#my-first-research-question",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#my-first-research-question",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "6.1 My First Research Question",
    "text": "6.1 My First Research Question\nE-Cigarette Perception, Smoking Habits, and their association with Heavy E-Cigarette Use:\nAre smoking e-cigarettes that contain nicotine, the perception of the purported healthiness of e-cigarettes when compared to smoking regular cigarettes, and smoking habits strong predictors of heavy e-cigarette use in adulthood?"
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#my-quantitative-outcome",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#my-quantitative-outcome",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "6.2 My Quantitative Outcome",
    "text": "6.2 My Quantitative Outcome\nThe quantitative variable I will be predicting is ecig_30day.\nIn the United States, e-cigarettes are not subject to the same marketing and promotion restrictions that apply to cigarettes. E-cigarette companies are permitted to advertise their products through mass media, and via the internet. Previous studies have demonstrated that e-cigarettes companies exacerbate and expand the tobacco epidemic by bringing lower risk youth into the market.[1] E-Cigarette use among young adults has been rapidly expanding, and has surpassed conventional cigarette use among young adults.[2]\nFrom a study conducted in 2017, while the demographic and behavioral risk profiles of most youth who reported smoking cigarettes (both cigarettes and e-cigarettes) are consistent with smoking cigarettes, the risk profiles of the remaining e-cigarette-only users (about 25% of e-cigarette users) suggested that these individuals would have been unlikely to have initiated tobacco product use with cigarettes.[3]\nE-cigarettes are reducing smoking cessation rates and expanding the nicotine market by attracting low-risk youth who would be unlikely to initiate nicotine use with conventional cigarettes.[4]\nBy attempting to predict higher use of e-cigarettes in the past 30 days(at the time of data collection) with the variables under study, the association between e-cigarette perception and smoking habits with heavy e-cigarette use can be studied.\n\n\nCode\ndata_lin |&gt;\n  filter(complete.cases(ecig_30day)) |&gt;\n  dim()\n\n\n[1] 1200    6\n\n\nOut of the 1200 rows in data_lin, all 1200 have complete data on the outcome variable ecig_30day.\n\n\nCode\np1 &lt;- \n  ggplot(data_lin, aes(x = ecig_30day)) +\n  geom_histogram(bins = 15, \n  fill = \"#2b8cbe\", col = \"white\") +\n  labs( x = \"Number of days used an e-cigarette\", y =\"\")\n\np2 &lt;- \n  ggplot(data_lin, aes(sample = ecig_30day)) + \n  geom_qq(col = \"#2b8cbe\") + geom_qq_line(col = \"red\") +\n  labs(x = \"\", y = \"Number of days used an e-cigarette\")\n\np3 &lt;- \n  ggplot(data_lin, aes(x = \"\", y = ecig_30day)) +\n  geom_violin(fill = \"#2b8cbe\", alpha = 0.3) + \n  geom_boxplot(fill = \"#2b8cbe\", width = 0.3,\n               outlier.color = \"red\") +\n  labs(y = \"Number of days used an e-cigarette\", x = \"\") + coord_flip()\n\np2 + p1 - p3 +\n  plot_layout(ncol = 1, height = c(3, 2)) + \n  plot_annotation(title = \"Distribution of the number of days participants used an e-cigarette in the past 30 days\",\n         subtitle = glue(\"Across \", nrow(data_lin), \n                           \" participants, taken as a random sample from the DS1001 dataset.\"),\n         caption = \"Data taken from the Population Assessment of Tobacco and Health (PATH) Study.\")\n\n\n\n\n\nThe distribution is very notably right skewed. Taking the logarithm of the outcome variable might help in making it’s distribution more normalized.\n\n\nCode\ndescribe(data_lin$ecig_30day)\n\n\ndata_lin$ecig_30day \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    1200        0       28    0.972     5.11    6.663        0        0 \n     .25      .50      .75      .90      .95 \n       0        2        6       15       20 \n\nlowest :  0  1  2  3  4, highest: 25 26 28 29 30\n\n\nThe describe function shows that there are 28 distinct values for the ecig_30day variable, and the variable has at least 10 different, ordered, observed values."
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#my-planned-predictors-linear-model",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#my-planned-predictors-linear-model",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "6.3 My Planned Predictors (Linear Model)",
    "text": "6.3 My Planned Predictors (Linear Model)\nThe inputs that will be used in the linear regression model are:\n\npuff_num: This is a quantitative variable that has atleast 10 distinct, ordered, observed values.\n\n\n\nCode\ndescribe(data_lin$puff_num)\n\n\ndata_lin$puff_num \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     351      849       28     0.99    10.77    10.15        2        3 \n     .25      .50      .75      .90      .95 \n       4        6       10       25       30 \n\nlowest :   1   2   3   4   5, highest:  50  51  60  75 100\n\n\n\nage_range: This is a categorical variable which has 6 categories, that has at least 30 observations in each level of the factor.\n\n\n\nCode\ndata_lin|&gt;\n  tabyl(age_range) |&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nage_range\nn\npercent\nvalid_percent\n\n\n\n\n&lt;18\n54\n0.0450000\n0.0453020\n\n\n18to24\n365\n0.3041667\n0.3062081\n\n\n25to34\n276\n0.2300000\n0.2315436\n\n\n35to44\n181\n0.1508333\n0.1518456\n\n\n45to54\n169\n0.1408333\n0.1417785\n\n\n&gt;55\n147\n0.1225000\n0.1233221\n\n\nNA\n8\n0.0066667\nNA\n\n\n\n\n\n\n\n\nless_harm, which is a binary categorical variable.\n\n\n\nCode\ndata_lin|&gt;\n  tabyl(less_harm) |&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nless_harm\nn\npercent\nvalid_percent\n\n\n\n\nYes\n955\n0.7958333\n0.7998325\n\n\nNo\n239\n0.1991667\n0.2001675\n\n\nNA\n6\n0.0050000\nNA\n\n\n\n\n\n\n\n\necig_nico, which is a binary categorical variable.\n\n\n\nCode\ndata_lin|&gt;\n  tabyl(ecig_nico)|&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\necig_nico\nn\npercent\nvalid_percent\n\n\n\n\nYes\n320\n0.2666667\n0.8695652\n\n\nNo\n48\n0.0400000\n0.1304348\n\n\nNA\n832\n0.6933333\nNA\n\n\n\n\n\n\n\nThere are a total of 4 predictors.\n\n\nCode\ndatatest1 &lt;- data_lin |&gt;\n  filter(complete.cases(personid,ecig_30day,puff_num,age_range,less_harm,ecig_nico)) |&gt;\n  select(personid,ecig_30day,puff_num,age_range,less_harm,ecig_nico)\ndim(datatest1)\n\n\n[1] 137   6\n\n\nWith 137 rows having complete observations on all the variables under study, [4 + (N1 - 100)/100] is 4.3, and the number of predictors does not exceed this value.\nFrom the cited literature,\nI expect higher values of puff_num to be associated with higher values of ecig_30day. For less_harm and ecig_nico, I expect the yes factor to be associated with higher values of ecig_30day, ie, as the two variables change from no to yes, the value of ecig_30day is expected to increase.\nI expect the lower levels of age_range to be associated with higher values of ecig_30day, ie, as age_range factors go from the lowest factors to the higher factors, I expect ecig_30day to decrease."
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#my-second-research-question",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#my-second-research-question",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "7.1 My Second Research Question",
    "text": "7.1 My Second Research Question\nRegular e-cigarette use and it’s associated factors:\nAre factors such as using flavored e-cigarettes, or needing to use e-cigarettes immediately after waking up, or using e-cigarettes with nicotine, or use of e-cigarettes as a healthier alternative to regular cigarettes associated with regular e-cigarette use?"
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#my-binary-outcome",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#my-binary-outcome",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "7.2 My Binary Outcome",
    "text": "7.2 My Binary Outcome\nThe binary outcome I will be predicting is ecig_reg.\nIn the United States, e-cigarettes and their market value have rapidly expanded over the past years.[2] E-cigarette use and it’s prevalent adoption have resulted in youth and young adults who would otherwise not have been exposed to nicotine/tobacco products to begin the use of the drug via e-cigarettes.[3]\nReasons for this increased use of e-cigarettes are many, including the perception of e-cigarettes as being healthier than regular cigarettes, the presence of flavored e-cigarettes available for use, and the availability of cheap, refillable cartridges that only require e-cigarette fluid.[1] By attempting to predict the odds of being a regular user of e-cigarettes using the predictors under study, the association between these factors and the odds of adoption of regular e-cigarette use can be more thoroughly studied.\n\n\nCode\ndata_log|&gt;\n  tabyl(ecig_reg)|&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\necig_reg\nn\npercent\n\n\n\n\nYes\n369\n0.3075\n\n\nNo\n831\n0.6925\n\n\n\n\n\n\n\nThere are no missing values for the outcome variable."
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#my-planned-predictors-logistic-model",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#my-planned-predictors-logistic-model",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "7.3 My Planned Predictors (Logistic Model)",
    "text": "7.3 My Planned Predictors (Logistic Model)\nFor the logistic regression model, the following are the predictors I intend to use:\n\nmin_num: This is a quantitative variable that has atleast 10 distinct, ordered, observed values.\n\n\n\nCode\ndescribe(data_log$min_num)\n\n\ndata_log$min_num \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     347      853       30    0.993    149.1      182        3        5 \n     .25      .50      .75      .90      .95 \n      15       60      240      360      522 \n\nlowest :    1    2    3    4    5, highest:  720  780  840  900 1020\n\n\n\nprice_paid: This is a categorical variable which has 4 categories, that has at least 30 observations in each level of the factor.\n\n\n\nCode\ndata_log |&gt;\n  tabyl(price_paid)|&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nprice_paid\nn\npercent\nvalid_percent\n\n\n\n\nBelow$10\n189\n0.1575000\n0.2342007\n\n\n$10to$20\n260\n0.2166667\n0.3221809\n\n\n$21to$100\n328\n0.2733333\n0.4064436\n\n\n&gt;$100\n30\n0.0250000\n0.0371747\n\n\nNA\n393\n0.3275000\nNA\n\n\n\n\n\n\n\n\nharm_perc, which is a binary categorical variable.\n\n\n\nCode\ndata_log |&gt;\n  tabyl(harm_perc)|&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nharm_perc\nn\npercent\nvalid_percent\n\n\n\n\nless_harm\n822\n0.685\n0.6919192\n\n\nsame_or_more\n366\n0.305\n0.3080808\n\n\nNA\n12\n0.010\nNA\n\n\n\n\n\n\n\n-ecig_nico, which is a binary categorical variable.\n\n\nCode\ndata_log |&gt;\n  tabyl(ecig_nico)|&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\necig_nico\nn\npercent\nvalid_percent\n\n\n\n\nYes\n320\n0.2666667\n0.8695652\n\n\nNo\n48\n0.0400000\n0.1304348\n\n\nNA\n832\n0.6933333\nNA\n\n\n\n\n\n\n\n-ecig_flavored, which is a binary categorical variable.\n\n\nCode\ndata_log |&gt;\n  tabyl(ecig_flavored)|&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\necig_flavored\nn\npercent\nvalid_percent\n\n\n\n\nYes\n246\n0.2050\n0.6666667\n\n\nNo\n123\n0.1025\n0.3333333\n\n\nNA\n831\n0.6925\nNA\n\n\n\n\n\n\n\nThere are 5 predictors.\nWith 369 subjects in the smaller of my two outcome groups, [4 + (N2 - 100)/100] is 6.69, and the number of predictors does not exceed this value.\nFrom the cited literature,\nI expect lower values of min_num to be associated with higher odds of being in the yes category of ecig_reg.I also expect lower values of price_paid to be associated with higher odds of being in the yes category of ecig_reg. I also expect yes values for ecig_nico and ecig_flavored to be associated with higher odds of being in the yes category of ecig_reg. I expect less_harm values for harm_perc to be associated with higher odds of being in the yes category of ecig_reg."
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#missingness",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#missingness",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "8.1 Missingness",
    "text": "8.1 Missingness\n\n\nCode\ndata_lin |&gt;\n  miss_var_summary() |&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nvariable\nn_miss\npct_miss\n\n\n\n\npuff_num\n849\n70.7500000\n\n\necig_nico\n832\n69.3333333\n\n\nage_range\n8\n0.6666667\n\n\nless_harm\n6\n0.5000000\n\n\npersonid\n0\n0.0000000\n\n\necig_30day\n0\n0.0000000\n\n\n\n\n\n\n\nThere is missing data in all of the variables under study, and the outcome variable ecig_30day does not have any missing values. There are missing predictor values for more than 10% of the subjects under study, and more than 50 subjects have missing values.\nThe data can be assumed to be MAR (Missing At Random), since the missing data are not randomly distributed. It cannot be assumed to be MNAR (Missing Not At Random) since there does not appear to be a relationship between the magnitude of a value and it’s missingness (or inclusion in the non-missing data), since there is no preponderance of values with lower or higher magnitude being missing.\nSingle imputation was then performed on the dataset, to account for missingness. The complete dataset, containing the data for both the analyses had it’s missing values imputed.\n\n\nCode\nset.seed(4322023)\ndata_final_i &lt;- data_final |&gt; data.frame() |&gt;\n  impute_rhd(less_harm ~ ecig_30day + ecig_reg) |&gt;\n  impute_rhd(age_range ~ ecig_30day + ecig_reg + less_harm) |&gt;\n  impute_rhd(ecig_nico ~ age_range ) |&gt;\n  impute_rlm(puff_num ~ ecig_30day ) |&gt;\n  impute_cart(harm_perc ~ ecig_reg) |&gt;\n  impute_cart(price_paid ~ ecig_reg + harm_perc) |&gt;\n  impute_cart(ecig_flavored ~ ecig_reg + price_paid) |&gt;\n  impute_cart(ecig_nico ~ ecig_reg + ecig_flavored) |&gt;\n  impute_pmm(min_num ~ ecig_30day + age_range + less_harm) |&gt;\n  as_tibble()\n\n\n\n\nCode\ndata_lin_i &lt;- data_final_i |&gt;\n  select(personid,ecig_30day,puff_num,ecig_nico,age_range,less_harm)\n\n\nThe imputed dataset was then checked to see if it was still missing any values.\n\n\nCode\nmiss_var_summary(data_lin_i) |&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nvariable\nn_miss\npct_miss\n\n\n\n\npersonid\n0\n0\n\n\necig_30day\n0\n0\n\n\npuff_num\n0\n0\n\n\necig_nico\n0\n0\n\n\nage_range\n0\n0\n\n\nless_harm\n0\n0"
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#outcome-transformation",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#outcome-transformation",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "8.2 Outcome Transformation",
    "text": "8.2 Outcome Transformation\nThe distribution of the outcome variable was plotted.\n\n\nCode\np1 &lt;- \n  ggplot(data_lin_i, aes(x = ecig_30day)) +\n  geom_histogram(bins = 15, \n  fill = \"#2b8cbe\", col = \"white\") +\n  labs( x = \"Number of days used an e-cigarette\", y =\"\")\n\np2 &lt;- \n  ggplot(data_lin_i, aes(sample = ecig_30day)) + \n  geom_qq(col = \"#2b8cbe\") + geom_qq_line(col = \"red\") +\n  labs(x = \"\", y = \"Number of days used an e-cigarette\")\n\np3 &lt;- \n  ggplot(data_lin_i, aes(x = \"\", y = ecig_30day)) +\n  geom_violin(fill = \"#2b8cbe\", alpha = 0.3) + \n  geom_boxplot(fill = \"#2b8cbe\", width = 0.3,\n               outlier.color = \"red\") +\n  labs(y = \"Number of days used an e-cigarette\", x = \"\") + coord_flip()\n\np2 + p1 - p3 +\n  plot_layout(ncol = 1, height = c(3, 2)) + \n  plot_annotation(title = \"Distribution of the number of days participants used an e-cigarette in the past 30 days\",\n         subtitle = glue(\"Across \", nrow(data_lin_i), \n                           \" participants, taken as a random sample from the DS1001 dataset.\"),\n         caption = \"Data taken from the Population Assessment of Tobacco and Health (PATH) Study.\")\n\n\n\n\n\nThe outcome variable was modified so that the data would be strictly positive, making it possible to assess the need for transformation. 1 was added to each value.\n\n\nCode\ndata_lin_i &lt;- data_lin_i |&gt;\n  mutate(int_ecig_30day = ecig_30day + 1) \nboxCox(data_lin_i$int_ecig_30day ~ 1)\n\n\n\n\n\n\n\nCode\npowerTransform(data_lin_i$int_ecig_30day ~ 1)\n\n\nEstimated transformation parameter \n        Y1 \n-0.1938334 \n\n\nThe Box-Cox plot peaks at a Y value of -0.19, which is approximately close enough to -1 to justify using -1 on Tukey’s ladder of power transformations, which suggests taking the inverse of the outcome.\n\n\nCode\np1 &lt;- \n  ggplot(data_lin_i, aes(x = log(int_ecig_30day))) +\n  geom_histogram(bins = 15, \n  fill = \"#2b8cbe\", col = \"white\") +\n  labs( x = \"Natural Logarithm of the Number of days participant used an e-cigarette\", y =\"\")\n\np2 &lt;- \n  ggplot(data_lin_i, aes(sample = log(int_ecig_30day))) + \n  geom_qq(col = \"#2b8cbe\") + geom_qq_line(col = \"red\") +\n  labs(x = \"\", y = str_wrap(\"Natural Logarithm of the Number of days participant used an e-cigarette\", width = 50))\n\np3 &lt;- \n  ggplot(data_lin_i, aes(x = \"\", y = log(int_ecig_30day))) +\n  geom_violin(fill = \"#2b8cbe\", alpha = 0.3) + \n  geom_boxplot(fill = \"#2b8cbe\", width = 0.3,\n               outlier.color = \"red\") +\n  labs(y = \"Natural Logarithm of the Number of days participant used an e-cigarette\", x = \"\") + coord_flip()\n\np2 + p1 - p3 +\n  plot_layout(ncol = 1, height = c(3, 2)) + \n  plot_annotation(title = \"Distribution of the natural logarithm of the number of days participants used an e-cigarette in the past 30 days\",\n         subtitle = glue(\"Across \", nrow(data_lin_i), \n                           \" participants, taken as a random sample from the DS1001 dataset.\"),\n         caption = \"Data taken from the Population Assessment of Tobacco and Health (PATH) Study.\")"
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#scatterplot-matrix-and-collinearity",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#scatterplot-matrix-and-collinearity",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "8.3 Scatterplot Matrix and Collinearity",
    "text": "8.3 Scatterplot Matrix and Collinearity\nA new variable log_ecig_30day was created by taking the natural logarithm of the outcome variable, which itself was modified by adding 1 to each of it’s values.\n\n\nCode\ndata_lin_i &lt;- data_lin_i |&gt;\n  mutate(log_ecig_30day = log(int_ecig_30day))\n\n\nThe ggpairs function was used to plot a scatterplot matrix.\n\n\nCode\nggpairs(data_lin_i, columns = c(\"puff_num\", \"age_range\", \"less_harm\", \n                               \"ecig_nico\", \"log_ecig_30day\"))\n\n\n\n\n\nmod_A was fit using the lm function. The vif function from the car package was used to estimate if the variables had significant collinearity with each other.\n\n\nCode\nmod_A_car &lt;- lm(ecig_30day ~ puff_num + age_range + less_harm + ecig_nico, data = data_lin_i )\ncar::vif(mod_A_car)\n\n\n              GVIF Df GVIF^(1/(2*Df))\npuff_num  1.011415  1        1.005691\nage_range 1.045301  5        1.004440\nless_harm 1.006246  1        1.003118\necig_nico 1.044649  1        1.022081\n\n\nFrom the output, the variables do not seem to have significant collinearity with each other."
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#model-a",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#model-a",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "8.4 Model A",
    "text": "8.4 Model A\n\n8.4.1 Fitting Model A\nModel A was fit using both lm and ols. The outcome variable is log_ecig_30day, and the predictors are puff_num (A Quantitative variable), age_range (A multi-categorical variable), less_harm and ecig_nico(both binary variables).\n\n\nCode\nmod_A &lt;- lm(log_ecig_30day ~ puff_num + age_range + less_harm + ecig_nico, data = data_lin_i )\n\n\n\n\nCode\ndd &lt;- datadist(data_lin_i)\noptions(datadist = \"dd\")\nmod_A_ols &lt;- ols(log_ecig_30day ~ puff_num + age_range + less_harm + ecig_nico, data = data_lin_i, \n                 x = TRUE, y = TRUE )\n\n\n\n\n8.4.2 Tidied Coefficient Estimates (Model A)\nWith confidence levels of 90%, the lm model (mod_A) had it’s coefficients tidied and placed in a table that was presented using kable.\n\n\nCode\ntidy(mod_A, conf.int = TRUE, conf.level = 0.90) |&gt;\n  select(term, estimate, se = std.error, \n         low90 = conf.low, high90 = conf.high, \n         p = p.value) |&gt;\n  kable(digits = 3) |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nterm\nestimate\nse\nlow90\nhigh90\np\n\n\n\n\n(Intercept)\n0.980\n0.138\n0.753\n1.207\n0.000\n\n\npuff_num\n0.055\n0.004\n0.048\n0.062\n0.000\n\n\nage_range18to24\n-0.204\n0.142\n-0.437\n0.030\n0.151\n\n\nage_range25to34\n-0.082\n0.145\n-0.320\n0.157\n0.574\n\n\nage_range35to44\n-0.169\n0.151\n-0.417\n0.080\n0.263\n\n\nage_range45to54\n-0.040\n0.152\n-0.290\n0.210\n0.794\n\n\nage_range&gt;55\n0.068\n0.155\n-0.187\n0.324\n0.660\n\n\nless_harmNo\n-0.183\n0.070\n-0.298\n-0.067\n0.009\n\n\necig_nicoNo\n-0.117\n0.086\n-0.258\n0.024\n0.172\n\n\n\n\n\n\n\n\n\n8.4.3 Summarizing Fit (Model A)\nUsing the glance function. the numerical summaries of mod_A’s fit were placed in a table.\n\n\nCode\nglance(mod_A) |&gt;\n  select(r2 = r.squared, adjr2 = adj.r.squared, sigma, \n         AIC, BIC, nobs, df, df.residual) |&gt;\n  kable(digits = c(3, 3, 2, 1, 1, 0, 0, 0))|&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nr2\nadjr2\nsigma\nAIC\nBIC\nnobs\ndf\ndf.residual\n\n\n\n\n0.142\n0.137\n0.97\n3350.3\n3401.2\n1200\n8\n1191\n\n\n\n\n\n\n\n\n\n8.4.4 Regression Diagnostics (Model A)\nThe four main diagnostic residual plots were visualized.\n\n\nCode\npar(mfrow = c(2,2)); plot(mod_A); par(mfrow = c(1,1))\n\n\n\n\n\nFrom the normal Q-Q plot, there appears to be serious problems with the assumption of normality, despite the transformation of the outcome variable. There is also a definite pattern in the residuals vs fitted values plot, which has implications regarding the assumption of linearity. The scale-location plot also has a definite non-linear pattern, indicating problems with the assumption of constant variance."
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#non-linearity",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#non-linearity",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "8.5 Non-Linearity",
    "text": "8.5 Non-Linearity\nAdding a non-linear term to model A might help with the problems visualized with the various assumptions made in a linear regression model. A Spearman \\(p^2\\) plot was made with the predictors in model A.\n\n\nCode\nplot(spearman2(log_ecig_30day ~ puff_num + age_range + less_harm + ecig_nico, data = data_lin_i ))\n\n\n\n\n\nFrom the plot, if a non-linear term were to improve the fit of the model, the predictor most likely to do so would be puff_num, followed much less closely by less_harm.\nA restricted cubic spline with 4 knots in puff_num was then added to the model, spending 2 additional degrees of freedom as compared to model A (with the main effect of puff_num).\nAn interaction term between the main effects of puff_num and less_harm was added to the model, spending a single additional degree of freedom.\nOverall, this would result in 3 additional degrees of freedom being spent."
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#model-b",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#model-b",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "8.6 Model B",
    "text": "8.6 Model B\n\n8.6.1 Fitting Model B\nUsing lm and ols, model B was fit, using a restricted cubic spline on puff_num and an interaction term between the main effects of puff_num and less_harm.\n\n\nCode\nmod_B &lt;- lm(log_ecig_30day ~ rcs(puff_num,4) + age_range + less_harm + ecig_nico + puff_num%ia%less_harm, data = data_lin_i )\n\n\n\n\nCode\ndd &lt;- datadist(data_lin_i)\noptions(datadist = \"dd\")\n\nmod_B_ols &lt;- ols(log_ecig_30day ~ rcs(puff_num,4) + age_range + less_harm + ecig_nico + puff_num%ia%less_harm, data = data_lin_i,\n                 x = TRUE, y = TRUE)\n\n\n\n\n8.6.2 Tidied Coefficient Estimates (Model B)\nmod_B had it’s coefficients tidied with a 90% confidence interval and placed in a tidy table.\n\n\nCode\ntidy(mod_B, conf.int = TRUE, conf.level = 0.90) |&gt;\n  select(term, estimate, se = std.error, \n         low90 = conf.low, high90 = conf.high, \n         p = p.value) |&gt;\n  kable(digits = 3) |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nterm\nestimate\nse\nlow90\nhigh90\np\n\n\n\n\n(Intercept)\n3.476\n0.214\n3.124\n3.828\n0.000\n\n\nrcs(puff_num, 4)puff_num\n-0.620\n0.040\n-0.686\n-0.554\n0.000\n\n\nrcs(puff_num, 4)puff_num'\n11.472\n0.493\n10.660\n12.283\n0.000\n\n\nrcs(puff_num, 4)puff_num''\n-57.329\n2.382\n-61.251\n-53.408\n0.000\n\n\nage_range18to24\n-0.103\n0.112\n-0.287\n0.081\n0.357\n\n\nage_range25to34\n0.020\n0.114\n-0.169\n0.208\n0.863\n\n\nage_range35to44\n-0.060\n0.119\n-0.256\n0.135\n0.612\n\n\nage_range45to54\n0.025\n0.120\n-0.173\n0.222\n0.838\n\n\nage_range&gt;55\n0.155\n0.122\n-0.047\n0.356\n0.206\n\n\nless_harmNo\n0.144\n0.114\n-0.043\n0.331\n0.204\n\n\necig_nicoNo\n-0.121\n0.068\n-0.232\n-0.009\n0.076\n\n\npuff_num %ia% less_harm\n-0.031\n0.014\n-0.054\n-0.008\n0.029\n\n\n\n\n\n\n\n\n\n8.6.3 Effects Plot for Model B\nThe effects plot for model mod_B_ols was created.\n\n\nCode\nplot(summary(mod_B_ols))\n\n\n\n\n\n\n\n8.6.4 Summarizing Fit (Model B)\nUsing the glance function, the numerical summaries of model B’s fit were placed in a table.\n\n\nCode\nglance(mod_B) |&gt;\n  select(r2 = r.squared, adjr2 = adj.r.squared, sigma, \n         AIC, BIC, nobs, df, df.residual) |&gt;\n  kable(digits = c(3, 3, 2, 1, 1, 0, 0, 0)) |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nr2\nadjr2\nsigma\nAIC\nBIC\nnobs\ndf\ndf.residual\n\n\n\n\n0.468\n0.463\n0.77\n2782.8\n2849\n1200\n11\n1188\n\n\n\n\n\n\n\n\n\n8.6.5 Regression Diagnostics (Model B)\nThe four residual diagnostic plots for model B were obtained.\n\n\nCode\npar(mfrow = c(2,2)); plot(mod_B); par(mfrow = c(1,1))\n\n\n\n\n\nThe residual plots are improved from the plots in model A. The q-q plot follows a more normal distribution. The residuals vs fitted values plot shows a more linear line, and the scale-location plot shows a more linear model as well. The assumptions of normality, linearity and constant variance are adhered to more rigorously in the residuals of model B, as compared to model A.\nWhile there are significant outliers in the q-q plot, the residuals vs leverage plot does not show points that have a Cook’s distance of more than 0.5, indicating that these outliers do not exert an undue influence on the model. There are points that have an unusual combination of predictor variables, but their influence on the model is not significant."
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#validating-models-a-and-b",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#validating-models-a-and-b",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "8.7 Validating Models A and B",
    "text": "8.7 Validating Models A and B\nAfter setting a seed, the data_lin_i dataset was divided into a training and a testing sample. Two models were created, one model with linear terms alone, and the other model with the non-linear terms recommended by the Spearman \\(p^2\\) plot. The coefficients obtained from these models were then used to obtain the predicted log_ecig_30day values in the holdout test sample.\n\n\nCode\nset.seed(4322023)\n\ndata_lin_split &lt;- initial_split(data_lin_i, prop = 0.7)\ndata_lin_train &lt;- training(data_lin_split)\ndata_lin_test &lt;- testing(data_lin_split)\n\n\nFunctions from the yardstick package were used to obtain key summary of fit statistics for both the models’ predictions of the holdout testing sample. The metrics were then placed in a tibble and displayed using kable.\n\n\nCode\nmod_A_train &lt;- ols(log_ecig_30day ~ puff_num + age_range + less_harm + ecig_nico, data = data_lin_train)\nmod_B_train &lt;- ols(log_ecig_30day ~ rcs(puff_num,4) + age_range + less_harm + ecig_nico + puff_num%ia%less_harm,\n                  data = data_lin_train )\nmod_A_test_aug &lt;- augment(mod_A, newdata = data_lin_test)\nmod_B_test_aug &lt;- augment(mod_B, newdata = data_lin_test)\nrep1 &lt;- rmse(data = mod_A_test_aug, truth = log_ecig_30day, estimate = .fitted)\nrep2 &lt;- rmse(data = mod_B_test_aug, truth = log_ecig_30day, estimate = .fitted)\nrep3 &lt;- rsq(data = mod_A_test_aug, truth = log_ecig_30day, estimate = .fitted)\nrep4 &lt;- rsq(data = mod_B_test_aug, truth = log_ecig_30day, estimate = .fitted)\nrep5 &lt;- mae(data = mod_A_test_aug, truth = log_ecig_30day, estimate = .fitted)\nrep6 &lt;- mae(data = mod_B_test_aug, truth = log_ecig_30day, estimate = .fitted)\nrep_full &lt;- bind_rows(rep1, rep2, rep3, rep4, rep5, rep6)\nrep_full &lt;- rep_full |&gt; \n  mutate(Models = c(\"Model A Holdout RMSE\",\"Model B Holdout RMSE\",\"Model A Holdout $R^2$\",\"Model B Holdout $R^2$\",\"Model A Holdout MAE\",\"Model B Holdout MAE\"))\nrep_full &lt;- rep_full |&gt;\n  select(Models, Metric = .metric, Estimate = .estimate)\nrep_full |&gt; kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nModels\nMetric\nEstimate\n\n\n\n\nModel A Holdout RMSE\nrmse\n0.9562983\n\n\nModel B Holdout RMSE\nrmse\n0.7614010\n\n\nModel A Holdout $R^2$\nrsq\n0.1708639\n\n\nModel B Holdout $R^2$\nrsq\n0.4727390\n\n\nModel A Holdout MAE\nmae\n0.8158778\n\n\nModel B Holdout MAE\nmae\n0.6037231\n\n\n\n\n\n\n\n\n8.7.1 Holdout RMSE, \\(R^2\\) and MAE\n\n\n\nModel\nHoldout RMSE\nHoldout \\(R^2\\)\nHoldout MAE\n\n\n\n\nA\n0.9563\n0.1709\n0.8159\n\n\nB\n0.7614\n0.4727\n0.5992\n\n\n\n\n\n8.7.2 Validated \\(R^2\\), MSE and IC statistics\nAfter setting a seed, the validate function was used to obtain the validated R-squared and MSE values for models A and B.\n\n\nCode\nset.seed(4322023); (valA &lt;- validate(mod_A_ols)) \n\n\n          index.orig training   test optimism index.corrected  n\nR-square      0.1423   0.1495 0.1330   0.0165          0.1258 40\nMSE           0.9393   0.9284 0.9495  -0.0210          0.9603 40\ng             0.3083   0.3251 0.3141   0.0110          0.2972 40\nIntercept     0.0000   0.0000 0.0266  -0.0266          0.0266 40\nSlope         1.0000   1.0000 0.9811   0.0189          0.9811 40\n\n\n\n\nCode\nset.seed(4322023); (valB &lt;- validate(mod_B_ols))\n\n\n          index.orig training   test optimism index.corrected  n\nR-square      0.4682   0.4825 0.4596   0.0229          0.4452 40\nMSE           0.5824   0.5647 0.5918  -0.0272          0.6096 40\ng             0.7450   0.7545 0.7403   0.0142          0.7308 40\nIntercept     0.0000   0.0000 0.0273  -0.0273          0.0273 40\nSlope         1.0000   1.0000 0.9757   0.0243          0.9757 40\n\n\n\n\n\nModel\nValidated \\(R^2\\)\nValidated MSE\nAIC\nBIC\nDF\n\n\n\n\nA\n0.1258\n0.9603\n3349.8\n3400.7\n8\n\n\nB\n0.4452\n0.6096\n2782.8\n2849\n11\n\n\n\n\n\n8.7.3 ANOVA to compare Models A and B\n\n\nCode\nanova(mod_A, mod_B)\n\n\nAnalysis of Variance Table\n\nModel 1: log_ecig_30day ~ puff_num + age_range + less_harm + ecig_nico\nModel 2: log_ecig_30day ~ rcs(puff_num, 4) + age_range + less_harm + ecig_nico + \n    puff_num %ia% less_harm\n  Res.Df     RSS Df Sum of Sq      F    Pr(&gt;F)    \n1   1191 1127.17                                  \n2   1188  698.93  3    428.24 242.63 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis ANOVA table shows that model B has 3 additional degrees of freedom as compared to model A, and that adding non linear terms has resulted in a significant improvement in the predictive power of the model."
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#final-linear-regression-model",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#final-linear-regression-model",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "8.8 Final Linear Regression Model",
    "text": "8.8 Final Linear Regression Model\nI prefer Model B, because:\n\nModel B has a higher validated \\(R^2\\), and a higher holdout sample \\(R^2\\).\nModel B has a lower validated MSE (Mean Squared Error) and a lower holdout RMSE.\nModel B has a lower holdout MAE, and a lower AIC and BIC.\nModel B’s residual plots show that the assumptions of linearity, normality and non-heteroscedasticity are not violated.\n\n\n8.8.1 Winning Model’s Parameter Estimates\nThe model B’s raw and adjusted \\(R^2\\) were obtained.\n\n\nCode\nmod_B_ols\n\n\nLinear Regression Model\n\nols(formula = log_ecig_30day ~ rcs(puff_num, 4) + age_range + \n    less_harm + ecig_nico + puff_num %ia% less_harm, data = data_lin_i, \n    x = TRUE, y = TRUE)\n\n                Model Likelihood    Discrimination    \n                      Ratio Test           Indexes    \nObs    1200    LR chi2    757.68    R2       0.468    \nsigma0.7670    d.f.           11    R2 adj   0.463    \nd.f.   1188    Pr(&gt; chi2) 0.0000    g        0.745    \n\nResiduals\n\n     Min       1Q   Median       3Q      Max \n-3.17538 -0.63628  0.04184  0.48501  2.62802 \n\n                        Coef     S.E.   t      Pr(&gt;|t|)\nIntercept                 3.4764 0.2139  16.26 &lt;0.0001 \npuff_num                 -0.6200 0.0398 -15.57 &lt;0.0001 \npuff_num'                11.4717 0.4931  23.26 &lt;0.0001 \npuff_num''              -57.3292 2.3822 -24.07 &lt;0.0001 \nage_range=18to24         -0.1032 0.1119  -0.92 0.3568  \nage_range=25to34          0.0197 0.1143   0.17 0.8635  \nage_range=35to44         -0.0605 0.1190  -0.51 0.6115  \nage_range=45to54          0.0246 0.1199   0.20 0.8377  \nage_range=&gt;55             0.1549 0.1224   1.27 0.2059  \nless_harm=No              0.1445 0.1136   1.27 0.2037  \necig_nico=No             -0.1206 0.0678  -1.78 0.0756  \npuff_num * less_harm=No  -0.0309 0.0141  -2.19 0.0290  \n\n\n\n\n8.8.2 Model Coefficients\nModel B’s coefficients were obtained with 90% confidence intervals and presented using kable.\n\n\nCode\ntidy_mod_B &lt;- tidy(mod_B, conf.int = TRUE, conf.level = 0.90)\n\ntidy_mod_B |&gt;\n  kbl(digits = 4) |&gt; \n  kable_classic_2(font_size = 24, full_width = F)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n3.4764\n0.2139\n16.2554\n0.0000\n3.1243\n3.8284\n\n\nrcs(puff_num, 4)puff_num\n-0.6200\n0.0398\n-15.5677\n0.0000\n-0.6855\n-0.5544\n\n\nrcs(puff_num, 4)puff_num'\n11.4717\n0.4931\n23.2637\n0.0000\n10.6599\n12.2834\n\n\nrcs(puff_num, 4)puff_num''\n-57.3292\n2.3822\n-24.0658\n0.0000\n-61.2506\n-53.4078\n\n\nage_range18to24\n-0.1032\n0.1119\n-0.9219\n0.3568\n-0.2874\n0.0811\n\n\nage_range25to34\n0.0197\n0.1143\n0.1720\n0.8635\n-0.1685\n0.2079\n\n\nage_range35to44\n-0.0605\n0.1190\n-0.5080\n0.6115\n-0.2564\n0.1355\n\n\nage_range45to54\n0.0246\n0.1199\n0.2048\n0.8377\n-0.1728\n0.2219\n\n\nage_range&gt;55\n0.1549\n0.1224\n1.2655\n0.2059\n-0.0466\n0.3564\n\n\nless_harmNo\n0.1445\n0.1136\n1.2718\n0.2037\n-0.0425\n0.3314\n\n\necig_nicoNo\n-0.1206\n0.0678\n-1.7783\n0.0756\n-0.2322\n-0.0090\n\n\npuff_num %ia% less_harm\n-0.0309\n0.0141\n-2.1867\n0.0290\n-0.0541\n-0.0076\n\n\n\n\n\n\n\n\n\nCode\nggplot(Predict(mod_B_ols))\n\n\n\n\n\nThe coefficients for the non-linear term are significantly larger than expected. Given that the outcome of interest is a count outcome, and the predictor (puff_num) has a very intuitive relationship with the outcome variable, in that individuals who have a zero for their outcome (ecig_30day) could be expected to have significantly lower (or even zero) values of puff_num, and individuals with any particular value of puff_num would have a collinear relationship with the outcome variable. It can be reasonably expected that the model is being driven significantly towards a particular value of log_ecig_30day, which could result in the exploding coefficients seen in the table of coefficients.\nThis indicates that model B is not the ideal sort of model to be used for a count outcome like ecig_30day.\nFollowing this, the validated \\(R^2\\) was also obtained, along with the validated MSE.\n\n\nCode\nset.seed(4322023); (valB_imp &lt;- validate(mod_B_ols))\n\n\n          index.orig training   test optimism index.corrected  n\nR-square      0.4682   0.4825 0.4596   0.0229          0.4452 40\nMSE           0.5824   0.5647 0.5918  -0.0272          0.6096 40\ng             0.7450   0.7545 0.7403   0.0142          0.7308 40\nIntercept     0.0000   0.0000 0.0273  -0.0273          0.0273 40\nSlope         1.0000   1.0000 0.9757   0.0243          0.9757 40\n\n\nAn ANOVA test performed on mod_B_ols shows that a non linear term (a restricted cubic spline with 4 knots) on puff_num had significant impact on the predictive ability of model B, along with the interaction term between the main effect of puff_num and less_harm.\n\n\nCode\nanova(mod_B_ols)\n\n\n                Analysis of Variance          Response: log_ecig_30day \n\n Factor                                              d.f. Partial SS\n puff_num  (Factor+Higher Order Factors)                4 589.511975\n  All Interactions                                      1   2.813085\n  Nonlinear                                             2 425.614545\n age_range                                              5   8.073412\n less_harm  (Factor+Higher Order Factors)               2   3.811854\n  All Interactions                                      1   2.813085\n ecig_nico                                              1   1.860506\n puff_num * less_harm  (Factor+Higher Order Factors)    1   2.813085\n TOTAL NONLINEAR + INTERACTION                          3 428.235704\n REGRESSION                                            11 615.229768\n ERROR                                               1188 698.931962\n MS          F      P     \n 147.3779937 250.50 &lt;.0001\n   2.8130849   4.78 0.0290\n 212.8072723 361.72 &lt;.0001\n   1.6146824   2.74 0.0179\n   1.9059270   3.24 0.0395\n   2.8130849   4.78 0.0290\n   1.8605056   3.16 0.0756\n   2.8130849   4.78 0.0290\n 142.7452346 242.63 &lt;.0001\n  55.9299789  95.07 &lt;.0001\n   0.5883266              \n\n\nThe ANOVA table shows that model B has 12 total degrees of freedom, with 4 non-linear terms of freedom.\n\n\n\nMetric\nValue\n\n\n\n\nRaw \\(R^2\\)\n0.468\n\n\nAdjusted \\(R^2\\)\n0.463\n\n\nValidated \\(R^2\\)\n0.4452\n\n\nAIC\n2782.8\n\n\nBIC\n2849\n\n\nValidated MSE (Mean Squared Error)\n0.6096\n\n\n\n\n\n8.8.3 Effects Plot for Winning Model\n\n\nCode\nsummary(mod_B_ols, conf.int = 0.90)\n\n\n             Effects              Response : log_ecig_30day \n\n Factor                    Low    High   Diff.  Effect    S.E.     Lower 0.9\n puff_num                  5.7586 7.0476 1.2889  0.596830 0.021791  0.560960\n age_range - &lt;18:18to24    2.0000 1.0000     NA  0.103180 0.111920 -0.081062\n age_range - 25to34:18to24 2.0000 3.0000     NA  0.122840 0.061711  0.021257\n age_range - 35to44:18to24 2.0000 4.0000     NA  0.042718 0.069549 -0.071770\n age_range - 45to54:18to24 2.0000 5.0000     NA  0.127730 0.071094  0.010703\n age_range - &gt;55:18to24    2.0000 6.0000     NA  0.258070 0.075061  0.134510\n less_harm - No:Yes        1.0000 2.0000     NA -0.041346 0.057248 -0.135580\n ecig_nico - No:Yes        1.0000 2.0000     NA -0.120600 0.067818 -0.232240\n Upper 0.9 \n  0.6327000\n  0.2874200\n  0.2244300\n  0.1572000\n  0.2447600\n  0.3816300\n  0.0528930\n -0.0089634\n\nAdjusted to: puff_num=6.016428 less_harm=Yes  \n\n\n\n\nCode\nplot(summary(mod_B_ols, conf.int = 0.90))\n\n\n\n\n\nFrom the summary above, for the puff_num coefficient, in model mod_b_ols,\nWe can conclude that the estimated effect of moving puff_num from 5.76 to 7.05 results in an increase of 0.597 in log_ecig_30day, with 90% confidence intervals of 0.595 to 0.733. IE, for model B,\nAs an individual’s number of e-cigarette puffs taken the last time they smoked an e-cigarette increases from 5.76 to 7.05, the predicted logarithm of the number of days they smoked an e-cigarette in the past 30 days (plus one day) increases by 0.597, with 90% CI(0.561 to 0.633), provided the individual’s age when they began smoking, their perception of the harmfulness of e-cigarettes, and their use or lack thereof of nicotine containing e-cigarettes are all held constant.\nGiven that this confidence interval does not include 0, this is a scientifically meaningful effect.\n\n\n8.8.4 Model Calibration\n\n\nCode\nset.seed(4322023); plot(calibrate(mod_B_ols))\n\n\n\n\n\n\nn=1200   Mean absolute error=0.222   Mean squared error=0.06564\n0.9 Quantile of absolute error=0.364\n\n\nThe model does not appear to be particularly well calibrated. It under-predicts at the high and low ends of the actual log_ecig_30day values, and over-predicts elsewhere.\n\n\n8.8.5 Numerical Description of Effect Sizes\n\n\nCode\nsummary(mod_B_ols, conf.int = 0.90) |&gt; kable(digits = 3) |&gt; \n  kable_classic_2(font_size = 24, full_width = F)\n\n\n\n\n\n\nLow\nHigh\nDiff.\nEffect\nS.E.\nLower 0.9\nUpper 0.9\nType\n\n\n\n\npuff_num\n5.759\n7.048\n1.289\n0.597\n0.022\n0.561\n0.633\n1\n\n\nage_range - &lt;18:18to24\n2.000\n1.000\nNA\n0.103\n0.112\n-0.081\n0.287\n1\n\n\nage_range - 25to34:18to24\n2.000\n3.000\nNA\n0.123\n0.062\n0.021\n0.224\n1\n\n\nage_range - 35to44:18to24\n2.000\n4.000\nNA\n0.043\n0.070\n-0.072\n0.157\n1\n\n\nage_range - 45to54:18to24\n2.000\n5.000\nNA\n0.128\n0.071\n0.011\n0.245\n1\n\n\nage_range - &gt;55:18to24\n2.000\n6.000\nNA\n0.258\n0.075\n0.135\n0.382\n1\n\n\nless_harm - No:Yes\n1.000\n2.000\nNA\n-0.041\n0.057\n-0.136\n0.053\n1\n\n\necig_nico - No:Yes\n1.000\n2.000\nNA\n-0.121\n0.068\n-0.232\n-0.009\n1\n\n\n\n\n\n\n\npuff_num description :\nFor two subjects with the same age range at which they started smoking e-cigarettes, the same perception of the harmfulness of e-cigarettes, the same kind of usage or lack thereof of e-cigarettes containing nicotine, and if one subject took 5.76 e-cigarette puffs the last time they smoked an e-cigarette, and the other took 7.05 e-cigarettes puffs 7.05 the last time they smoked an e-cigarette, the model estimates that subject 1 will have a log(number of days smoked in the past 30 days + 1) value 0.597 units higher than subject 2’s log(number of days smoked in the past 30 days + 1). The 90% confidence interval for that estimate ranges from 0.561 to 0.633.\n\n\n8.8.6 Nomogram of Winning Model\n\n\nCode\nplot(nomogram(mod_B_ols))\n\n\n\n\n\n\n\n8.8.7 Prediction for a New Subject\nA new theoretical subject, who took 5 puffs from an e-cigarette the last time they smoked an e-cigarette, who started smoking e-cigarettes when they were under 18, who uses e-cigarettes because they thought e-cigarettes might be less harmful to them than regular cigarettes, who usually uses an e-cigarette containing nicotine, was created. Their predicted log_ecig_30day value was then obtained and exponentiated. 1 was added to the exponentiated value to complete the transformation back into the original value.\n\n\nCode\npreds &lt;- predict(mod_B_ols, expand.grid(puff_num = 7, age_range = \"&lt;18\",\n                           less_harm = \"Yes\", ecig_nico = \"Yes\"),\nconf.int = 0.90, conf.type = \"individual\") |&gt; as_tibble() |&gt;\n  mutate(point_estimate = (exp(linear.predictors) - 1),\n         lower_orig = (exp(lower) - 1),\n         upper_orig = (exp(upper) - 1)) |&gt;\n  select(point_estimate,lower_orig,upper_orig)\npreds |&gt; kable() |&gt;\n  kable_classic_2(font_size = 24,full_width = F) \n\n\n\n\n\npoint_estimate\nlower_orig\nupper_orig\n\n\n\n\n2.720541\n0.040103\n12.3087\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictor Values\nPredicted days participant used an e-cigarette in the past 30 days\n90% Confidence Interval\n\n\n\n\npuff_num = 7, age_range = “&lt;18”,less_harm = “Yes”, ecig_nico = “Yes”\n2.72\n0.04 to 12.31"
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#missingness-1",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#missingness-1",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "9.1 Missingness",
    "text": "9.1 Missingness\n\n\nCode\ndata_log |&gt;\n  miss_var_summary() |&gt;\n  kbl() |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nvariable\nn_miss\npct_miss\n\n\n\n\nmin_num\n853\n71.08333\n\n\necig_nico\n832\n69.33333\n\n\necig_flavored\n831\n69.25000\n\n\nprice_paid\n393\n32.75000\n\n\nharm_perc\n12\n1.00000\n\n\npersonid\n0\n0.00000\n\n\necig_reg\n0\n0.00000\n\n\n\n\n\n\n\nThere is missing data in all of the variables under study, and the outcome variable ecig_reg does not have any missing values. There are missing predictor values for more than 10% of the subjects under study, and more than 50 subjects have missing values.\nThe data can be assumed to be MAR (Missing At Random), since the missing data are not randomly distributed. It cannot be assumed to be MNAR (Missing Not At Random) since there does not appear to be a relationship between the magnitude of a value and it’s missingness (or inclusion in the non-missing data), since there is no preponderance of values with lower or higher magnitude being missing.\nSingle imputation was already performed on the complete dataset, to account for missingness. The logistic regression dataset was selected.\n\n\nCode\ndata_log_i &lt;- data_final_i |&gt;\n  select(personid,ecig_reg,harm_perc,price_paid,ecig_flavored,ecig_nico,min_num)\nmiss_var_summary(data_log_i)\n\n\n# A tibble: 7 × 3\n  variable      n_miss pct_miss\n  &lt;chr&gt;          &lt;int&gt;    &lt;dbl&gt;\n1 personid           0        0\n2 ecig_reg           0        0\n3 harm_perc          0        0\n4 price_paid         0        0\n5 ecig_flavored      0        0\n6 ecig_nico          0        0\n7 min_num            0        0\n\n\nThere are no missing predictor variables in the logistic regression dataset after performing single imputation."
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#model-y",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#model-y",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "9.2 Model Y",
    "text": "9.2 Model Y\n\n9.2.1 Fitting Model Y\nModel Y was fit with both lrm and glm. ecig_reg was the outcome variable, and the predictors were min_num, ecig_nico, harm_perc and price_paid.\n\n\nCode\nmod_Y &lt;- glm(ecig_reg ~ min_num + ecig_nico + ecig_flavored + harm_perc + price_paid,\n            data = data_log_i, family = binomial())\n\nddd &lt;- datadist(data_log_i)\noptions(datadist = \"ddd\")\n\nmod_Y_lrm &lt;- lrm(ecig_reg ~ min_num + ecig_nico + ecig_flavored + harm_perc + price_paid,\n            data = data_log_i, x = TRUE, y = TRUE)\n\n\n\n\n9.2.2 Tidied Odds Ratio Estimates (Model Y)\n\n\nCode\ntidy(mod_Y, exponentiate = TRUE, conf.int = TRUE, conf.level = 0.95) |&gt;\n  select(term, estimate, se = std.error, \n         low95 = conf.low, high95 = conf.high, p = p.value) |&gt;\n  kable(digits = 3) |&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nterm\nestimate\nse\nlow95\nhigh95\np\n\n\n\n\n(Intercept)\n18.682\n0.314\n10.250\n35.172\n0.000\n\n\nmin_num\n1.000\n0.001\n0.999\n1.001\n0.733\n\n\necig_nicoNo\n0.927\n0.198\n0.632\n1.375\n0.700\n\n\necig_flavoredNo\n0.116\n0.248\n0.070\n0.186\n0.000\n\n\nharm_percsame_or_more\n1.196\n0.164\n0.869\n1.653\n0.274\n\n\nprice_paid$10to$20\n0.230\n0.299\n0.126\n0.409\n0.000\n\n\nprice_paid$21to$100\n0.117\n0.293\n0.065\n0.204\n0.000\n\n\nprice_paid&gt;$100\n0.055\n0.479\n0.021\n0.140\n0.000\n\n\n\n\n\n\n\n\n\n9.2.3 Effects Plot (Model Y)\n\n\nCode\nplot(summary(mod_Y_lrm))\n\n\n\n\n\n\n\n9.2.4 Summarizing Fit (Model Y)\n\n\nCode\nmod_Y_lrm\n\n\nLogistic Regression Model\n\nlrm(formula = ecig_reg ~ min_num + ecig_nico + ecig_flavored + \n    harm_perc + price_paid, data = data_log_i, x = TRUE, y = TRUE)\n\n                       Model Likelihood      Discrimination    Rank Discrim.    \n                             Ratio Test             Indexes          Indexes    \nObs          1200    LR chi2     129.72      R2       0.145    C       0.630    \n Yes          369    d.f.             7     R2(7,1200)0.097    Dxy     0.261    \n No           831    Pr(&gt; chi2) &lt;0.0001    R2(7,766.6)0.148    gamma   0.264    \nmax |deriv| 1e-07                            Brier    0.181    tau-a   0.111    \n\n                       Coef    S.E.   Wald Z Pr(&gt;|Z|)\nIntercept               2.9276 0.3140  9.32  &lt;0.0001 \nmin_num                -0.0002 0.0006 -0.34  0.7335  \necig_nico=No           -0.0762 0.1979 -0.39  0.7001  \necig_flavored=No       -2.1513 0.2479 -8.68  &lt;0.0001 \nharm_perc=same_or_more  0.1790 0.1638  1.09  0.2743  \nprice_paid=$10to$20    -1.4684 0.2994 -4.91  &lt;0.0001 \nprice_paid=$21to$100   -2.1453 0.2925 -7.33  &lt;0.0001 \nprice_paid=&gt;$100       -2.8969 0.4788 -6.05  &lt;0.0001 \n\n\n\n\nCode\nglance(mod_Y) |&gt;\n  mutate(df = nobs - df.residual - 1) |&gt;\n  select(AIC, BIC, df, df.residual, nobs) |&gt;\n  kable(digits = 1)|&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nAIC\nBIC\ndf\ndf.residual\nnobs\n\n\n\n\n1367.3\n1408\n7\n1192\n1200\n\n\n\n\n\n\n\n\n\n9.2.5 Confusion Matrix (Model Y)\nA temporary dataset, log_i_temp, for the putpose of generating a confusion matrix, was created. The outcome variable ecig_reg had it’s levels changed from “Yes” and “No” to 1 and 0.\n\n\nCode\nlog_i_temp &lt;- data_log_i |&gt;\n  mutate(ecig_reg = as.factor(ifelse(ecig_reg == \"Yes\",1,0)))\n\nmod_Y_cm &lt;- glm(ecig_reg ~ min_num + ecig_nico + ecig_flavored + harm_perc + price_paid,\n            data = log_i_temp, family = binomial())\n\nresY_aug &lt;- augment(mod_Y_cm, type.predict = \"response\")\n\n\nMy prediction rule is that the fitted value of pr(ecig_reg = 1) needs to be greater than or equal to 0.5 for me to predict that ecig_reg is 1.\n\n\nCode\ncm_Y &lt;- caret::confusionMatrix(\n  data = factor(resY_aug$.fitted &gt;= 0.5),\n  reference = factor(resY_aug$ecig_reg == 1),\n  positive = \"TRUE\")\n\ncm_Y\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction FALSE TRUE\n     FALSE   826  274\n     TRUE      5   95\n                                          \n               Accuracy : 0.7675          \n                 95% CI : (0.7425, 0.7911)\n    No Information Rate : 0.6925          \n    P-Value [Acc &gt; NIR] : 4.676e-09       \n                                          \n                  Kappa : 0.3153          \n                                          \n Mcnemar's Test P-Value : &lt; 2.2e-16       \n                                          \n            Sensitivity : 0.25745         \n            Specificity : 0.99398         \n         Pos Pred Value : 0.95000         \n         Neg Pred Value : 0.75091         \n             Prevalence : 0.30750         \n         Detection Rate : 0.07917         \n   Detection Prevalence : 0.08333         \n      Balanced Accuracy : 0.62572         \n                                          \n       'Positive' Class : TRUE"
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#non-linearity-1",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#non-linearity-1",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "9.3 Non-Linearity",
    "text": "9.3 Non-Linearity\nA Spearman \\(p^2\\) plot was created for assessing the terms that are most likely to add predictive power to the model.\n\n\nCode\nplot(spearman2(ecig_reg ~ min_num + ecig_nico + ecig_flavored + harm_perc + price_paid,\n            data = data_log_i))\n\n\n\n\n\nThe Spearman plot suggests the use of a non-linear term in min_num, so a restricted cubic spline with 4 knots in min_num will be added to the model, spending two additional degrees of freedom than the main effects model.\nAn interaction term between the main effect of ecig_flavored and min_num will be added, spending an additional degree of freedom."
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#model-z",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#model-z",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "9.4 Model Z",
    "text": "9.4 Model Z\nModel Z will add a total of 3 additional degrees of freedom to Model Y, using two non-linear terms.\n\n9.4.1 Fitting Model Z\nModel Z was fit using both glm and lrm.\n\n\nCode\nmod_Z &lt;- glm(ecig_reg ~ ecig_nico + rcs(min_num,4) + ecig_flavored + harm_perc + price_paid + ecig_flavored %ia% min_num,\n            data = data_log_i, family = binomial())\n\nddd &lt;- datadist(data_log_i)\noptions(datadist = \"ddd\")\n\nmod_Z_lrm &lt;- lrm(ecig_reg ~ rcs(min_num,4) + ecig_nico + ecig_flavored + harm_perc + price_paid + ecig_flavored %ia% min_num,\n            data = data_log_i, x = TRUE, y = TRUE)\n\n\n\n\n9.4.2 Tidied Odds Ratio Estimates (Model Z)\n\n\nCode\ntidy(mod_Z, exponentiate = TRUE, conf.int = TRUE, conf.level = 0.90) |&gt;\n  select(term, estimate, se = std.error, \n         low90 = conf.low, high90 = conf.high, p = p.value) |&gt;\n  kable(digits = 3)|&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n\n\n\nterm\nestimate\nse\nlow90\nhigh90\np\n\n\n\n\n(Intercept)\n0.021\n0.726\n0.006\n0.065\n0.000\n\n\necig_nicoNo\n0.796\n0.277\n0.510\n1.271\n0.412\n\n\nrcs(min_num, 4)min_num\n1.104\n0.009\n1.088\n1.121\n0.000\n\n\nrcs(min_num, 4)min_num'\n0.843\n0.021\n0.814\n0.871\n0.000\n\n\nrcs(min_num, 4)min_num''\n1.538\n0.081\n1.349\n1.760\n0.000\n\n\necig_flavoredNo\n0.048\n0.715\n0.014\n0.151\n0.000\n\n\nharm_percsame_or_more\n1.589\n0.254\n1.052\n2.429\n0.068\n\n\nprice_paid$10to$20\n0.111\n0.432\n0.054\n0.224\n0.000\n\n\nprice_paid$21to$100\n0.057\n0.424\n0.028\n0.113\n0.000\n\n\nprice_paid&gt;$100\n0.035\n0.709\n0.011\n0.112\n0.000\n\n\necig_flavored %ia% min_num\n1.003\n0.004\n0.997\n1.009\n0.446\n\n\n\n\n\n\n\n\n\n9.4.3 Effects Plot (Model Z)\n\n\nCode\nplot(summary(mod_Z_lrm, conf.int = 0.90))\n\n\n\n\n\n\n\n9.4.4 Summarizing Fit (Model Z)\n\n\nCode\nmod_Z_lrm\n\n\nLogistic Regression Model\n\nlrm(formula = ecig_reg ~ rcs(min_num, 4) + ecig_nico + ecig_flavored + \n    harm_perc + price_paid + ecig_flavored %ia% min_num, data = data_log_i, \n    x = TRUE, y = TRUE)\n\n                       Model Likelihood       Discrimination    Rank Discrim.    \n                             Ratio Test              Indexes          Indexes    \nObs          1200    LR chi2     773.30       R2       0.670    C       0.891    \n Yes          369    d.f.            10     R2(10,1200)0.471    Dxy     0.783    \n No           831    Pr(&gt; chi2) &lt;0.0001    R2(10,766.6)0.631    gamma   0.792    \nmax |deriv| 2e-08                             Brier    0.084    tau-a   0.334    \n\n                           Coef    S.E.   Wald Z Pr(&gt;|Z|)\nIntercept                  -3.8676 0.7261 -5.33  &lt;0.0001 \nmin_num                     0.0987 0.0090 10.99  &lt;0.0001 \nmin_num'                   -0.1707 0.0206 -8.29  &lt;0.0001 \nmin_num''                   0.4303 0.0807  5.33  &lt;0.0001 \necig_nico=No               -0.2277 0.2773 -0.82  0.4116  \necig_flavored=No           -3.0412 0.7147 -4.26  &lt;0.0001 \nharm_perc=same_or_more      0.4633 0.2540  1.82  0.0682  \nprice_paid=$10to$20        -2.1952 0.4318 -5.08  &lt;0.0001 \nprice_paid=$21to$100       -2.8602 0.4241 -6.74  &lt;0.0001 \nprice_paid=&gt;$100           -3.3635 0.7092 -4.74  &lt;0.0001 \necig_flavored=No * min_num  0.0028 0.0037  0.76  0.4457  \n\n\nThe Nagelkerke \\(R^2\\) for model Z is 0.670. The C statistic is 0.891.\n\n\nCode\nglance(mod_Z) |&gt;\n  mutate(df = nobs - df.residual - 1) |&gt;\n  select(AIC, BIC, df, df.residual, nobs) |&gt;\n  kable(digits = 1)|&gt;\n  kable_classic_2(font_size = 24,full_width = F)\n\n\n\n\n\nAIC\nBIC\ndf\ndf.residual\nnobs\n\n\n\n\n729.7\n785.7\n10\n1189\n1200\n\n\n\n\n\n\n\n\n\n9.4.5 Confusion Matrix (Model Z)\nAs in Model Y, my prediction rule is that the fitted value of pr(ecig_reg = 1) needs to be greater than or equal to 0.5 for me to predict that ecig_reg is 1.\n\n\nCode\nmod_Z_cm &lt;- glm(ecig_reg ~ ecig_nico + rcs(min_num,4) + ecig_flavored + harm_perc + price_paid + ecig_flavored %ia% min_num,\n            data = log_i_temp, family = binomial())\n\nresZ_aug &lt;- augment(mod_Z_cm, type.predict = \"response\")\n\n\n\n\nCode\ncm_Z &lt;- caret::confusionMatrix(\n  data = factor(resZ_aug$.fitted &gt;= 0.5),\n  reference = factor(resZ_aug$ecig_reg == 1),\n  positive = \"TRUE\")\n\ncm_Z\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction FALSE TRUE\n     FALSE   817   99\n     TRUE     14  270\n                                          \n               Accuracy : 0.9058          \n                 95% CI : (0.8879, 0.9218)\n    No Information Rate : 0.6925          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.7638          \n                                          \n Mcnemar's Test P-Value : 2.743e-15       \n                                          \n            Sensitivity : 0.7317          \n            Specificity : 0.9832          \n         Pos Pred Value : 0.9507          \n         Neg Pred Value : 0.8919          \n             Prevalence : 0.3075          \n         Detection Rate : 0.2250          \n   Detection Prevalence : 0.2367          \n      Balanced Accuracy : 0.8574          \n                                          \n       'Positive' Class : TRUE            \n                                          \n\n\n\n\n\nMODEL\nNAGELKERKE \\(R^2\\)\nC STATISTIC\nAIC\nBIC\nDF\n\n\n\n\nY\n0.145\n0.630\n1367.3\n1408\n7\n\n\nZ\n0.670\n0.891\n729.7\n785.7\n10\n\n\n\n\n\n\nMODEL\nSENSITIVITY\nSPECIFICITY\nPOSITIVE PREDICTIVE VALUE\n\n\n\n\nY\n0.258\n0.994\n0.950\n\n\nZ\n0.732\n0.983\n0.951"
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#validating-models-y-and-z",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#validating-models-y-and-z",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "9.5 Validating Models Y and Z",
    "text": "9.5 Validating Models Y and Z\n\n\nCode\nset.seed(4322023); (valY &lt;- validate(mod_Y_lrm))\n\n\n          index.orig training   test optimism index.corrected  n\nDxy           0.2606   0.3282 0.2917   0.0366          0.2241 40\nR2            0.1445   0.1617 0.1356   0.0260          0.1185 40\nIntercept     0.0000   0.0000 0.0657  -0.0657          0.0657 40\nSlope         1.0000   1.0000 0.9093   0.0907          0.9093 40\nEmax          0.0000   0.0000 0.0325   0.0325          0.0325 40\nD             0.1073   0.1211 0.1003   0.0208          0.0864 40\nU            -0.0017  -0.0017 0.0011  -0.0028          0.0011 40\nQ             0.1089   0.1228 0.0992   0.0236          0.0853 40\nB             0.1814   0.1786 0.1828  -0.0042          0.1856 40\ng             0.7310   0.7958 0.7134   0.0824          0.6486 40\ngp            0.1421   0.1532 0.1406   0.0126          0.1295 40\n\n\n\n\nCode\nset.seed(4322023); (valZ &lt;- validate(mod_Z_lrm))\n\n\n          index.orig training   test optimism index.corrected  n\nDxy           0.7827   0.7994 0.7809   0.0186          0.7641 40\nR2            0.6701   0.6814 0.6641   0.0173          0.6528 40\nIntercept     0.0000   0.0000 0.0495  -0.0495          0.0495 40\nSlope         1.0000   1.0000 0.9496   0.0504          0.9496 40\nEmax          0.0000   0.0000 0.0202   0.0202          0.0202 40\nD             0.6436   0.6591 0.6356   0.0235          0.6201 40\nU            -0.0017  -0.0017 0.0003  -0.0020          0.0003 40\nQ             0.6452   0.6607 0.6353   0.0254          0.6198 40\nB             0.0839   0.0815 0.0852  -0.0037          0.0876 40\ng             3.3667   3.4866 3.3112   0.1754          3.1914 40\ngp            0.3511   0.3546 0.3497   0.0049          0.3462 40\n\n\n\n9.5.1 Validated \\(R^2\\) and \\(C\\) statistics for each model\n\n\n\nMODEL\nVALIDATED \\(R^2\\)\nVALIDATED \\(C\\) STATISTIC\n\n\n\n\nY\n0.1185\n0.612\n\n\nZ\n0.6528\n0.882"
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#final-logistic-regression-model",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#final-logistic-regression-model",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "9.6 Final Logistic Regression Model",
    "text": "9.6 Final Logistic Regression Model\nI prefer Model Z, for the following reasons:\n\nModel Z has a higher validated \\(R^2\\) and \\(C\\) statistic\nModel Z has a higher Naegelkerke \\(R^2\\)\nModel Z has a lower AIC and BIC\nModel Z has a higher sensitivity, and a higher positive predictive value. While it has the lower specificity, the difference is not wide.\n\n\n9.6.1 Winning Model’s Parameter Estimates\n\n\nCode\nmod_Z_lrm\n\n\nLogistic Regression Model\n\nlrm(formula = ecig_reg ~ rcs(min_num, 4) + ecig_nico + ecig_flavored + \n    harm_perc + price_paid + ecig_flavored %ia% min_num, data = data_log_i, \n    x = TRUE, y = TRUE)\n\n                       Model Likelihood       Discrimination    Rank Discrim.    \n                             Ratio Test              Indexes          Indexes    \nObs          1200    LR chi2     773.30       R2       0.670    C       0.891    \n Yes          369    d.f.            10     R2(10,1200)0.471    Dxy     0.783    \n No           831    Pr(&gt; chi2) &lt;0.0001    R2(10,766.6)0.631    gamma   0.792    \nmax |deriv| 2e-08                             Brier    0.084    tau-a   0.334    \n\n                           Coef    S.E.   Wald Z Pr(&gt;|Z|)\nIntercept                  -3.8676 0.7261 -5.33  &lt;0.0001 \nmin_num                     0.0987 0.0090 10.99  &lt;0.0001 \nmin_num'                   -0.1707 0.0206 -8.29  &lt;0.0001 \nmin_num''                   0.4303 0.0807  5.33  &lt;0.0001 \necig_nico=No               -0.2277 0.2773 -0.82  0.4116  \necig_flavored=No           -3.0412 0.7147 -4.26  &lt;0.0001 \nharm_perc=same_or_more      0.4633 0.2540  1.82  0.0682  \nprice_paid=$10to$20        -2.1952 0.4318 -5.08  &lt;0.0001 \nprice_paid=$21to$100       -2.8602 0.4241 -6.74  &lt;0.0001 \nprice_paid=&gt;$100           -3.3635 0.7092 -4.74  &lt;0.0001 \necig_flavored=No * min_num  0.0028 0.0037  0.76  0.4457  \n\n\n\n\nCode\n# Obtaining model coefficients and exponentiating them\ntidy(mod_Z, exponentiate = TRUE,\nconf.int = TRUE, conf.level = 0.90) |&gt;\nkable(digits = 3) |&gt; \n  kable_classic_2(font_size = 24, full_width = F) \n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n0.021\n0.726\n-5.327\n0.000\n0.006\n0.065\n\n\necig_nicoNo\n0.796\n0.277\n-0.821\n0.412\n0.510\n1.271\n\n\nrcs(min_num, 4)min_num\n1.104\n0.009\n10.992\n0.000\n1.088\n1.121\n\n\nrcs(min_num, 4)min_num'\n0.843\n0.021\n-8.289\n0.000\n0.814\n0.871\n\n\nrcs(min_num, 4)min_num''\n1.538\n0.081\n5.330\n0.000\n1.349\n1.760\n\n\necig_flavoredNo\n0.048\n0.715\n-4.255\n0.000\n0.014\n0.151\n\n\nharm_percsame_or_more\n1.589\n0.254\n1.823\n0.068\n1.052\n2.429\n\n\nprice_paid$10to$20\n0.111\n0.432\n-5.084\n0.000\n0.054\n0.224\n\n\nprice_paid$21to$100\n0.057\n0.424\n-6.744\n0.000\n0.028\n0.113\n\n\nprice_paid&gt;$100\n0.035\n0.709\n-4.743\n0.000\n0.011\n0.112\n\n\necig_flavored %ia% min_num\n1.003\n0.004\n0.763\n0.446\n0.997\n1.009\n\n\n\n\n\n\n\n\n\n9.6.2 Plot of Effect Sizes for Winning Model\n\n\nCode\nsummary(mod_Z_lrm)\n\n\n             Effects              Response : ecig_reg \n\n Factor                             Low High Diff. Effect    S.E.    Lower 0.95\n min_num                            90  180  90     1.056400 0.24375  0.578620 \n  Odds Ratio                        90  180  90     2.875900      NA  1.783600 \n ecig_nico - No:Yes                  1    2  NA    -0.227660 0.27727 -0.771110 \n  Odds Ratio                         1    2  NA     0.796390      NA  0.462500 \n ecig_flavored - No:Yes              1    2  NA    -2.705000 0.41467 -3.517800 \n  Odds Ratio                         1    2  NA     0.066868      NA  0.029666 \n harm_perc - same_or_more:less_harm  1    2  NA     0.463250 0.25405 -0.034679 \n  Odds Ratio                         1    2  NA     1.589200      NA  0.965920 \n price_paid - Below$10:$21to$100     3    1  NA     2.860200 0.42410  2.029000 \n  Odds Ratio                         3    1  NA    17.465000      NA  7.606200 \n price_paid - $10to$20:$21to$100     3    2  NA     0.664980 0.25607  0.163100 \n  Odds Ratio                         3    2  NA     1.944400      NA  1.177100 \n price_paid - &gt;$100:$21to$100        3    4  NA    -0.503340 0.60451 -1.688200 \n  Odds Ratio                         3    4  NA     0.604510      NA  0.184860 \n Upper 0.95\n  1.53410  \n  4.63710  \n  0.31579  \n  1.37130  \n -1.89230  \n  0.15072  \n  0.96118  \n  2.61480  \n  3.69140  \n 40.10100  \n  1.16690  \n  3.21190  \n  0.68147  \n  1.97680  \n\nAdjusted to: min_num=120 ecig_flavored=Yes  \n\n\n\n\nCode\nplot(summary(mod_Z_lrm))\n\n\n\n\n\n\n\n9.6.3 Numerical Description of Effect Sizes\n\n\nCode\nsummary(mod_Z_lrm, conf.int = 0.90) |&gt; kable(digits = 3) |&gt; \n  kable_classic_2(font_size = 24, full_width = F)\n\n\n\n\n\n\nLow\nHigh\nDiff.\nEffect\nS.E.\nLower 0.9\nUpper 0.9\nType\n\n\n\n\nmin_num\n90\n180\n90\n1.056\n0.244\n0.655\n1.457\n1\n\n\nOdds Ratio\n90\n180\n90\n2.876\nNA\n1.926\n4.294\n2\n\n\necig_nico - No:Yes\n1\n2\nNA\n-0.228\n0.277\n-0.684\n0.228\n1\n\n\nOdds Ratio\n1\n2\nNA\n0.796\nNA\n0.505\n1.257\n2\n\n\necig_flavored - No:Yes\n1\n2\nNA\n-2.705\n0.415\n-3.387\n-2.023\n1\n\n\nOdds Ratio\n1\n2\nNA\n0.067\nNA\n0.034\n0.132\n2\n\n\nharm_perc - same_or_more:less_harm\n1\n2\nNA\n0.463\n0.254\n0.045\n0.881\n1\n\n\nOdds Ratio\n1\n2\nNA\n1.589\nNA\n1.046\n2.414\n2\n\n\nprice_paid - Below$10:$21to$100\n3\n1\nNA\n2.860\n0.424\n2.163\n3.558\n1\n\n\nOdds Ratio\n3\n1\nNA\n17.465\nNA\n8.694\n35.085\n2\n\n\nprice_paid - $10to$20:$21to$100\n3\n2\nNA\n0.665\n0.256\n0.244\n1.086\n1\n\n\nOdds Ratio\n3\n2\nNA\n1.944\nNA\n1.276\n2.963\n2\n\n\nprice_paid - &gt;$100:$21to$100\n3\n4\nNA\n-0.503\n0.605\n-1.498\n0.491\n1\n\n\nOdds Ratio\n3\n4\nNA\n0.605\nNA\n0.224\n1.634\n2\n\n\n\n\n\n\n\nprice_paid description:\nFor two individuals, subject 1 and 2, who have their first e-cigarette puff of the day the same number of minutes after waking up, who both use nicotine containing e-cigarettes, who have the same perception of the harmfulness of e-cigarettes in comparison to regular cigarettes, and use flavored e-cigarettes, and subject 1 pays less than 10 dollars for their e-cigarettes, and subject 2 pays between 21 to a 100 dollars for their e-cigarettes, according to model Z, the ratio of the odds of subject 1 being a regular e-cigarette smoker to the odds of subject 2 being a regular e-cigarette smoker is 17.465 The 90% confidence intervals around this estimate are (8.69 to 35.09).\nGiven that this interval does not include 1, this is a meaningful effect.\n\n\n9.6.4 Plot of ROC Curve for Winning Model\nThe roc function from the pROC package was used to obtain the ROC curve for model Z(the lrm model). The dataset with the outcome variable recoded as “Yes” = 1, “No” = 0 was used.\n\n\nCode\nroc.mod &lt;- \n    roc(log_i_temp$ecig_reg ~ predict(mod_Z_lrm, type=\"fitted\"),\n        ci = TRUE)\n\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &gt; cases\n\n\n\n\nCode\nplot(roc.mod, auc.polygon = TRUE, print.auc = TRUE,max.auc.polygon = TRUE,\n     grid=c(0.1, 0.2),\n     grid.col = c(\"green\", \"red\"),\n     auc.polygon.col = \"skyblue\",title = \"ROC Curve for Model Z (lrm) with the AUC score\")\n\n\n\n\n\n\n\n9.6.5 Validated \\(R^2\\) and \\(C\\) statistic for Winning Model\n\n\n\nMODEL\nVALIDATED \\(R^2\\)\nVALIDATED \\(C\\) STATISTIC\n\n\n\n\nZ\n0.6528\n0.882\n\n\n\n\n\n9.6.6 Nomogram of Winning Model\n\n\nCode\nplot(nomogram(mod_Z_lrm, fun = plogis))\n\n\n\n\n\n\n\n9.6.7 Predictions for Two New Subjects\n\n\nCode\nnew_subj &lt;-\n  data.frame(min_num = c(60,60), ecig_flavored = c(\"Yes\",\"Yes\"),ecig_nico = c(\"Yes\",\"Yes\"),harm_perc = c(\"less_harm\",\"less_harm\"), price_paid = c(\"Below$10\",\"$21to$100\")) \npreds4 &lt;- predict(mod_Z_lrm, newdata = new_subj, type = \"fitted\")\n\npreds4 \n\n\n        1         2 \n0.8584043 0.2576755 \n\n\n\n\n\n\n\n\n\n\nSubject Name\nSubject Characteristics\nPredicted Probability of being a regular E-cigarette smoker\n\n\n\n\nSubject 1\nmin_num = 60 minutes, ecig_flavored = Yes, ecig_nico = Yes, harm_perc = “less_harm”, price_paid = “Below$10”\n0.858\n\n\nSubject 2\nmin_num = 60 minutes, ecig_flavored = Yes, ecig_nico = Yes, harm_perc = “less_harm”, price_paid = “$21to$100”\n0.258"
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#answering-my-research-questions",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#answering-my-research-questions",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "10.1 Answering My Research Questions",
    "text": "10.1 Answering My Research Questions\n\n10.1.1 My First Research Question\nE-Cigarette Perception, Smoking Habits, and their association with Heavy E-Cigarette Use:\nAre smoking e-cigarettes that contain nicotine, the perception of the purported healthiness of e-cigarettes when compared to smoking regular cigarettes, and smoking habits strong predictors of heavy e-cigarette use in adulthood?\n\n\nCode\n# Transforming the coefficients of Model B back to the original variable's scale\ntidy_mod_B |&gt; \n  mutate(estimate = sprintf(\"%.2f\",(exp(estimate) - 1)),\n         conf.low = sprintf(\"%.2f\",(exp(conf.low) - 1)),\n         conf.high = sprintf(\"%.2f\",(exp(conf.high) - 1))) |&gt;\n  select(term,estimate,conf.low,conf.high, p.value) |&gt;\n  kbl(digits = 3) |&gt; \n  kable_classic_2(font_size = 24, full_width = F)  \n\n\n\n\n\nterm\nestimate\nconf.low\nconf.high\np.value\n\n\n\n\n(Intercept)\n31.34\n21.74\n44.99\n0.000\n\n\nrcs(puff_num, 4)puff_num\n-0.46\n-0.50\n-0.43\n0.000\n\n\nrcs(puff_num, 4)puff_num'\n95957.21\n42612.78\n216078.82\n0.000\n\n\nrcs(puff_num, 4)puff_num''\n-1.00\n-1.00\n-1.00\n0.000\n\n\nage_range18to24\n-0.10\n-0.25\n0.08\n0.357\n\n\nage_range25to34\n0.02\n-0.16\n0.23\n0.863\n\n\nage_range35to44\n-0.06\n-0.23\n0.15\n0.612\n\n\nage_range45to54\n0.02\n-0.16\n0.25\n0.838\n\n\nage_range&gt;55\n0.17\n-0.05\n0.43\n0.206\n\n\nless_harmNo\n0.16\n-0.04\n0.39\n0.204\n\n\necig_nicoNo\n-0.11\n-0.21\n-0.01\n0.076\n\n\npuff_num %ia% less_harm\n-0.03\n-0.05\n-0.01\n0.029\n\n\n\n\n\n\n\nE-CIGARETTE PUFFS PER DAY:\nHigher number of puffs taken per day are correlated with a higher number of days an e-cigarette was smoked in the past 30 days, provided all other variables are held constant. This is evident from the confidence intervals of puff_num, which do not include zero.\nUSAGE OF E-CIGARETTES WITH NICOTINE:\nThe usage of e-cigarettes that do not contain nicotine are associated with a significantly lower number of days an e-cigarette was smoked by a study participant in the past 30 days, provided all other variables are held constant.\nThe point estimate for this effect is -0.11, with 90%CI(-0.21 to -0.01). Given that the confidence interval does not include zero, this is a meaningful effect. (p = 0.076)\nE-CIGARETTE SMOKING INITIATION AGE:\nInitiation of e-cigarette smoking at a younger age does not have a meaningful correlation with heavy e-cigarette smoking by an individual in the past 30 days, provided all other variables are held constant. This is evident from the confidence intervals for the different levels of age_range, all of which contain zero, when compared to the number of days an e-cigarette was smoked in the past 30 days by individuals who initiated e-cigarette smoking when they were less than 18 years of age.\nPERCEPTION OF THE HARMFULNESS OF E-CIGARETTES COMPARED TO REGULAR CIGARETTES:\nThe perception of the supposed healthiness of e-cigarettes when compared to regular cigarettes do not have a meaningful correlation with an individual’s e-cigarette smoking in the past 30 days, provided all other variables are held constant. This is evident from the confidence intervals for the term less_harmNo from the table of model B’s coefficients, which include zero. (p = 0.204)\nIn summation, to answer my first research question, \nFactors such as a higher number of puffs taken from an e-cigarette daily, and usage of nicotine-containing e-cigarettes are correlated with heavier smoking habits over the past 30 days.\nFactors such as an individual’s perception of the supposed healthiness of e-cigarettes when compared to regular cigarettes, and their age when they began smoking e-cigarettes, do not have a meaningful impact on an individual’s smoking habits over the past 30 days.\n\n\n10.1.2 My Second Research Question\nRegular e-cigarette use and it’s associated factors:\nAre factors such as using flavored e-cigarettes, or needing to use e-cigarettes immediately after waking up, or using e-cigarettes with nicotine, or use of e-cigarettes as a healthier alternative to regular cigarettes associated with regular e-cigarette use?\n\n\nCode\nsummary(mod_Z_lrm, conf.int = 0.90) |&gt; kable(digits = 3) |&gt; \n  kable_classic_2(font_size = 24, full_width = F)\n\n\n\n\n\n\nLow\nHigh\nDiff.\nEffect\nS.E.\nLower 0.9\nUpper 0.9\nType\n\n\n\n\nmin_num\n90\n180\n90\n1.056\n0.244\n0.655\n1.457\n1\n\n\nOdds Ratio\n90\n180\n90\n2.876\nNA\n1.926\n4.294\n2\n\n\necig_nico - No:Yes\n1\n2\nNA\n-0.228\n0.277\n-0.684\n0.228\n1\n\n\nOdds Ratio\n1\n2\nNA\n0.796\nNA\n0.505\n1.257\n2\n\n\necig_flavored - No:Yes\n1\n2\nNA\n-2.705\n0.415\n-3.387\n-2.023\n1\n\n\nOdds Ratio\n1\n2\nNA\n0.067\nNA\n0.034\n0.132\n2\n\n\nharm_perc - same_or_more:less_harm\n1\n2\nNA\n0.463\n0.254\n0.045\n0.881\n1\n\n\nOdds Ratio\n1\n2\nNA\n1.589\nNA\n1.046\n2.414\n2\n\n\nprice_paid - Below$10:$21to$100\n3\n1\nNA\n2.860\n0.424\n2.163\n3.558\n1\n\n\nOdds Ratio\n3\n1\nNA\n17.465\nNA\n8.694\n35.085\n2\n\n\nprice_paid - $10to$20:$21to$100\n3\n2\nNA\n0.665\n0.256\n0.244\n1.086\n1\n\n\nOdds Ratio\n3\n2\nNA\n1.944\nNA\n1.276\n2.963\n2\n\n\nprice_paid - &gt;$100:$21to$100\n3\n4\nNA\n-0.503\n0.605\n-1.498\n0.491\n1\n\n\nOdds Ratio\n3\n4\nNA\n0.605\nNA\n0.224\n1.634\n2\n\n\n\n\n\n\n\nTIME TAKEN FOR THE FIRST E-CIGARETTE PUFF OF THE DAY\nIndividuals who need to use e-cigarettes within an hour or an hour and a half of waking up have detectable higher odds of identifying as regular users of e-cigarettes, provided the other variables are held constant. This is evident in the 90% confidence intervals for the odds ratio in the coefficient table, which does not include 1.\nUSAGE OF FLAVORED E-CIGARETTES:\nIndividuals who do not use flavored e-cigarettes have detectable lower odds of identifying as regular users of e-cigarettes, provided the other variables are held constant. This is evident in the 90% confidence intervals for the odds ratio in the coefficient table, which does not include 1. The point estimate for this odds ratio is 0.07, with 90% confidence intervals (0.03 to 0.13). Individuals who do not use flavored e-cigarettes have only 0.07 times the odds(90%CI 0.03 to 0.13) of identifying as regular e-cigarette smokers, compared to individuals who do use flavored e-cigarettes.\nCOST OF E-CIGARETTE BRAND:\nIndividuals who pay less than 10 dollars for their e-cigarettes have detectably higher odds of identifying as regular e-cigarettes, as compared to individuals who paid anywhere between 21 to 100 dollars for their e-cigarette, provided the other variables are held constant. This is evident in the 90% confidence intervals for the odds ratio in the coefficient table, which do not include zero. Similarly, individuals who pay anywhere between 10 to 20 dollars for their e-cigarette also have detectably higher odds of identifying as regular e-cigarette smokers\nHowever, individuals who pay more than 100 dollars for their e-cigarettes do not have meaningfully different odds of identifying as regular e-cigarette smokers compared to individuals who pay anywhere between 21 to 100 dollars for their e-cigarettes, provided the other variables remain constant. This is evident in the 90% confidence intervals for the odds ratio of this effect in the coefficient table, which includes 1.\nUSAGE OF E-CIGARETTES CONTAINING NICOTINE:\nIndividuals who do not use e-cigarettes with nicotine do not have meaningfully different odds of identifying as regular e-cigarette smokers compared to individuals who use e-cigarettes containing nicotine, provided the other variables are held constant. The 90% confidence intervals for the odds ratio of this effect includes 1.\nPERCEPTION OF THE HARMFULNESS OF E-CIGARETTES AS COMPARED TO REGULAR CIGARETTES:\nIndividuals who perceive e-cigarettes as being the same or more harmful than regular e-cigarettes do not have meaningfully different odds of identifying as regular e-cigarette smokers compared to individuals who perceive e-cigarettes as being less harmful than regular e-cigarettes, provided the other variables are held constant. The odds ratio for this effect includes 1.\nIn summation, to answer my second research question, \nFactors such as needing to use e-cigarettes immediately after waking up, using flavored e-cigarettes, and using cheaper e-cigarette brands are all associated with detectably higher odds of identifying as a regular e-cigarette user, provided the other variables are held constant.\nFactors such as the usage of nicotine containing e-cigarettes, and the perceptiom of the harmfulness of e-cigarettes as compared to regular cigarettes do not have a meaningful impact on the odds of identifying as a regular e-cigarette user, provided all other variables remain constant."
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#limitations-and-next-steps",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#limitations-and-next-steps",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "10.2 Limitations and Next Steps",
    "text": "10.2 Limitations and Next Steps\n\n10.2.1 Model B and it’s limitations\nGiven that model B was predicting a count outcome, and given then the outcome was transformed using a logarithm transformation after adding a non-zero integer to the outcome, the resulting model is very highly driven by the puff_num variable, resulting in exploding coefficients for the restricted cubic spline applied on puff_num.\nModel B outperformed Model A on all metrics that were studied. Visualization of the transformed outcome variable showed why that was the case, given the non-linear, skewed nature of the data under study. The data lent itself very well towards a non-linear term, even if the restricted cubic spline had exploding coefficients.\nThis outcome could be better modeled with a Poisson regression, and by using a larger dataset with lesser missing values. The next ideal step would be to expand the dataset to more than 1200, and model the outcome using a Poisson regression.\n\n\n10.2.2 Model Z and it’s limitations\nThe glm fit of the model produced a warning stating that predicted probabilities of 0 or 1 had occurred. Given that the data under analysis had a large amount of missingness, imputation possibly skewed the model towards one direction or the other. It is important to note that this warning was only produced in the glm model, while the lrm model did not produce such a warning.\nThis model could be improved by vastly expanding the sample size, minimizing the effect of the missing data and smoothing out the predicted probabilites so that the preceding skew does not repeat itself."
  },
  {
    "objectID": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#thoughts-on-project-a",
    "href": "portfolio_projects/PATH_study/Project_A_Portfolio_Naveen_Kannan.html#thoughts-on-project-a",
    "title": "Predictors of Heavy E-Cigarette Use among Adults in the United States",
    "section": "10.3 Thoughts on Project A",
    "text": "10.3 Thoughts on Project A\nProject A was more difficult than I expected it to be. I went in knowing it would take a decent amount of work and effort, and I was reasonably confident that I could tackle it, since I knew my experience in 431 had already helped prepare me for the work that would be required. The hardest part of Project A was dealing with the large number of missing values. The impact of imputation on both models was significant, and discussing the limitations and understanding how the models could be improved was a difficult task, although I did learn a lot from the difficulty I had with the data.\nAt the end of Project A, I wished that I had chosen a non count outcome for the linear regression.We hadn’t covered count outcomes at the time of submission of the Plan, and if I’d known the difficulties inherent in using linear regression for a count outcome when I started, I would have picked a different outcome variable."
  },
  {
    "objectID": "posts/2023-03-22/01_blog.html",
    "href": "posts/2023-03-22/01_blog.html",
    "title": "p values, Statistical Significance, and the magic number.",
    "section": "",
    "text": "Ever since I’ve been in academia, the term (‘p &lt;0.05’) has always been a constant.\nFrom the days of medical school, where my classmates and I learned to read and understand scientific journals and articles, all the way to grad school (where I currently am), for better or for worse, p values have been an eternal constant.\nWe accept reality in the way it is presented to us. In much the same fashion, I accepted the notion of p values and the effect it had on statistical significance as a fundamental part of basic science.\nAnd why wouldn’t I?\nP values are everywhere. In the scientific articles of most sciences related to healthcare, even tangentially, p values seem not just important, but essential to back up the validity of an observation, or of a statistical test.\n\nThe Dichotomization of Science\nWhy have p values become so inextricably linked with scientific investigation? Millions of articles use them in conjunction with the term statistical significance. Is it really so simple as a bright line in the sand being the only metric that separates meaningless noise from meaningful findings?\nTo be more precise,\nIs a p value of 0.051 so different from a p value of 0.049? Is one really so much more significant than the other?\nFirst, what does statistically significant even mean to begin with?\nThe term’s original use (all the way back in 1885!) was to be a simple tool to identify if a particular observation or inference merited further exploration. It was never meant to be a yardstick for what was scientifically important, and what wasn’t. Are p values such infallible arbiters of something being, for example,\nstatistically significant versus clinically significant?\n\n\nWhere do we go from here?\nOkay, I think you get the point. I’m hardly the first person to bring this up.\nBut where do we go from here? How do we navigate the uncertainty that is inherent to data?\nDo we replace p values with something else? Perhaps confidence intervals that include or exclude the null hypothesis?\nA Bonferroni correction?\nMuch like how there is no clear line between noise and signal, there is no one-size-fits-all solution to this problem. As long as statistical tests exist, the urge to use them to explain patterns and give meaning to uncertainity will also exist.\nWe cannot just simply swap one statistical test for another.\nTo me, atleast, the way forward is what has been recommended by the ASA.\nHonest and open communication of results and inferences, such as simply providing the p value as a continuous value, and providing point and interval estimates would go a long way in fostering a thoughtful, open environment.\nThis would be more conducive for replicablity and reproducible research, one of the cornerstones of responsible research!\nThis is a learning process, and I’m still working on it too. But hopefully, the learning pain of moving into a world that has moved past p&lt;0.05 will ease up with thoughtful research and open communication!"
  },
  {
    "objectID": "posts/ansible_cluster/ansible_cluster_config.html",
    "href": "posts/ansible_cluster/ansible_cluster_config.html",
    "title": "Using Ansible to remotely configure a cluster.",
    "section": "",
    "text": "Ansible is an open-source IT automation tool that allows for automated management of remote systems.\n\n\n\n\n\nA basic Ansible environment has the following three components:\n\nControl Node: This is a system on which Ansible is installed, and the system from which Ansible commands such as ansible-inventory are issued. This is also where Ansible playbooks and configuration files are stored.\nManaged node: This is a remote system that Ansible intends to manage and configure.\nInventory: This is a list of managed nodes that are organized locally on the control node. This lists the IP addresses or the hostnames of the remote systems being managed along with any connection information needed.\n\n\n\n\nA flowchart demonstrating the basic architecture of an Ansible environment."
  },
  {
    "objectID": "posts/ansible_cluster/ansible_cluster_config.html#generating-an-ssh-key",
    "href": "posts/ansible_cluster/ansible_cluster_config.html#generating-an-ssh-key",
    "title": "Using Ansible to remotely configure a cluster.",
    "section": "Generating an SSH key",
    "text": "Generating an SSH key\nI created a folder inside the ansible_mount directory called .ssh.\nI mounted /path/to/ansible_mount/.ssh to /root/.ssh in the Docker container.\nThen, once within the Docker instance, I ran ssh-keygen -t ed25519 to generate a SSH key inside /root/.ssh.\nSince the folder is mounted, this key persists, and is not deleted after termination of the container.\nThis generates two things. id_ed25519, and id_ed25519.pub.\nid_ed25519 is the private key and cannot be shared under any circumstances.\nid_ed25519.pub is our public key, and this is what we shall use to connect to the 4 nodes."
  },
  {
    "objectID": "posts/ansible_cluster/ansible_cluster_config.html#editing-the-config-file-in-.ssh-to-set-up-a-jump-host",
    "href": "posts/ansible_cluster/ansible_cluster_config.html#editing-the-config-file-in-.ssh-to-set-up-a-jump-host",
    "title": "Using Ansible to remotely configure a cluster.",
    "section": "Editing the config file in .ssh to set up a jump host",
    "text": "Editing the config file in .ssh to set up a jump host\nNow, in order to be able to SSH into the clusters of the nodes in one line of code, I created a config file config at path/to/ansible_mount/.ssh/config.\nSince the entirety of path/to/ansible_mount/.ssh is being mounted into the container at /root/.ssh, this config file will persist.\nTo reiterate, path/to/ansible_mount/.ssh contains both the keys and the config file.\nThis config file has the following text:\nHost XXX.XX.XXX.X\n    HostName abc.host.domain\n\nHost YY.Y.Y.2\n    ProxyJump sudo_user@XXX.XX.XXX.X\n\nHost YY.Y.Y.3\n    ProxyJump sudo_user@XXX.XX.XXX.X\n\nHost YY.Y.Y.4\n    ProxyJump sudo_user@XXX.XX.XXX.X\nThis sets up the head node as a jump server through which we can SSH into the cluster nodes using the private IP.\nThis allows for using XXX.XX.XXX.X(the head node of the cluster) as an intermediate server from which one can jump directly into the clusters.\nHere, the term sudo_user refers to the user(s) who have sudo access to the head node."
  },
  {
    "objectID": "posts/ansible_cluster/ansible_cluster_config.html#exchanging-ssh-keys-with-the-head-node",
    "href": "posts/ansible_cluster/ansible_cluster_config.html#exchanging-ssh-keys-with-the-head-node",
    "title": "Using Ansible to remotely configure a cluster.",
    "section": "Exchanging SSH keys with the head node",
    "text": "Exchanging SSH keys with the head node\nHowever, first I needed to set up a key exchange between the Docker container and the head node, so that I could directly SSH into the head node without a need for a password.\nConnecting to a server for the first time through SSH requires input from the user to establish the authenticity of the host. Manually SSH into the head node if you’re doing this for the first time to confirm the authenticity of the host.\nYou will see something like this if you’re connecting to the head node for the first time:\nThe authenticity of host 'YY.Y.Y.4 (&lt;no hostip for proxy command&gt;)' can't be established.\nECDSA key fingerprint is xxx.\nAre you sure you want to continue connecting (yes/no/[fingerprint])? \nMake sure to type in yes and accept this process.\nFrom the docker container, I ran:\nssh-copy-id -i /root/.ssh/id_ed25519.pub sudo_user@abc.host.domain\nYou will see something like:\n/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: \"/root/.ssh/id_ed25519.pub\"\n/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed\n/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys\nEnter your password when prompted and you should see:\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   \"ssh 'sudo_user@abc.host.domain'\"\nand check to make sure that only the key(s) you wanted were added.\nThis has copied the public key from the docker container into the head node at /.ssh/authorized_keys.\nThis made the docker container trusted by the head node, so when SSH’ing into the head node, there is no need for a password.\nTest this out by running ssh sudo_user@abc.host.domain and see if it gets you into the head node without prompting you for a password."
  },
  {
    "objectID": "posts/ansible_cluster/ansible_cluster_config.html#exchanging-ssh-keys-with-the-cluster-nodes",
    "href": "posts/ansible_cluster/ansible_cluster_config.html#exchanging-ssh-keys-with-the-cluster-nodes",
    "title": "Using Ansible to remotely configure a cluster.",
    "section": "Exchanging SSH keys with the cluster nodes",
    "text": "Exchanging SSH keys with the cluster nodes\nAs said before, connecting to a server for the first time through SSH requires input from the user to establish the authenticity of the host. However, this is a bit more complicated for the cluster nodes. This will require you to do the following:\nOnce in the container, run:\nssh -J sudo_user@abc.host.domain sudo_user@Y.Y.Y.2\nTo connect to the cluster node using the head node as a jump node. This uses the head node as a jump server between the container and the cluster node.\nAccept the connection process as before and enter the password for the sudo_user on the cluster node.\nDo the same for all the nodes.\nMake sure you accept the authenticity of the host by performing the above ssh command before proceeding!\nNow, running these commands:\nssh sudo_user@XX.X.X.2\nssh sudo_user@XX.X.X.3\nssh sudo_user@XX.X.X.4\nAllowed me to directly SSH into the cluster nodes as the user sudo_temp from the container, since we set up the ProxyJump function using the XXX.XX.XXX.X IP address of the head node!\nNote that running these commands before completing the fingerprinting process will fail. You will need to manually connect to the clusters individually first.\nHowever, this would still require me to enter the password for sudo_user@XXX.XX.XXX.X, since the cluster nodes did not have the keys in the docker container exchanged with it.\nFrom the container, I ran:\nssh-copy-id -i /root/.ssh/id_ed25519.pub sudo_user@&lt;private_IP_address_for_cluster&gt;\nFor every single cluster. Enter the password when prompted. Again, like before, you should see something like:\n/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: \"/root/.ssh/id_ed25519.pub\"\n/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed\n/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys\nEnter your password for the sudo_user on the cluster nodes. Again, you should see something like:\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   \"ssh 'sudo_user@YY.Y.Y.2'\"\nand check to make sure that only the key(s) you wanted were added.\nTest the key exchange process by running:\nssh sudo_user@YY.Y.Y.2.\nDo the same for all the other cluster nodes.\nThis copied the public key of the docker container into the authorized_keys for the 3 servers."
  },
  {
    "objectID": "posts/ansible_cluster/ansible_cluster_config.html#direct-ssh-connections-into-the-head-node-and-the-cluster-nodes",
    "href": "posts/ansible_cluster/ansible_cluster_config.html#direct-ssh-connections-into-the-head-node-and-the-cluster-nodes",
    "title": "Using Ansible to remotely configure a cluster.",
    "section": "Direct SSH connections into the head node and the cluster nodes",
    "text": "Direct SSH connections into the head node and the cluster nodes\nNow it was possible to SSH into all 4 servers, without needing to enter a password.\nFor the head node, it was with the command:\nssh sudo_user@abc.host.domain\nFor the cluster nodes, it was with the command:\nssh &lt;private_IP_address_for_cluster&gt; -l sudo_user\nThis is essential to streamline the process by which Ansible can connect to the cluster nodes, since otherwise it would need the inventory files and the configuration files edited to enable ProxyJump and SSH password entry.\nAnsible allows for using of a -u flag for user, and -u will always be sudo_user, since the keys are authorized for user temp."
  },
  {
    "objectID": "posts/containers_hpc/containers_HPC.html#container-lingo",
    "href": "posts/containers_hpc/containers_HPC.html#container-lingo",
    "title": "Docker, Singularity, and HPC.",
    "section": "Container Lingo",
    "text": "Container Lingo\nContainers are built from images.\nImages are built from a series of read-only “layers”, and each one of these layers are instructions from the specification file(most commonly, a dockerfile for Docker containers, and a definition(def) file for Singularity.) that was used to build an image.\nThink of images as the template from which containers are run.\nA container is a running instance of an image, and they are isolated from the host system."
  },
  {
    "objectID": "posts/docker_root/docker_root.html",
    "href": "posts/docker_root/docker_root.html",
    "title": "Moving Docker’s Data directory to another location.",
    "section": "",
    "text": "Docker is a container service that we have discussed previously in my blog posts.\n\n\n\n\n\nThe default storage location for Docker is at /var/lib/docker. Speaking from experience, as images and containers are built over a period of time, especially if there are multiple users using the Docker service, the root filesystem can run into issues where the size of the Docker storage directory can cause potential out-of-space crises, and significantly deteriorate overall system performance.\nIn this blog post, we will look at transferring the Storage directory to another location. (In my case, I transferred it to a physical hard drive with expanded storage capacity). It’s an essential step to ensure that Docker operations continue to run smoothly and efficiently, particularly in production environments."
  },
  {
    "objectID": "posts/docker_root/docker_root.html#stopping-the-docker-service.",
    "href": "posts/docker_root/docker_root.html#stopping-the-docker-service.",
    "title": "Moving Docker’s Data directory to another location.",
    "section": "Stopping the Docker service.",
    "text": "Stopping the Docker service.\nThe Docker service can be stopped with the command:\nsystemctl stop docker\nSystemctl is a Linux command used for controlling and managing system services, including starting, stopping, enabling, and disabling them, among other tasks."
  },
  {
    "objectID": "posts/docker_root/docker_root.html#transferring-the-docker-storage-files-from-varlibdocker-to-its-new-location-while-preserving-the-files-metadata.",
    "href": "posts/docker_root/docker_root.html#transferring-the-docker-storage-files-from-varlibdocker-to-its-new-location-while-preserving-the-files-metadata.",
    "title": "Moving Docker’s Data directory to another location.",
    "section": "Transferring the Docker storage files from /var/lib/docker to it’s new location, while preserving the file’s metadata.",
    "text": "Transferring the Docker storage files from /var/lib/docker to it’s new location, while preserving the file’s metadata.\n\nCreating a new directory for the storage\nmkdir /path/to/new/storage\n\n\nCopying the existing storage to the new location\nrsync -aqxP /var/lib/docker /path/to/new/storage\nThe rsync command is used to synchronize and copy files and directories from one location to another. It stands for remote sync, and is a file synchronization tool.\n-aqxP:These are the options and flags used with rsync.\n\n-a: This option stands for “archive” and is used to perform a recursive copy while preserving file permissions, ownership, timestamps, and more.\n-q: The “quiet” option suppresses non-error messages, making the output less verbose.\n-x: This option ensures that rsync does not cross filesystem boundaries. It prevents rsync from copying data to a different filesystem.\n-P: This combines two options: -P is equivalent to -rlptgD and –partial. It enables resumable copying and shows progress during the transfer."
  },
  {
    "objectID": "posts/docker_root/docker_root.html#renaming-the-old-docker-storage.",
    "href": "posts/docker_root/docker_root.html#renaming-the-old-docker-storage.",
    "title": "Moving Docker’s Data directory to another location.",
    "section": "Renaming the old Docker storage.",
    "text": "Renaming the old Docker storage.\nmv /var/lib/docker /var/lib/docker.old"
  },
  {
    "objectID": "posts/docker_root/docker_root.html#creating-a-daemon.json-file-at-etcdocker.",
    "href": "posts/docker_root/docker_root.html#creating-a-daemon.json-file-at-etcdocker.",
    "title": "Moving Docker’s Data directory to another location.",
    "section": "Creating a daemon.json file at /etc/docker.",
    "text": "Creating a daemon.json file at /etc/docker.\nThis file will help point the Docker service to the new storage location.\necho '{\n  \"data-root\": \"/path/to/new/storage\"\n}' &gt; /etc/docker/daemon.json"
  },
  {
    "objectID": "posts/docker_root/docker_root.html#editing-the-systemctl-file-to-allow-docker-to-connect-to-the-docker-daemon",
    "href": "posts/docker_root/docker_root.html#editing-the-systemctl-file-to-allow-docker-to-connect-to-the-docker-daemon",
    "title": "Moving Docker’s Data directory to another location.",
    "section": "Editing the systemctl file to allow Docker to connect to the Docker Daemon",
    "text": "Editing the systemctl file to allow Docker to connect to the Docker Daemon\nWhen I attempted to perform the storage transfer on my own, I originally did not have this step included. When restarting the Docker service, I got the following error whenever I attempted any Docker commands.\nCannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\nThe Docker daemon was very clearly running (by running systemctl status docker), but the docker cli was unable to connect to the socket file.\nThis was because I had restarted the Docker daemon as a service using the systemctl command.\nI found my service file path from the output of systemctl status docker.\nThe following is the first two lines of the output produced by this command.\n● docker.service - Docker Application Container Engine\n   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)\nEdit this file at the specified location.\nThe following was the content in the file on my system. Pay attention to the line that starts with ExecStart=/usr/bin/dockerd.\n[Unit]\nDescription=Docker Application Container Engine\n...\n\n[Service]\nType=notify\n# the default is not to use systemd for cgroups because the delegate issues still\n# exists and systemd currently does not support the cgroup feature set required\n# for containers run by docker\nExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock\n...\nThe -H fd:// means the daemon is using a file descriptor managed by systemctl, without pointing towards the socket file at /var/run/docker.sock.\nModify the line to look like this:\nExecStart=/usr/bin/dockerd -H unix:///var/run/docker.sock --containerd=/run/containerd/containerd.sock\n\nThis command can be run to do it as part of a script.\nsed -i 's/^ExecStart=.*/ExecStart=\\/usr\\/bin\\/dockerd -H unix:\\/\\/\\/var\\/run\\/docker.sock --containerd=\\/run\\/containerd\\/containerd.sock/' /lib/systemd/system/docker.service"
  },
  {
    "objectID": "posts/docker_root/docker_root.html#restarting-the-daemons-and-the-docker-service",
    "href": "posts/docker_root/docker_root.html#restarting-the-daemons-and-the-docker-service",
    "title": "Moving Docker’s Data directory to another location.",
    "section": "Restarting the Daemons and the Docker service",
    "text": "Restarting the Daemons and the Docker service\nsystemctl daemon-reload\nsystemctl start docker"
  },
  {
    "objectID": "posts/docker_root/docker_root.html#testing",
    "href": "posts/docker_root/docker_root.html#testing",
    "title": "Moving Docker’s Data directory to another location.",
    "section": "Testing!",
    "text": "Testing!\n\nRun docker image ls and docker ps -a commands to see if the images and containers have been succesfully transferred, and Docker is able to see them at the new location.\nRun docker info and look for the line that starts with Docker Root Dir:. Check if it’s pointing towards the new storage location."
  },
  {
    "objectID": "posts/docker_root/docker_root.html#deleting-the-old-storage-directory",
    "href": "posts/docker_root/docker_root.html#deleting-the-old-storage-directory",
    "title": "Moving Docker’s Data directory to another location.",
    "section": "Deleting the old storage directory",
    "text": "Deleting the old storage directory\nThis is a risky step in my opinion. I have the original storage directory preserved at the /var/lib folder, only having renamed it to docker.old. If you are able to run the Docker service with the new storage location without any issues ( try creating new images and containers!) then it may be a good idea to delete it. (Definitely make a backup of it regardless.)"
  },
  {
    "objectID": "posts/hive_sql/hive_sql.html#metadata-management",
    "href": "posts/hive_sql/hive_sql.html#metadata-management",
    "title": "Installing and configuring the HIVE metastore with a MySQL backend.",
    "section": "Metadata Management",
    "text": "Metadata Management\nThe Hive Metastore serves as a central repository for storing and managing metadata related to datasets within Hive. This metadata includes essential information such as:\n\nTable schemas\nData types\nColumn names\nPhysical data file locations\n\nWithout the metastore, Hive would struggle to efficiently keep track of these critical metadata elements."
  },
  {
    "objectID": "posts/hive_sql/hive_sql.html#schema-definition",
    "href": "posts/hive_sql/hive_sql.html#schema-definition",
    "title": "Installing and configuring the HIVE metastore with a MySQL backend.",
    "section": "Schema Definition",
    "text": "Schema Definition\nIn Hive, tables are defined using a schema that specifies column names and their associated data types. This schema is fundamental for querying and processing data correctly. The metastore takes on the responsibility of storing and managing these table schemas, offering users the flexibility to:\n\nCreate new tables\nModify existing schemas\nDrop tables that are no longer needed"
  },
  {
    "objectID": "posts/hive_sql/hive_sql.html#metadata-scaling",
    "href": "posts/hive_sql/hive_sql.html#metadata-scaling",
    "title": "Installing and configuring the HIVE metastore with a MySQL backend.",
    "section": "Metadata Scaling",
    "text": "Metadata Scaling\nIn large-scale data environments, metadata can become extensive. Fortunately, the Hive Metastore can scale horizontally to handle substantial volumes of metadata efficiently. Additionally, it can be configured for high availability, ensuring continuous access to metadata even in the face of hardware failures or other issues."
  },
  {
    "objectID": "posts/hive_sql/hive_sql.html#download-the-mysql-repo-onto-the-node",
    "href": "posts/hive_sql/hive_sql.html#download-the-mysql-repo-onto-the-node",
    "title": "Installing and configuring the HIVE metastore with a MySQL backend.",
    "section": "Download the MySQL Repo onto the node",
    "text": "Download the MySQL Repo onto the node\nThe yum repo for MySQL needs to be downloaded to the system. The appropriate repo for your system can be found here."
  },
  {
    "objectID": "posts/hive_sql/hive_sql.html#install-the-repo-using-yum",
    "href": "posts/hive_sql/hive_sql.html#install-the-repo-using-yum",
    "title": "Installing and configuring the HIVE metastore with a MySQL backend.",
    "section": "Install the repo using yum",
    "text": "Install the repo using yum\nI downloaded the repo to /opt/mysql80-community-release-el8-8.noarch.rpm in my file directory.\nInstall the contents of the repo using\nyum localinstall /opt/mysql80-community-release-el8-8.noarch.rpm -y"
  },
  {
    "objectID": "posts/hive_sql/hive_sql.html#disabling-the-local-mysql-module",
    "href": "posts/hive_sql/hive_sql.html#disabling-the-local-mysql-module",
    "title": "Installing and configuring the HIVE metastore with a MySQL backend.",
    "section": "Disabling the local mysql module",
    "text": "Disabling the local mysql module\nDisable the local mysql module to prevent conflicts with the installation of the mysql-community-server module.\nyum -y module disable mysql"
  },
  {
    "objectID": "posts/hive_sql/hive_sql.html#installing-mysql-community-server",
    "href": "posts/hive_sql/hive_sql.html#installing-mysql-community-server",
    "title": "Installing and configuring the HIVE metastore with a MySQL backend.",
    "section": "Installing mysql-community-server",
    "text": "Installing mysql-community-server\nInstall the Community Server version of MySQL using:\nyum -y install mysql-community-server"
  },
  {
    "objectID": "posts/hive_sql/hive_sql.html#installing-mysql-connector-java",
    "href": "posts/hive_sql/hive_sql.html#installing-mysql-connector-java",
    "title": "Installing and configuring the HIVE metastore with a MySQL backend.",
    "section": "Installing mysql-connector-java",
    "text": "Installing mysql-connector-java\nmysql-connector-java, or the MySQL JDBC (Java Database Connectivity) driver, is a Java-based library that provides connectivity between Java applications and the MySQL database management system. It allows Java programs to interact with MySQL databases by facilitating the exchange of data and SQL queries.\nInstall it by running\nyum -y install mysql-connector-java"
  },
  {
    "objectID": "posts/hive_sql/hive_sql.html#initializing-the-mysql-service",
    "href": "posts/hive_sql/hive_sql.html#initializing-the-mysql-service",
    "title": "Installing and configuring the HIVE metastore with a MySQL backend.",
    "section": "Initializing the mysql service",
    "text": "Initializing the mysql service\nInitialize MySQL by running:\nservice mysqld start\nThis will initialize the MySQL service. However, this installation of MySQL needs to be secured!"
  },
  {
    "objectID": "posts/hive_sql/hive_sql.html#running-mysql_secure_installation",
    "href": "posts/hive_sql/hive_sql.html#running-mysql_secure_installation",
    "title": "Installing and configuring the HIVE metastore with a MySQL backend.",
    "section": "Running mysql_secure_installation",
    "text": "Running mysql_secure_installation\nMySQL version 8.0 or higher generates a temporary random password in /var/log/mysqld.log after installation.\nFind your random password by running\ngrep 'A temporary password' /var/log/mysqld.log |tail -1 |awk '{split($0,a,\": \"); print a[2]}'\nOnce you have that, run mysql_secure_installation.\nThis script will guide you through setting up a root password, removing anonymous users, disallowing remote root login, and other security-related configurations."
  },
  {
    "objectID": "posts/hive_sql/hive_sql.html#adding-new-users-and-setting-up-the-metastore",
    "href": "posts/hive_sql/hive_sql.html#adding-new-users-and-setting-up-the-metastore",
    "title": "Installing and configuring the HIVE metastore with a MySQL backend.",
    "section": "Adding new users and setting up the metastore",
    "text": "Adding new users and setting up the metastore\nInitialize MySQL by running\nmysql -u root -p\nand enter your password.\nRun the following:\nmysql&gt; CREATE DATABASE metastore;\nQuery OK, 1 row affected (0.02 sec)\nmysql&gt; use metastore;\nDatabase changed\nmysql&gt; create user 'hive'@'localhost' identified by '&lt;root_password&gt;';\nQuery OK, 0 rows affected (0.02 sec)\nmysql&gt; grant all PRIVILEGES on *.* to 'hive'@'localhost' with grant option;\nQuery OK, 0 rows affected (0.02 sec)\nmysql&gt; create user 'hdfs'@'localhost' identified by '&lt;root_password&gt;';\nQuery OK, 0 rows affected (0.04 sec)\nmysql&gt; grant all PRIVILEGES on *.* to 'hdfs'@'localhost' with grant option;\nQuery OK, 0 rows affected (0.03 sec)\nmysql&gt; FLUSH PRIVILEGES;\nQuery OK, 0 rows affected (0.02 sec)\nmysql&gt; exit\nBye\nReplace &lt;root_password&gt; with your actual password. This will create the metastore and also create the users hive and hdfs and grant them all necessary privileges to work within the metastore and it’s data.\n\nBonus! Automate this!\nIf you’ve been following my posts, you’ll know that I’ve been using Ansible to automate this process. The above command is interactive! How do we get around this? If you’re deploying MySQL for production level systems, then this might be an unnecessary expenditure of time.\nIn this section, I’ve added a bash script that automates the process of securing MySQL, adding new users and creating the metastore database.\n#!/bin/bash\n# Obtaining the  Temporary Password\nroot_temp_pass=$(grep 'A temporary password' /var/log/mysqld.log |tail -1 |awk '{split($0,a,\": \"); print a[2]}')\necho \"root_temp_pass:\"$root_temp_pass\n\n# Creating a mysql_secure_installation.sql script to secure MySQL\ncat &gt; mysql_secure_installation.sql &lt;&lt; EOF\n# Ensuring that the server cannot be accessed without a password\n# The password will be the cluster password\n# Adding a user 'hdfs' and a user 'hive'\nALTER USER 'root'@'localhost' IDENTIFIED BY 'root_password';\nCREATE USER 'hive'@'localhost' IDENTIFIED BY 'root_password';\nCREATE USER 'hdfs'@'localhost' IDENTIFIED BY 'root_password';\n# Removing the anonymous users\nDELETE FROM mysql.user WHERE User='';\n# Disallowing remote login for root\nDELETE FROM mysql.user WHERE User='root' AND Host NOT IN ('localhost', '127.0.0.1', '::1');\n# Removing the demo database\nDROP DATABASE IF EXISTS test;\nDELETE FROM mysql.db WHERE Db='test' OR Db='test\\\\_%';\n# Make our changes take effect\nFLUSH PRIVILEGES;\nEOF\n\n# Running the SQL script to secure the installation of MySQL\nmysql -uroot -p\"$root_temp_pass\" --connect-expired-password &lt;mysql_secure_installation.sql\n\n# Creating another script to create the metastore and grant privileges to the hive, hdfs and root users for it\ncat &gt; mysql_privilege_grant.sql &lt;&lt; EOF\n# Granting privileges to the users hive, hdfs and root\nCREATE DATABASE metastore;\nuse metastore;\ngrant all PRIVILEGES on *.* to \"hive\"@\"localhost\" with grant option;\ngrant all PRIVILEGES on *.* to \"hdfs\"@\"localhost\" with grant option;\ngrant all PRIVILEGES on *.* to \"root\"@\"localhost\" with grant option;\nFLUSH PRIVILEGES;\nEOF\n\n# Running the SQL script to create the metastore and grant priviliges to the root, hive and hdfs users for the metastore\nmysql -u root -p\"root_password\" --connect-expired-password &lt;mysql_privilege_grant.sql\nReplace root_password with your actual password and run this script. Remember to delete this script and the ones created by this script once it finishes running!"
  },
  {
    "objectID": "posts/hive_sql/hive_sql.html#deleting-a-duplicate-library",
    "href": "posts/hive_sql/hive_sql.html#deleting-a-duplicate-library",
    "title": "Installing and configuring the HIVE metastore with a MySQL backend.",
    "section": "Deleting a duplicate library",
    "text": "Deleting a duplicate library\nWe will need to remove the log4j-slf4j-impl-2.17.1.jar file since MySQL has the same library, and the presence of multiple duplicate libraries in the CLASSPATH may cause Hive to fail.\nChange directories to your Hive installation, and then enter the lib directory, and delete the log4j-slf4j-impl-2.17.1.jar file."
  },
  {
    "objectID": "posts/hive_sql/hive_sql.html#adding-the-mysql-jdbc-library-to-hive",
    "href": "posts/hive_sql/hive_sql.html#adding-the-mysql-jdbc-library-to-hive",
    "title": "Installing and configuring the HIVE metastore with a MySQL backend.",
    "section": "Adding the MySQL JDBC library to Hive",
    "text": "Adding the MySQL JDBC library to Hive\nCopy the mysql-connector-java.jar file located at /usr/share/java/mysql-connector-java.jar to the lib directory of your Hive installation. This ensures that Hive can utilize the MySQL JDBC library to connect to MySQL databases seamlessly.\nFor example,\ncp /usr/share/java/mysql-connector-java.jar /opt/apache-hive-3.1.3-bin/lib"
  },
  {
    "objectID": "posts/hive_sql/hive_sql.html#adding-the-mysql-jdbc-library-to-spark",
    "href": "posts/hive_sql/hive_sql.html#adding-the-mysql-jdbc-library-to-spark",
    "title": "Installing and configuring the HIVE metastore with a MySQL backend.",
    "section": "Adding the MySQL JDBC library to Spark",
    "text": "Adding the MySQL JDBC library to Spark\nCopy the mysql-connector-java.jar file located at /usr/share/java/mysql-connector-java.jar to the jars directory of your Spark installation.\nThis will enable Spark to also utilize the MySQL JDBC library to connect to MySQL databases seamlessly.\nFor example,\ncp /usr/share/java/mysql-connector-java.jar /opt/spark-3.3.2-bin-hadoop3/jars"
  },
  {
    "objectID": "posts/hive_sql/hive_sql.html#creating-a-hive-site.xml-file",
    "href": "posts/hive_sql/hive_sql.html#creating-a-hive-site.xml-file",
    "title": "Installing and configuring the HIVE metastore with a MySQL backend.",
    "section": "Creating a hive-site.xml file",
    "text": "Creating a hive-site.xml file\nCreate a file named hive-site.xml in the conf directory of your Hive installation with the following content:\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;\n    &lt;value&gt;jdbc:mysql://localhost/metastore?createDatabaseIfNotExist=true&lt;/value&gt;\n    &lt;description&gt;URL for establishing a connection to the Hive Metastore database.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;\n    &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;\n    &lt;description&gt;Fully qualified class name of the JDBC driver used for connecting Hive and MySQL.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;\n    &lt;value&gt;hive&lt;/value&gt;\n    &lt;description&gt;User name used to connect to the MySQL database for Hive Metastore.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;\n    &lt;value&gt;root_password&lt;/value&gt;\n    &lt;description&gt;Password for authenticating and connecting to the MySQL database.&lt;/description&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;datanucleus.autoCreateSchema&lt;/name&gt;\n    &lt;description&gt;\n      When set to 'true,' this option allows automatic creation of the schema (database structure) \n      if it doesn't already exist when Hive Metastore initializes.\n    &lt;/description&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;datanucleus.fixedDatastore&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;datanucleus.autoCreateTables&lt;/name&gt;\n    &lt;description&gt;\n      When set to 'true,' it indicates that the datastore (MySQL database) structure is fixed \n      and should not be altered automatically by Hive Metastore.\n    &lt;/description&gt;\n    &lt;value&gt;True&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;"
  },
  {
    "objectID": "posts/hive_sql/hive_sql.html#editing-the-spark-env.sh-file",
    "href": "posts/hive_sql/hive_sql.html#editing-the-spark-env.sh-file",
    "title": "Installing and configuring the HIVE metastore with a MySQL backend.",
    "section": "Editing the spark-env.sh file",
    "text": "Editing the spark-env.sh file\nAdd the following line to your spark-env.sh file located in the conf directory of your Spark installation.\nexport SPARK_CLASSPATH=/opt/apache-hive-3.1.3-bin/jars/mysql-connector-java.jar"
  },
  {
    "objectID": "posts/hive_sql/hive_sql.html#editing-the-spark-defaults.conf-file",
    "href": "posts/hive_sql/hive_sql.html#editing-the-spark-defaults.conf-file",
    "title": "Installing and configuring the HIVE metastore with a MySQL backend.",
    "section": "Editing the spark-defaults.conf file",
    "text": "Editing the spark-defaults.conf file\nAdd the following line to your spark-defaults.conf file located in the conf directory of your Spark installation.\nspark.sql.warehouse.dir=hdfs://localhost:9000/user/hive/warehouse"
  },
  {
    "objectID": "posts/hive_sql/hive_sql.html#restarting-the-mysql-service",
    "href": "posts/hive_sql/hive_sql.html#restarting-the-mysql-service",
    "title": "Installing and configuring the HIVE metastore with a MySQL backend.",
    "section": "Restarting the MySQL service",
    "text": "Restarting the MySQL service\nRun the following commands to restart the MySQL service:\nservice mysqld stop\nservice mysqld start"
  },
  {
    "objectID": "posts/hive_sql/hive_sql.html#initializing-the-metastore",
    "href": "posts/hive_sql/hive_sql.html#initializing-the-metastore",
    "title": "Installing and configuring the HIVE metastore with a MySQL backend.",
    "section": "Initializing the Metastore",
    "text": "Initializing the Metastore\nRun the following commands:\nhdfs dfs -mkdir /tmp\nhdfs dfs -mkdir /user\nhdfs dfs -mkdir /user/hive\nhdfs dfs -mkdir /user/hive/warehouse\nhdfs dfs -chmod g+w /tmp\nhdfs dfs -chmod g+w /user/hive/warehouse\nschematool -dbType mysql -initSchema\nThis will initialize the Hive Metastore.\nThis should successfully have Hive configured to use the MySQL database as the backend RDBMS for metadata storage!"
  },
  {
    "objectID": "posts/pgbackrest_sftp/pgbackrest_sftp.html",
    "href": "posts/pgbackrest_sftp/pgbackrest_sftp.html",
    "title": "Secure SFTP Backups with pgBackRest for PostgreSQL: A Step-by-Step Guide",
    "section": "",
    "text": "pgBackRest is an open source tool that allows you to perform automated backup and restore operations for a PostgreSQL server. It allows you to take both full and incremental file system backups of your data. With pgBackRest, you can set up multiple repositories for your backups, both local and remote. You can further customize the configuration to take backups at specific times of the day so that regular operations are not affected, and backup retention and rotation can be customized as needed.\nFurthermore, it allows mirroring of backups to cloud-compatible object stores, including S3, Azure, and GCS compatible object stores.\n\n\n\npgBackRest helps make taking database backups pretty straightforward.\n\n\nInsecurely configured SFTP backups can expose critical database content to unauthorized access, yet many organizations rely on them for off-site storage. This guide addresses the security gap and will explain the process of setting up remote SFTP repositories in a secure manner. We will create a system user on the SFTP repository and limit their permissions, and perform scheduled backups exclusively via the limited user account.\nA good database backup strategy uses multiple repositories of different types, alongside both physical and logical backups. Do not limit yourself to merely SFTP repositories. Remember, with database backups, redundancy is good. This tutorial is simply to explain how to securely configure SFTP backups.\n\n\n\nPostgreSQL 12.0 or newer (this tutorial uses PostgreSQL 17.0)\n\nPostgreSQL 12.0+ is required because earlier versions lack WAL archiving features that enable point-in-time recovery when using pgBackRest.\n\npgBackRest 2.41 or newer (this tutorial uses pgBackRest 2.55.1)\n\nEarlier versions may have limitations with certain authentication methods\n\nOpenSSH 8.0+ client for improved Ed25519 key support\nlibssh2 1.9.0+ for modern cryptographic algorithm support\nSufficient disk space for temporary backup files before transfer\n\n\n\n\n\nLinux server with SSH daemon supporting SFTP subsystem\n\nRecommended: OpenSSH 8.0+ for optimal security features\n\nSufficient storage space for your backup retention needs. Plan for minimum 3x your database size (1x for full backup + 1x for transaction logs + 1x overhead and retention)\nRoot access for initial configuration\nAbility to create dedicated system users and groups\nFirewall configured to allow SFTP connections (typically port 22) from your database server IP\n\n\n\n\n\nStable network connection between database and SFTP servers\nFirewall rules allowing SFTP traffic (TCP port 22 by default)\nRecommended: Internal network connectivity (avoid public internet exposure if possible)\n\nYou can check your versions with these commands:\npostgres --version\npgbackrest version\nssh -V"
  },
  {
    "objectID": "posts/pgbackrest_sftp/pgbackrest_sftp.html#database-server-requirements",
    "href": "posts/pgbackrest_sftp/pgbackrest_sftp.html#database-server-requirements",
    "title": "Secure SFTP Backups with pgBackRest for PostgreSQL: A Step-by-Step Guide",
    "section": "",
    "text": "PostgreSQL 12.0 or newer (this tutorial uses PostgreSQL 17.0)\n\nPostgreSQL 12.0+ is required because earlier versions lack WAL archiving features that enable point-in-time recovery when using pgBackRest.\n\npgBackRest 2.41 or newer (this tutorial uses pgBackRest 2.55.1)\n\nEarlier versions may have limitations with certain authentication methods\n\nOpenSSH 8.0+ client for improved Ed25519 key support\nlibssh2 1.9.0+ for modern cryptographic algorithm support\nSufficient disk space for temporary backup files before transfer"
  },
  {
    "objectID": "posts/pgbackrest_sftp/pgbackrest_sftp.html#sftp-host-requirements",
    "href": "posts/pgbackrest_sftp/pgbackrest_sftp.html#sftp-host-requirements",
    "title": "Secure SFTP Backups with pgBackRest for PostgreSQL: A Step-by-Step Guide",
    "section": "",
    "text": "Linux server with SSH daemon supporting SFTP subsystem\n\nRecommended: OpenSSH 8.0+ for optimal security features\n\nSufficient storage space for your backup retention needs. Plan for minimum 3x your database size (1x for full backup + 1x for transaction logs + 1x overhead and retention)\nRoot access for initial configuration\nAbility to create dedicated system users and groups\nFirewall configured to allow SFTP connections (typically port 22) from your database server IP"
  },
  {
    "objectID": "posts/pgbackrest_sftp/pgbackrest_sftp.html#network-requirements",
    "href": "posts/pgbackrest_sftp/pgbackrest_sftp.html#network-requirements",
    "title": "Secure SFTP Backups with pgBackRest for PostgreSQL: A Step-by-Step Guide",
    "section": "",
    "text": "Stable network connection between database and SFTP servers\nFirewall rules allowing SFTP traffic (TCP port 22 by default)\nRecommended: Internal network connectivity (avoid public internet exposure if possible)\n\nYou can check your versions with these commands:\npostgres --version\npgbackrest version\nssh -V"
  },
  {
    "objectID": "posts/pgbackrest_sftp/pgbackrest_sftp.html#on-the-database-host",
    "href": "posts/pgbackrest_sftp/pgbackrest_sftp.html#on-the-database-host",
    "title": "Secure SFTP Backups with pgBackRest for PostgreSQL: A Step-by-Step Guide",
    "section": "On the database host",
    "text": "On the database host\n\nCreate an SSH key pair in the postgres user’s home directory.\n\n# Adjust this path based on your system's postgres home\nexport PGHOME=/var/lib/postgresql\nssh-keygen -f $PGHOME/.ssh/id_ed25519_sftp_key -t ed25519 -N \"\"\nEnsure that the .ssh folder is owned by the postgres user. We use Ed25519 keys because they provide stronger security with shorter keys compared to RSA, and are supported by modern OpenSSH implementations.\n\nPerform a key scan on the SFTP host to gather the public keys. Make sure you verify the legitimacy of the host server before you build your known hosts file.\n\n\nssh-keyscan 10.0.0.2 &gt;&gt; $PGHOME/.ssh/known_hosts \nYou can validate the host keys via fingerprints by running:\nssh-keygen -lf $PGHOME/.ssh/known_hosts"
  },
  {
    "objectID": "posts/pgbackrest_sftp/pgbackrest_sftp.html#on-the-sftp-server-host",
    "href": "posts/pgbackrest_sftp/pgbackrest_sftp.html#on-the-sftp-server-host",
    "title": "Secure SFTP Backups with pgBackRest for PostgreSQL: A Step-by-Step Guide",
    "section": "On the SFTP server host",
    "text": "On the SFTP server host\n\nCreate a directory that the SFTP user account will be restricted to.\nWe create a dedicated /data directory separate from standard system paths to isolate backup data and simplify permission management.\n# Creating a folder in the root directory\nmkdir /data\n\n# Creating the home directory for the SFTP user\nmkdir -p /data/sftpuser/upload\n\n## Setting the appropriate permissions for this filesystem\nchmod 701 /data\n\n## /data needs to have root:root permissions\nchown root:root /data\n\n\nCreate an SFTP group, followed by an SFTP user with minimal permissions and as a system user.\n\n# Creating an SFTP group\ngroupadd sftpusers\n# Creating the SFTP user as a system user\nuseradd -g sftpusers -d /data/sftpuser/upload/ -s /usr/sbin/nologin -r sftpuser\n\nThis process creates a group called sftpusers and creates a system user in that group called sftpuser, with their home directory being /data/sftpuser/upload.\n\n\nChange the permissions for the SFTP directory.\n\n## Giving ownership to the sftpusers group for the sftpuser folder\nchown root:sftpusers /data/sftpuser\n\n## Giving the sftpuser system account access to the home directory we created\nchown sftpuser:sftpusers /data/sftpuser/upload\nIt is very important that /data/sftpuser/upload must not be owned by root. This is a common cause of failure for chroot setups.\n\n\nUpdating the SSH configuration on the SFTP server.\nAdd the following to the end /etc/ssh/sshd_config:\n# For sftp backups\nMatch Group sftpusers\n  ChrootDirectory /data/%u\n  ForceCommand internal-sftp\n  AllowTcpForwarding no\n  X11Forwarding no\nHere’s what this does:\n\nThe first line Match Group sftpusers starts a conditional block that only applies to users in the sftpusers group.\nThe second line ChrootDirectory /data/%u (jails) the user to the directory /data/username, where %u is replaced with the username. This ensures the user is jailed to their upload directory and cannot access the broader file system.\nThe third line ForceCommand internal-sftp forces the SSH session to use the internal SFTP subsystem. The user cannot get a shell session, and only SFTP is allowed.\nThe fourth line AllowTcpForwarding no prevents the user using SSH port forwarding to avoid tunneling through SSH.\nThe fifth line X11Forwarding no disables X11 forwarding to prevent GUI application tunneling over SSH.\n\n\n\nValidate your SSH config and then Restart the SSH daemon\n\nsudo sshd -T && sudo systemctl restart sshd\n\n\n\n\n\n\nWarning\n\n\n\nDo not restart the SSH daemon without validating your SSH config with sudo sshd -T! If your config is wrong, the daemon will fail to initiate, and you may be locked out of being able to SSH into the server!\n\n\n\n\nCreate the SSH directory for the sftpuser account with appropriate permissions\n\n# Creating the .ssh directory\nmkdir /data/sftpuser/upload/.ssh\n\n## Creating the authorized_keys and known_hosts files\ntouch /data/sftpuser/upload/.ssh/authorized_keys\ntouch /data/sftpuser/upload/.ssh/known_hosts\n\n# Changing permissions for the newly created folders and files\nchown -R sftpuser:sftpusers /data/sftpuser/upload/.ssh\nchmod 700 /data/sftpuser/upload/.ssh\nchmod 640 /data/sftpuser/upload/.ssh/authorized_keys\nchmod 644 /data/sftpuser/upload/.ssh/known_hosts\n\n\nAppend the contents of the new public SSH key to the authorized_keys file\nOn the database host, copy the contents of the public key we created ($PGHOME/.ssh/id_ed25519_sftp_key.pub) and append it to the authorized_keys file on the SFTP user’s SSH directory (‘/data/sftpuser/upload/.ssh/authorized_keys’).\n\n\n\n\n\n\nWarning\n\n\n\nRemember to never share your private key with any server, or to anyone else! Only copy the contents of the public key!"
  },
  {
    "objectID": "posts/pgbackrest_sftp/pgbackrest_sftp.html#using-secure-key-algorithms-and-hash-functions",
    "href": "posts/pgbackrest_sftp/pgbackrest_sftp.html#using-secure-key-algorithms-and-hash-functions",
    "title": "Secure SFTP Backups with pgBackRest for PostgreSQL: A Step-by-Step Guide",
    "section": "Using secure key algorithms and hash functions",
    "text": "Using secure key algorithms and hash functions\nIf you are using older versions of cryptography libraries like libssh2 and openssl, your configuration might be restricted to using older, more outdated key algorithms like RSA or DSS, along with older hash functions like SHA1. I recommend using more secure, newer key algorithms like Ed25519, along with more secure hashing functions like SHA256.\n\nCheck the key algorithms supported by your installation of libssh2\nRun the following code to see what key types are supported by your installation of libssh2:\nstrings /usr/lib/x86_64-linux-gnu/libssh2.so.1 | grep ssh-\nYour output should return more modern key algorithms like ssh-ed25519. If you output only includes algorithms like ssh-rsa or ssh-dss, then you will need to update your installation of libssh2. Some older servers, like ones running outdated operating systems like Ubuntu 20.04, may not allow you to update your libssh2 library using package managers. In that case, you will have to manually build the newer versions from source.\n## Installing dependencies\nsudo apt update\nsudo apt install build-essential cmake zlib1g-dev libssl-dev\n\n# Downloading and building libssh2 from source\nwget https://www.libssh2.org/download/libssh2-1.11.0.tar.gz\ntar -xzf libssh2-1.11.0.tar.gz\ncd libssh2-1.11.0\n\n# Installing libssh2\n./configure\nmake -j$(nproc)\nsudo make install\nsudo ldconfig\nThe new libssh2 library will have it’s binaries located at /usr/local/lib/libssh2.so. You can confirm if it was installed by running:\nstrings /usr/local/lib/libssh2.so | grep ssh-\nYou should see the newer key algorithms like ecdsa-sha2-nistp256and ssh-ed25519.\nIf you built/installed pgBackRest before updating your libssh2 library, you will need to rebuild it from source so that it uses the newer library.\nTo check if pgBackRest is using the newly installed libssh2 as a dependency, run:\nldd $(which pgbackrest) | grep libssh2\nYour output should look like this:\nlibssh2.so.1 =&gt; /usr/local/lib/libssh2.so.1 (0x00007f20882cf000)\nThis confirms that pgBackRest is using the manually installed libssh2 from /usr/local/lib, rather than the system-wide default at /usr/lib/x86_64-linux-gnu."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Naveen Kannan’s Publications",
    "section": "",
    "text": "Ajayakumar, J., Curtis, A. J., Maisha, F. M., Bempah, S., Ali, A., Kannan, N., Armstrong, G., & Morris, J. G. (2024). Using spatial video and deep learning for automated mapping of ground-level context in relief camps. International Journal of Health Geographics, 23(1), 23.\n\nRuksakulpiwat, S., Thongking, W., Kannan, N., Wright, E., Niyomyart, A., Benjasirisan, C., Chiaranai, C., Smothers, C., Aldossary, H. M., & Still, C. H. (9900). Understanding the Relationship Between Comorbidities, Medication Nonadherence, Activities of Daily Living, and Heart Condition Status Among Older Adults in the United States: A Regression Analysis and Machine Learning Approach. Journal of Cardiovascular Nursing.\n\nAnnotation and scoring of the deleteriousness of individual genetic variants in the 4th release of the Alzheimer’s Disease Sequencing Project. (PB4451)\nNaveen Kannan1, Nicholas Wheeler1, Genome Center for Alzheimer’s Disease, Li-San Wang2, Yuk Yee Leung2, William S. Bush1\n\nCleveland Institute for Computational Biology, Department for Population and Quantitative Health Sciences, Case Western Reserve University, Cleveland, Ohio 44106, USA.\n\nDepartment of Pathology and Laboratory Medicine, Penn Neurodegeneration Genomics Center, Perelman School of Medicine, University of Pennsylvania, Philadelphia, Pennsylvania 19104, USA.\n\nPresented at the Annual Meeting of The American Society of Human Genetics, November 3, 2023 in Washington DC.\n\nSegamalai, D., Abdul Jameel, A. R., Kannan, N., Anbalagan, A., Duraisamy, B., Raju, P., & Devy Gounder, K. (2017). Mediastinal pseudocyst: varied presentations and management—experience from a tertiary referral care centre in India. HPB Surgery, 2017.\n\nVellaisamy, R., Kannan, N., Anbalagan, A., Raju, P., Duraisamy, B., Murugesan, C. S., & Gounder, K. D. (2016). Endoscopic access to hepatic duct through duodenum during follow up–after primary surgery for hepatolithiasis. HPB, 18, e530.\n\nRamasamy, V., Vellaisamy, R., Kannan, N., & Gounder, K. D. (2016). Refined technique of access loop in hepatobiliary surgery. HPB, 18, e593-e594.\n\nKannan, N., Vellaisamy, R., Govindarajan, M., & Gounder, K. D. (2016). Pellagra following pancreaticoduodenectomy for malignant pancreatic carcinoid with pluripotent hormonal potential. HPB, 18, e381-e382."
  }
]