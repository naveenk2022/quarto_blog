{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: 'SLURM and HPC'\n",
        "date: '2023-06-14'\n",
        "categories: ['Computing','HPC','Discussion']\n",
        "description: 'An overview of SLURM in the context of HPC clusters.'\n",
        "execute: \n",
        "  message: false\n",
        "  warning: false\n",
        "editor_options: \n",
        "  chunk_output_type: console\n",
        "---"
      ],
      "id": "4a346cf8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SLURM Workload Manager\n",
        "\n",
        "SLURM (formerly known as Simple Linux Utility for Resource Management) is an open-source job scheduling system for Linux clusters.\n",
        "\n",
        "![](slurm-png.png){fig-align=\"center\" width=\"587\"}\n",
        "\n",
        "It does not require kernel modification and is relatively self contained. It has three key functions:\n",
        "\n",
        "-   Allocation of access to resources (compute nodes) to users for a defined period of time.\n",
        "\n",
        "-   Providing a framework that allows for starting and executing jobs, including parallel computing processes.\n",
        "\n",
        "-   Queue management to arbitrate resource contention.\n",
        "\n",
        "# SLURM and HPC clusters\n",
        "\n",
        "SLURM is uniquely suited for use in HPC clusters, due to its ability to facilitate *efficient utilization* of the resources available for an HPC cluster.\n",
        "\n",
        "SLURM places jobs/tasks in a queue, and access to resources is allowed based on the processes that are already running at the time, which is very well suited for an HPC cluster, where resources are under heavy usage.\n",
        "\n",
        "![SLURM architecture. Slurm has a centralized manager, slurmctld, to monitor resources and work. Each compute server (node) has a slurmd daemon, which can be compared to a remote shell: it waits for work, executes that work, returns status, and waits for more work.](slurm-diag.png){fig-align=\"center\"}\n",
        "\n",
        "# SLURM Directives\n",
        "\n",
        "SLURM allows for specifying commands in the SLURM scripts that allow it to arbitrate job allocation and resource management. These commands are present at the top of a SLURM script, preceeded by `#SBATCH`.\n",
        "\n",
        "These `SBATCH` commands and how specific they can get are the lynchpin of SLURM's ability to perform resource and task arbitration. There are a [large number of `#SBATCH` commands.](https://slurm.schedmd.com/cpu_management.html)\n",
        "\n",
        "Of these commands, the most relevant are:\n",
        "\n",
        "| Command                                           | Utility                                                          |\n",
        "|--------------------------------|----------------------------------------|\n",
        "| **-N** or **\\--nodes**=minnodes\\[-maxnodes\\]      | Request that a minimum of N nodes be allocated                   |\n",
        "| **-n** or **\\--ntasks**=number                    | Request sufficient resources to launch a maximum of number tasks |\n",
        "| **\\--mem-per-node**=#MB or **\\--mem-per-cpu**=#MB | Specify minimum memory requirement per node or per CPU           |\n",
        "| **\\--ntasks-per-node**=ntasks                     | Request the maximum ntasks be invoked on each node               |\n",
        "\n",
        "Based on these request parameters, SLURM can queue and allocate jobs to run on the available compute nodes.\n",
        "\n",
        "# SLURM Scripts\n",
        "\n",
        "\n",
        "```{bash}\n",
        "\n",
        "```"
      ],
      "id": "d8688034"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}