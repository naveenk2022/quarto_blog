{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Installing and configuring Hadoop and Spark on a 4 node cluster.\"\n",
        "date: \"2023-08-21\"\n",
        "categories: ['Computing','Clusters','Discussion','Hadoop','Spark','HDFS', 'YARN']\n",
        "description: \"A guide to installing Hadoop and Spark on a 4 node cluster, while configuring and setting up HDFS, YARN and MapReduce.\"\n",
        "execute: \n",
        "  message: false\n",
        "  warning: false\n",
        "editor_options: \n",
        "  chunk_output_type: console\n",
        "---"
      ],
      "id": "35abc963"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A brief introduction to Hadoop\n",
        "\n",
        "Apache Hadoop is an open source software library that allows for the distributed processing of large datasets across a computing cluster.\n",
        "\n",
        "![](1200px-Hadoop_logo_new.svg.png){fig-align=\"center\"}\n",
        "\n",
        "Hadoop shines at processing enormously massive amounts of data that is too big to fit on a single computing system.\n",
        "\n",
        "# Hadoop's Main Components\n",
        "\n",
        "Hadoop has components integral to its functioning. These include:\n",
        "\n",
        "**The HDFS (Hadoop Distributed File System)**\n",
        "\n",
        "HDFS is a distributed file system that provides high-throughput access to application data, and is used to scale a single Apache Hadoop cluster to hundreds (and even thousands) of nodes.\n",
        "\n",
        "HDFS takes massive data files and splits them into smaller chunks called \"blocks.\" These blocks are typically quite large (often 128 MB or 256 MB), which helps in efficient data management and processing.\n",
        "\n",
        "These blocks are spread out and replicated across multiple machines in the cluster, ensuring that if one machine fails, the stored data is still safe because there are replicated copies of each block spread across different machines.\n",
        "\n",
        "HDFS has two components: the NameNode and the DataNode.\n",
        "\n",
        "The NameNode keeps track of where each block of data is located in the cluster.\n",
        "\n",
        "The DataNode(s) store the actual data blocks and are spread across the cluster.\n",
        "\n",
        "**YARN (Yet Another Resource Negotiator)**\n",
        "\n",
        "YARN, which stands for Yet Another Resource Negotiator, plays a pivotal role in managing resources and scheduling tasks within the Hadoop architecture. It consists of two primary components: the ResourceManager (RM) and the NodeManager (NM).\n",
        "\n",
        "YARN effectively distributes computing resources and manages job scheduling across various applications running on the Hadoop cluster.\n",
        "\n",
        "The ResourceManager is the central decision-maker for resource allocation and is responsible for fairly distributing resources among all the applications running on the cluster.\n",
        "\n",
        "The NodeManager is responsible for managing resources on a per-machine basis.\n",
        "\n",
        "**MapReduce**\n",
        "\n",
        "MapReduce is a programming model and processing framework that divides big data tasks into smaller sub-tasks, processes them in parallel across a cluster, and then combines the results.\n",
        "\n",
        "Mapping:\n",
        "\n",
        "-   Data is split into chunks and assigned to \"mappers.\"\n",
        "-   Mappers process data and produce intermediate key-value pairs.\n",
        "\n",
        "Shuffling and Sorting:\n",
        "\n",
        "-   Intermediate pairs are sorted and grouped by keys.\n",
        "-   Data with the same key is grouped together.\n",
        "\n",
        "Reducing:\n",
        "\n",
        "-   Grouped data is sent to \"reducers.\"\n",
        "-   Reducers process the data, performing operations and producing final results.\n",
        "\n",
        "**Hadoop Common**\n",
        "\n",
        "Hadoop Common provides essential fundamental libraries and utilities for different components within the Hadoop architecture. It serves as a shared resource that supports the entire Hadoop framework.\n",
        "\n",
        "![A diagrammatic representation of the Hadoop architecture across a computing cluster with a Master Node and 3 Slave Nodes.](hadoop_architecture.png){fig-align=\"center\"}\n",
        "\n",
        "# Installing Hadoop on a 4 node cluster \n",
        "\n",
        "## Prerequisites: \n",
        "\n",
        "To begin with, the 4 clusters require a Unix-like operating system. \n",
        "\n",
        "Furthermore, while not essential, using Ansible enormously reduces the time needed to configure and install HDFS with the aforementioned components. \n",
        "\n",
        "In my previous blog post, I described how to use Ansible (deployed via a Docker container) to remotely configure a computing cluster. This obviates the need for Ansible itself to be installed on the head node. For details on how to use Ansible to configure a cluster, please refer my previous blog post. \n",
        "\n",
        "I used Ansible to perform all of the tasks detailed in this blog post. While this post describes the general steps needed for installing Hadoop, the next blog post will have the Ansible playbooks posted so as to describe each step of the process in more detail. \n",
        "\n",
        "### JDK 8 \n",
        "\n",
        "Ensure that each cluster node has OpenJDK installed. \n",
        "\n",
        "Note that if this is being installed using the `yum` package manager, the EPEL (Extra Packages for Enterprise Linux) repository needs to be imported and activated. \n",
        "\n",
        "### SSH (Secure Shell) and PDSH \n",
        "\n",
        "Hadoop requires SSH and PDSH to be installed on each node of the cluster. Communication between nodes is done via Hadoop. \n",
        "\n",
        "Also, ensure that each node can SSH into every other node without a password prompt. This will require performing an SSH Key exchange between every node of the cluster and every other node. (Refer to my blog post about using Ansible to remotely configure a cluster for more details on SSH key exchanges.)\n",
        "\n",
        "Pro-tip! On my RHEL 8 head node, I ran into a lot of erros from using `pdsh` since it didn't come with SSH incorporated. I used `dnf` to download `pdsh-rcmd-ssh` after installing `pdsh` using `yum.`\n",
        "\n",
        "Update your `.bashrc` or `.bash_profile` file with the following: \n",
        "\n",
        "```\n",
        "export PDSH_RCMD_TYPE=ssh\n",
        "```\n",
        "\n",
        "### Python 3.9 \n",
        "\n",
        "While not a hard requirement for just Hadoop, we intend to install Spark, and use Python to interact with the Scala API, using Jupyter Notebook as the driver. \n",
        "\n",
        "### Jupyter Notebook \n",
        "\n",
        "Once Python is installed, run `python3 -m pip install --upgrade pip` in order to upgrade pip, and then install Jupyter using `pip install jupyter`. \n",
        "\n",
        "\n",
        "## Installing Hadoop and Spark\n",
        "\n",
        "### Downloading and Unzipping Hadoop \n",
        "\n",
        "Download a recent stable release from one of the [Apache Download Mirrors for Hadoop.](https://www.apache.org/dyn/closer.cgi/hadoop/common/) (I downloaded Hadoop 3.3.6.)\n",
        "\n",
        "Unzip the downloaded tarball. \n",
        "\n",
        "### Downloading and Unzipping Spark  \n",
        "\n",
        "Download a recent stable release from one of the [Apache Download Mirrors for Spark.](https://www.apache.org/dyn/closer.lua/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz) (I downloaded Spark 3.3.2 to be compatiable with Hail and Glow.)\n",
        "\n",
        "Unzip the downloaded tarball. \n",
        "\n",
        "### Configuring Hadoop and Spark\n",
        "\n",
        "In your `.bashrc` or `bash_profile`, update the locations of Java, Hadoop and Spark to your PATH. \n",
        "\n",
        "Update the locations of HDFS, MapReduce and Common.\n",
        "\n",
        "Reload the terminal. \n",
        "\n",
        "\n",
        "```{bash}\n",
        "export HADOOP_HOME=/opt/hadoop-3.3.6\n",
        "export PATH=$PATH:$HADOOP_HOME/bin\n",
        "export PATH=$PATH:$HADOOP_HOME/sbin\n",
        "export HADOOP_MAPRED_HOME=${HADOOP_HOME}\n",
        "export HADOOP_COMMON_HOME=${HADOOP_HOME}\n",
        "export HADOOP_HDFS_HOME=${HADOOP_HOME}\n",
        "export HADOOP_YARN_HOME=${HADOOP_HOME}\n",
        "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.382.b05-2.el8.x86_64\n",
        "export LANG=en_US.UTF-8\n",
        "export TZ=UTC\n",
        "export CLASSPATH_PREPEND_DISTCACHE=\n",
        "export SPARK_HOME=/opt/spark-3.3.2-bin-hadoop3\n",
        "export PATH=$PATH:$SPARK_HOME/bin\n",
        "export HADOOP_CONF_DIR=/opt/hadoop-3.3.6/etc/hadoop\n",
        "export LD_LIBRARY_PATH=/opt/hadoop-3.3.6/lib/native:$LD_LIBRARY_PATH\n",
        "export PYSPARK_DRIVER_PYTHON=jupyter\n",
        "export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --NotebookApp.open_browser=False --NotebookApp.ip='localhost' \"\n",
        "export PATH=$PATH:/usr/local/bin\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "### Updating the Hadoop configuration files \n",
        "\n",
        "The following configuration files need to be edited across the cluster. \n",
        "\n",
        "-   hadoop-env.sh \n",
        "\n",
        "This file can be found in your installation directory. Assuming the name of the directory hadoop was unzipped to is /hadoop, the file can be found at /hadoop/etc/hadoop/hadoop-env.sh.\n",
        "\n",
        "Add the following line to the file in order to add the location of the Java installation to Hadoop:\n",
        "\n",
        "```\n",
        "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/\n",
        "\n",
        "```\n",
        "\n",
        "Note that the name/location of the Java installation can change depending on your system. \n",
        "\n",
        "-   workers \n",
        "\n",
        "Edit the `workers` file to add the names of your cluster. Note that this requires the names of the nodes with their IP addresses to be present in the /etc/hosts file. \n",
        "\n",
        "-   core-site.xml\n",
        "\n",
        "This file can be found in your installation directory. Assuming the name of the directory hadoop was unzipped to is /hadoop, the file can be found at /hadoop/etc/hadoop/core-site.xml.\n",
        "\n",
        "Insert this code block between `<configuration>` and `</configuration>`.\n",
        "\n",
        "```\n",
        "<property>\n",
        "    <name>fs.defaultFS</name>\n",
        "    <value>hdfs://namenode:9000</value>\n",
        "</property>\n",
        "```\n",
        "\n",
        "-   hdfs-site.xml\n",
        "\n",
        "This file can be found in your installation directory. Assuming the name of the directory hadoop was unzipped to is /hadoop, the file can be found at /hadoop/etc/hadoop/hdfs-site.xml.\n",
        "\n",
        "Insert this code block between `<configuration>` and `</configuration>`.\n",
        "\n",
        "```\n",
        "<property>\n",
        "    <name>dfs.namenode.name.dir</name>\n",
        "    <value>/home/temp/data/nameNode</value>\n",
        "</property>\n",
        "\n",
        "<property>\n",
        "    <name>dfs.datanode.data.dir</name>\n",
        "    <value>/home/temp/data/dataNode</value>\n",
        "</property>\n",
        "\n",
        "<property>\n",
        "    <name>dfs.replication</name>\n",
        "    <value>4</value>\n",
        "</property>\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "-   yarn-site.xml\n",
        "\n",
        "This file can be found in your installation directory. Assuming the name of the directory hadoop was unzipped to is /hadoop, the file can be found at /hadoop/etc/hadoop/yarn-site.xml.\n",
        "\n",
        "Insert this code block between `<configuration>` and `</configuration>`.\n",
        "\n",
        "```\n",
        "<property>\n",
        "    <name>yarn.nodemanager.aux-services</name>\n",
        "    <value>mapreduce_shuffle</value>\n",
        "</property>\n",
        "\n",
        "<property>\n",
        "    <name>yarn.nodemanager.env-whitelist</name>\n",
        "    <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME</value>\n",
        "</property>\n",
        "\n",
        "<property>\n",
        "    <name>yarn.resourcemanager.hostname</name>\n",
        "    <value>(namenode IP Address)</value>\n",
        "</property>\n",
        "\n",
        "<property>\n",
        "    <name>yarn.acl.enable</name>\n",
        "    <value>0</value>\n",
        "</property>\n",
        "\n",
        "```\n",
        "\n",
        "-   mapred-site.xml\n",
        "\n",
        "This file can be found in your installation directory. Assuming the name of the directory hadoop was unzipped to is /hadoop, the file can be found at /hadoop/etc/hadoop/mapred-site.xml.\n",
        "\n",
        "Insert this code block between `<configuration>` and `</configuration>`.\n",
        "\n",
        "```\n",
        "<property>\n",
        "    <name>mapreduce.framework.name</name>\n",
        "    <value>yarn</value>\n",
        "</property>\n",
        "\n",
        "<property>\n",
        "    <name>yarn.app.mapreduce.am.env</name>\n",
        "    <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>\n",
        "</property>\n",
        "\n",
        "<property>\n",
        "    <name>mapreduce.map.env</name>\n",
        "    <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>\n",
        "</property>\n",
        "\n",
        "<property>\n",
        "    <name>mapreduce.reduce.env</name>\n",
        "    <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>\n",
        "</property>\n",
        "\n",
        "<property>\n",
        "    <name>mapreduce.application.classpath</name>\n",
        "    <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*,$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*,$HADOOP_MAPRED_HOME/share/hadoop/common/*,$HADOOP_MAPRED_HOME/share/hadoop/common/lib/*,$HADOOP_MAPRED_HOME/share/hadoop/yarn/*,$HADOOP_MAPRED_HOME/share/hadoop/yarn/lib/*,$HADOOP_MAPRED_HOME/share/hadoop/hdfs/*,$HADOOP_MAPRED_HOME/share/hadoop/hdfs/lib/*</value>\n",
        "</property>\n",
        "```\n",
        "\n",
        "### Updating the Spark configuration files \n",
        "\n",
        "Rename spark-defaults.conf.template to spark-defaults.conf. The file can be found at spark-3.3.2-bin-hadoop3/conf/spark-defaults.conf.template. \n",
        "\n",
        "Replace `spark.master   spark://master:7077` with `spark.master   yarn`.\n",
        "\n",
        "### Starting the HDFS and YARN services \n",
        "\n",
        "Once the configuration files are edited and the PATH is updated, the HDFS namenode can be formatted by running `hdfs namenode -format`. \n",
        "\n",
        "The HDFS node services can be initiated by running `start-dfs.sh`. \n",
        "\n",
        "YARN can be initiated by running `start-yarn.sh`.\n",
        "\n",
        "Ensure that these services are running by running the `jps` command. \n",
        "\n",
        "The head node will have the services `NameNode` and `ResourceManager` running. Remember that these services run only on the head node. \n",
        "\n",
        "If used as a data node, it will also have the services `DataNode` and `NodeManager` running on it. \n",
        "\n",
        "The slave nodes will have `DataNode` and `NodeManager` running on it. \n",
        "\n",
        "# Monitoring the services \n",
        "\n",
        "Initiate the history server by running `$SPARK_HOME/sbin/start-history-server.sh`. \n",
        "\n",
        "This can be accessed at `http://localhost:8088`. \n"
      ],
      "id": "64947723"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}